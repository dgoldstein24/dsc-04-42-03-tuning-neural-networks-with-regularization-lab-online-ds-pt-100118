{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regularization and Optimization of Neural Networks - Lab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "Recall from the last lab that we had a training accuracy close to 90% and a test set accuracy close to 76%.\n",
    "\n",
    "As with our previous machine learning work, we should be asking a couple of questions:\n",
    "- Is there a high bias? yes/no\n",
    "- Is there a high variance? yes/no\n",
    "\n",
    "Also recall that \"high bias\" is a relative concept. Knowing we have 7 classes and the topics are related, we'll assume that a 90% accuracy is pretty good and the bias on the training set is low. (We've also discussed concepts like precision, recall as well as AUC and ROC curves.)   \n",
    "\n",
    "In this lab, we'll use the notion of training/validation/test set to get better insights of how we can mitigate our variance, and we'll look at a few regularization techniques. You'll start by repeating the process from the last section: importing the data and performing preprocessing including one-hot encoding. Then, just before you go on to train the model, we'll introduce how to include a validation set. You'll then define and compile the model as before. This time, when you are presented with the `history` dictionary of the model, you will have additional data entries for not only the train and test, but the train, test and validation  and then defigning, compiling and training the model. \n",
    "\n",
    "\n",
    "## Objectives\n",
    "\n",
    "You will be able to:\n",
    "\n",
    "* Construct and run a basic model in Keras\n",
    "* Construct a validation set and explain potential benefits\n",
    "* Apply L1 and L2 regularization\n",
    "* Aplly dropout regularization\n",
    "* Observe and comment on the effect of using more data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import the libraries\n",
    "\n",
    "As usual, start by importing some of the packages and modules that you intend to use. The first thing we'll be doing is importing the data and taking a random sample, so that should clue you in to what tools to import. If you need more tools down the line, you can always import additional packages later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "#Your code here; import some packages/modules you plan to use\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from sklearn import preprocessing\n",
    "from keras.preprocessing.text import Tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the Data\n",
    "\n",
    "As with the previous lab, the data is stored in a file **Bank_complaints.csv**. Load and preview the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 60000 entries, 0 to 59999\n",
      "Data columns (total 2 columns):\n",
      "Product                         60000 non-null object\n",
      "Consumer complaint narrative    60000 non-null object\n",
      "dtypes: object(2)\n",
      "memory usage: 937.6+ KB\n",
      "None\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Product</th>\n",
       "      <th>Consumer complaint narrative</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Student loan</td>\n",
       "      <td>In XX/XX/XXXX I filled out the Fedlaon applica...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Student loan</td>\n",
       "      <td>I am being contacted by a debt collector for p...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Student loan</td>\n",
       "      <td>I cosigned XXXX student loans at SallieMae for...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Student loan</td>\n",
       "      <td>Navient has sytematically and illegally failed...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Student loan</td>\n",
       "      <td>My wife became eligible for XXXX Loan Forgiven...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Product                       Consumer complaint narrative\n",
       "0  Student loan  In XX/XX/XXXX I filled out the Fedlaon applica...\n",
       "1  Student loan  I am being contacted by a debt collector for p...\n",
       "2  Student loan  I cosigned XXXX student loans at SallieMae for...\n",
       "3  Student loan  Navient has sytematically and illegally failed...\n",
       "4  Student loan  My wife became eligible for XXXX Loan Forgiven..."
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Your code here; load and preview the dataset\n",
    "df = pd.read_csv('Bank_complaints.csv')\n",
    "print(df.info())\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df.Product.value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing Overview\n",
    "\n",
    "Before we begin to practice some of our new tools regarding regularization and optimization, let's practice munging some data as we did in the previous section with bank complaints. Recall some techniques:\n",
    "\n",
    "* Sampling in order to reduce training time (investigate model accuracy vs data size later on)\n",
    "* One-hot encoding our complaint text\n",
    "* Transforming our category labels\n",
    "* Train - test split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing: Generate a Random Sample\n",
    "\n",
    "Since we have quite a bit of data and training networks takes a substantial amount of time and resources, we will downsample in order to test our initial pipeline. Going forward, these can be interesting areas of investigation: how does our models performance change as we increase (or decrease) the size of our dataset?  \n",
    "\n",
    "Generate the random sample using seed 123 for consistency of results. Make your new sample have 10,000 observations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Your code here\n",
    "\n",
    "random.seed(123)\n",
    "df = df.sample(10000)\n",
    "df.reset_index(drop = True, inplace = True)\n",
    "product = df.Product\n",
    "complaints = df['Consumer complaint narrative']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing: One-hot Encoding of the Complaints\n",
    "\n",
    "As before, we need to do some preprocessing and data manipulationg before building the neural network. Last time, we guided you through the process, and now its time for you to practice that pipeline independently.  \n",
    "\n",
    "Only keep 2,000 most common words and use one-hot encoding to reformat the complaints into a matrix of vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 2000)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Your code here; use one-hot encoding to reformat the complaints into a matrix of vectors.\n",
    "#Only keep the 2000 most common words.\n",
    "\n",
    "tokenizer = Tokenizer(num_words = 2000)\n",
    "tokenizer.fit_on_texts(complaints)\n",
    "\n",
    "onehot_results = tokenizer.texts_to_matrix(complaints, mode = 'binary')\n",
    "word_index = tokenizer.word_index\n",
    "np.shape(encoding_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing: Encoding the Products\n",
    "\n",
    "Similarly, now transform the descriptive product labels to integers labels. After transforming them to integer labels, retransform them into a matrix of binary flags, one for each of the various product labels.  \n",
    "  \n",
    "  (Note: this is similar to our previous work with dummy variables: each of the various product categories will be its own column, and each observation will be a row. Each of these observation rows will have a 1 in the column associated with it's label, and all other entries for the row will be zero.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., ..., 0., 0., 1.],\n",
       "       [0., 1., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 1., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 1., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 1.]], dtype=float32)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Your code here; transform the product labels to numerical values\n",
    "#Then transform these integer values into a matrix of binary flags\n",
    "\n",
    "label_encoder = preprocessing.LabelEncoder()\n",
    "label_encoder.fit(product)\n",
    "product_categories = label_encoder.transform(product)\n",
    "\n",
    "product_onehot = to_categorical(product_categories)\n",
    "\n",
    "product_onehot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train-test Split\n",
    "\n",
    "Now onto the ever familiar train-test split! Be sure to split both the complaint data (now transformed into word vectors) as well as their associated labels. Perform an appropriate train test split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Your code here\n",
    "X_train, X_test, y_train, y_test = train_test_split(onehot_results,product_onehot, test_size = 1500, random_state = 42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running the model using a validation set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating the Validation Set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the lecture we mentioned that in deep learning, we generally keep aside a validation set, which is used during hyperparameter tuning. Then when we have made the final model decision, the test set is used to define the final model perforance. \n",
    "\n",
    "In this example, let's take the first 1000 cases out of the training set to become the validation set. You should do this for both `train` and `label_train`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Just run this block of code \n",
    "random.seed(123)\n",
    "val = X_train[:1000]\n",
    "train_final = X_train[1000:]\n",
    "label_val = y_train[:1000]\n",
    "label_train_final = y_train[1000:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's rebuild a fully connected (Dense) layer network with relu activations in Keras."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall that we used 2 hidden with 50 units in the first layer and 25 in the second, both with a `relu` activation function. Because we are dealing with a multiclass problem (classifying the complaints into 7 classes), we use a use a softmax classifyer in order to output 7 class probabilities per case.  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Your code here; build a neural network using Keras as described above.\n",
    "random.seed(123)\n",
    "from keras import models, layers\n",
    "\n",
    "\n",
    "model = models.Sequential()\n",
    "model.add(layers.Dense(50, activation = 'relu', input_shape = (2000,)))\n",
    "model.add(layers.Dense(25, activation = 'relu'))\n",
    "model.add(layers.Dense(7, activation = 'softmax'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compiling the Model\n",
    "In the compiler, you'll be passing the optimizer, loss function, and metrics. Train the model for 120 epochs in mini-batches of 256 samples. This time, let's include the argument `validation_data` and assign it `(val, label_val)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Your code here\n",
    "\n",
    "model.compile(optimizer = 'SGD', loss = 'categorical_crossentropy', metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Code Along\n",
    "\n",
    "The remaining portion of this lab will introduce you to code snippets for a myriad of different methods discussed in the lecture."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the Model\n",
    "\n",
    "Ok, now for the resource intensive part: time to train our model! Note that this is where we also introduce the validation data to the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 7500 samples, validate on 1000 samples\n",
      "Epoch 1/120\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 1.9503 - acc: 0.1604 - val_loss: 1.9421 - val_acc: 0.1520\n",
      "Epoch 2/120\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 1.9267 - acc: 0.1884 - val_loss: 1.9228 - val_acc: 0.1940\n",
      "Epoch 3/120\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 1.9075 - acc: 0.2219 - val_loss: 1.9040 - val_acc: 0.2250\n",
      "Epoch 4/120\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 1.8872 - acc: 0.2473 - val_loss: 1.8838 - val_acc: 0.2530\n",
      "Epoch 5/120\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 1.8644 - acc: 0.2721 - val_loss: 1.8612 - val_acc: 0.2810\n",
      "Epoch 6/120\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 1.8394 - acc: 0.3015 - val_loss: 1.8361 - val_acc: 0.3010\n",
      "Epoch 7/120\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 1.8114 - acc: 0.3303 - val_loss: 1.8079 - val_acc: 0.3340\n",
      "Epoch 8/120\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 1.7799 - acc: 0.3697 - val_loss: 1.7761 - val_acc: 0.3650\n",
      "Epoch 9/120\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 1.7451 - acc: 0.4113 - val_loss: 1.7417 - val_acc: 0.3870\n",
      "Epoch 10/120\n",
      "7500/7500 [==============================] - 0s 22us/step - loss: 1.7066 - acc: 0.4444 - val_loss: 1.7037 - val_acc: 0.4070\n",
      "Epoch 11/120\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 1.6645 - acc: 0.4771 - val_loss: 1.6627 - val_acc: 0.4410\n",
      "Epoch 12/120\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 1.6197 - acc: 0.5088 - val_loss: 1.6192 - val_acc: 0.4640\n",
      "Epoch 13/120\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 1.5716 - acc: 0.5375 - val_loss: 1.5723 - val_acc: 0.4860\n",
      "Epoch 14/120\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 1.5217 - acc: 0.5561 - val_loss: 1.5242 - val_acc: 0.4970\n",
      "Epoch 15/120\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 1.4707 - acc: 0.5769 - val_loss: 1.4754 - val_acc: 0.5150\n",
      "Epoch 16/120\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 1.4187 - acc: 0.5928 - val_loss: 1.4261 - val_acc: 0.5380\n",
      "Epoch 17/120\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 1.3667 - acc: 0.6088 - val_loss: 1.3772 - val_acc: 0.5500\n",
      "Epoch 18/120\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 1.3160 - acc: 0.6223 - val_loss: 1.3294 - val_acc: 0.5730\n",
      "Epoch 19/120\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 1.2665 - acc: 0.6347 - val_loss: 1.2821 - val_acc: 0.5820\n",
      "Epoch 20/120\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 1.2198 - acc: 0.6421 - val_loss: 1.2381 - val_acc: 0.5980\n",
      "Epoch 21/120\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 1.1747 - acc: 0.6577 - val_loss: 1.1978 - val_acc: 0.6110\n",
      "Epoch 22/120\n",
      "7500/7500 [==============================] - 0s 21us/step - loss: 1.1327 - acc: 0.6637 - val_loss: 1.1573 - val_acc: 0.6200\n",
      "Epoch 23/120\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 1.0931 - acc: 0.6752 - val_loss: 1.1198 - val_acc: 0.6250\n",
      "Epoch 24/120\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 1.0563 - acc: 0.6839 - val_loss: 1.0876 - val_acc: 0.6310\n",
      "Epoch 25/120\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 1.0222 - acc: 0.6911 - val_loss: 1.0552 - val_acc: 0.6380\n",
      "Epoch 26/120\n",
      "7500/7500 [==============================] - 0s 21us/step - loss: 0.9900 - acc: 0.6997 - val_loss: 1.0283 - val_acc: 0.6390\n",
      "Epoch 27/120\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.9605 - acc: 0.7052 - val_loss: 0.9997 - val_acc: 0.6470\n",
      "Epoch 28/120\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.9327 - acc: 0.7125 - val_loss: 0.9766 - val_acc: 0.6460\n",
      "Epoch 29/120\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.9072 - acc: 0.7187 - val_loss: 0.9516 - val_acc: 0.6620\n",
      "Epoch 30/120\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.8828 - acc: 0.7241 - val_loss: 0.9307 - val_acc: 0.6650\n",
      "Epoch 31/120\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.8601 - acc: 0.7280 - val_loss: 0.9103 - val_acc: 0.6660\n",
      "Epoch 32/120\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.8386 - acc: 0.7368 - val_loss: 0.8915 - val_acc: 0.6750\n",
      "Epoch 33/120\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.8195 - acc: 0.7399 - val_loss: 0.8744 - val_acc: 0.6800\n",
      "Epoch 34/120\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.8014 - acc: 0.7419 - val_loss: 0.8597 - val_acc: 0.6800\n",
      "Epoch 35/120\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.7843 - acc: 0.7476 - val_loss: 0.8441 - val_acc: 0.6850\n",
      "Epoch 36/120\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.7683 - acc: 0.7496 - val_loss: 0.8298 - val_acc: 0.6910\n",
      "Epoch 37/120\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.7538 - acc: 0.7549 - val_loss: 0.8184 - val_acc: 0.6950\n",
      "Epoch 38/120\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.7397 - acc: 0.7580 - val_loss: 0.8057 - val_acc: 0.6930\n",
      "Epoch 39/120\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.7262 - acc: 0.7592 - val_loss: 0.7953 - val_acc: 0.7000\n",
      "Epoch 40/120\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.7141 - acc: 0.7628 - val_loss: 0.7843 - val_acc: 0.7040\n",
      "Epoch 41/120\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 0.7021 - acc: 0.7669 - val_loss: 0.7771 - val_acc: 0.7050\n",
      "Epoch 42/120\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 0.6915 - acc: 0.7687 - val_loss: 0.7665 - val_acc: 0.7130\n",
      "Epoch 43/120\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.6807 - acc: 0.7735 - val_loss: 0.7596 - val_acc: 0.7080\n",
      "Epoch 44/120\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.6711 - acc: 0.7761 - val_loss: 0.7497 - val_acc: 0.7170\n",
      "Epoch 45/120\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 0.6612 - acc: 0.7788 - val_loss: 0.7430 - val_acc: 0.7220\n",
      "Epoch 46/120\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 0.6526 - acc: 0.7827 - val_loss: 0.7369 - val_acc: 0.7150\n",
      "Epoch 47/120\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.6438 - acc: 0.7821 - val_loss: 0.7292 - val_acc: 0.7310\n",
      "Epoch 48/120\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.6354 - acc: 0.7869 - val_loss: 0.7241 - val_acc: 0.7220\n",
      "Epoch 49/120\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.6276 - acc: 0.7897 - val_loss: 0.7188 - val_acc: 0.7290\n",
      "Epoch 50/120\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.6201 - acc: 0.7916 - val_loss: 0.7116 - val_acc: 0.7270\n",
      "Epoch 51/120\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.6125 - acc: 0.7945 - val_loss: 0.7065 - val_acc: 0.7290\n",
      "Epoch 52/120\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.6054 - acc: 0.7952 - val_loss: 0.7024 - val_acc: 0.7280\n",
      "Epoch 53/120\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 0.5988 - acc: 0.7995 - val_loss: 0.6977 - val_acc: 0.7340\n",
      "Epoch 54/120\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.5922 - acc: 0.8020 - val_loss: 0.6958 - val_acc: 0.7360\n",
      "Epoch 55/120\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.5861 - acc: 0.8031 - val_loss: 0.6919 - val_acc: 0.7390\n",
      "Epoch 56/120\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.5797 - acc: 0.8044 - val_loss: 0.6861 - val_acc: 0.7410\n",
      "Epoch 57/120\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.5735 - acc: 0.8067 - val_loss: 0.6834 - val_acc: 0.7380\n",
      "Epoch 58/120\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.5676 - acc: 0.8103 - val_loss: 0.6798 - val_acc: 0.7420\n",
      "Epoch 59/120\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.5622 - acc: 0.8112 - val_loss: 0.6775 - val_acc: 0.7390\n",
      "Epoch 60/120\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.5566 - acc: 0.8133 - val_loss: 0.6726 - val_acc: 0.7450\n",
      "Epoch 61/120\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.5514 - acc: 0.8157 - val_loss: 0.6708 - val_acc: 0.7460\n",
      "Epoch 62/120\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 0.5461 - acc: 0.8167 - val_loss: 0.6681 - val_acc: 0.7450\n",
      "Epoch 63/120\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 0.5411 - acc: 0.8197 - val_loss: 0.6660 - val_acc: 0.7490\n",
      "Epoch 64/120\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.5358 - acc: 0.8196 - val_loss: 0.6615 - val_acc: 0.7410\n",
      "Epoch 65/120\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.5311 - acc: 0.8237 - val_loss: 0.6631 - val_acc: 0.7440\n",
      "Epoch 66/120\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.5265 - acc: 0.8223 - val_loss: 0.6587 - val_acc: 0.7470\n",
      "Epoch 67/120\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.5216 - acc: 0.8260 - val_loss: 0.6557 - val_acc: 0.7510\n",
      "Epoch 68/120\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.5174 - acc: 0.8295 - val_loss: 0.6536 - val_acc: 0.7520\n",
      "Epoch 69/120\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.5128 - acc: 0.8296 - val_loss: 0.6508 - val_acc: 0.7510\n",
      "Epoch 70/120\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.5080 - acc: 0.8316 - val_loss: 0.6514 - val_acc: 0.7490\n",
      "Epoch 71/120\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.5043 - acc: 0.8336 - val_loss: 0.6485 - val_acc: 0.7560\n",
      "Epoch 72/120\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.4996 - acc: 0.8340 - val_loss: 0.6470 - val_acc: 0.7560\n",
      "Epoch 73/120\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.4958 - acc: 0.8365 - val_loss: 0.6463 - val_acc: 0.7510\n",
      "Epoch 74/120\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.4916 - acc: 0.8360 - val_loss: 0.6455 - val_acc: 0.7570\n",
      "Epoch 75/120\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.4876 - acc: 0.8393 - val_loss: 0.6422 - val_acc: 0.7610\n",
      "Epoch 76/120\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.4835 - acc: 0.8405 - val_loss: 0.6404 - val_acc: 0.7600\n",
      "Epoch 77/120\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.4797 - acc: 0.8427 - val_loss: 0.6389 - val_acc: 0.7590\n",
      "Epoch 78/120\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.4759 - acc: 0.8408 - val_loss: 0.6377 - val_acc: 0.7600\n",
      "Epoch 79/120\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.4718 - acc: 0.8453 - val_loss: 0.6377 - val_acc: 0.7610\n",
      "Epoch 80/120\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.4683 - acc: 0.8453 - val_loss: 0.6361 - val_acc: 0.7610\n",
      "Epoch 81/120\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.4645 - acc: 0.8464 - val_loss: 0.6361 - val_acc: 0.7600\n",
      "Epoch 82/120\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.4607 - acc: 0.8472 - val_loss: 0.6332 - val_acc: 0.7620\n",
      "Epoch 83/120\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.4573 - acc: 0.8499 - val_loss: 0.6344 - val_acc: 0.7600\n",
      "Epoch 84/120\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.4537 - acc: 0.8513 - val_loss: 0.6333 - val_acc: 0.7650\n",
      "Epoch 85/120\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.4505 - acc: 0.8512 - val_loss: 0.6303 - val_acc: 0.7670\n",
      "Epoch 86/120\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.4473 - acc: 0.8523 - val_loss: 0.6324 - val_acc: 0.7590\n",
      "Epoch 87/120\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.4435 - acc: 0.8525 - val_loss: 0.6292 - val_acc: 0.7670\n",
      "Epoch 88/120\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.4399 - acc: 0.8545 - val_loss: 0.6303 - val_acc: 0.7640\n",
      "Epoch 89/120\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.4365 - acc: 0.8568 - val_loss: 0.6276 - val_acc: 0.7640\n",
      "Epoch 90/120\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.4331 - acc: 0.8568 - val_loss: 0.6277 - val_acc: 0.7670\n",
      "Epoch 91/120\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.4301 - acc: 0.8591 - val_loss: 0.6303 - val_acc: 0.7570\n",
      "Epoch 92/120\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.4271 - acc: 0.8601 - val_loss: 0.6274 - val_acc: 0.7680\n",
      "Epoch 93/120\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.4238 - acc: 0.8605 - val_loss: 0.6253 - val_acc: 0.7710\n",
      "Epoch 94/120\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.4203 - acc: 0.8615 - val_loss: 0.6259 - val_acc: 0.7630\n",
      "Epoch 95/120\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.4175 - acc: 0.8629 - val_loss: 0.6237 - val_acc: 0.7690\n",
      "Epoch 96/120\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.4144 - acc: 0.8632 - val_loss: 0.6223 - val_acc: 0.7730\n",
      "Epoch 97/120\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.4111 - acc: 0.8647 - val_loss: 0.6229 - val_acc: 0.7670\n",
      "Epoch 98/120\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.4081 - acc: 0.8680 - val_loss: 0.6242 - val_acc: 0.7690\n",
      "Epoch 99/120\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.4052 - acc: 0.8679 - val_loss: 0.6222 - val_acc: 0.7640\n",
      "Epoch 100/120\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.4021 - acc: 0.8687 - val_loss: 0.6220 - val_acc: 0.7700\n",
      "Epoch 101/120\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.3990 - acc: 0.8712 - val_loss: 0.6236 - val_acc: 0.7690\n",
      "Epoch 102/120\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.3962 - acc: 0.8727 - val_loss: 0.6215 - val_acc: 0.7730\n",
      "Epoch 103/120\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.3932 - acc: 0.8735 - val_loss: 0.6220 - val_acc: 0.7660\n",
      "Epoch 104/120\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.3907 - acc: 0.8741 - val_loss: 0.6225 - val_acc: 0.7730\n",
      "Epoch 105/120\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.3877 - acc: 0.8753 - val_loss: 0.6225 - val_acc: 0.7670\n",
      "Epoch 106/120\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.3848 - acc: 0.8751 - val_loss: 0.6210 - val_acc: 0.7720\n",
      "Epoch 107/120\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.3821 - acc: 0.8775 - val_loss: 0.6215 - val_acc: 0.7690\n",
      "Epoch 108/120\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.3794 - acc: 0.8784 - val_loss: 0.6200 - val_acc: 0.7720\n",
      "Epoch 109/120\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.3767 - acc: 0.8779 - val_loss: 0.6198 - val_acc: 0.7740\n",
      "Epoch 110/120\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.3740 - acc: 0.8803 - val_loss: 0.6209 - val_acc: 0.7730\n",
      "Epoch 111/120\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.3711 - acc: 0.8807 - val_loss: 0.6256 - val_acc: 0.7680\n",
      "Epoch 112/120\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.3690 - acc: 0.8821 - val_loss: 0.6203 - val_acc: 0.7700\n",
      "Epoch 113/120\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.3659 - acc: 0.8841 - val_loss: 0.6183 - val_acc: 0.7740\n",
      "Epoch 114/120\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.3633 - acc: 0.8840 - val_loss: 0.6173 - val_acc: 0.7740\n",
      "Epoch 115/120\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.3610 - acc: 0.8837 - val_loss: 0.6227 - val_acc: 0.7730\n",
      "Epoch 116/120\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.3584 - acc: 0.8863 - val_loss: 0.6190 - val_acc: 0.7780\n",
      "Epoch 117/120\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.3557 - acc: 0.8877 - val_loss: 0.6194 - val_acc: 0.7730\n",
      "Epoch 118/120\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.3532 - acc: 0.8888 - val_loss: 0.6241 - val_acc: 0.7680\n",
      "Epoch 119/120\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.3509 - acc: 0.8900 - val_loss: 0.6221 - val_acc: 0.7760\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 120/120\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.3483 - acc: 0.8900 - val_loss: 0.6184 - val_acc: 0.7760\n"
     ]
    }
   ],
   "source": [
    "#Code provided; note the extra validation parameter passed.\n",
    "model_val = model.fit(train_final,\n",
    "                    label_train_final,\n",
    "                    epochs=120,\n",
    "                    batch_size=256,\n",
    "                    validation_data=(val, label_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retrieving Performance Results: the `history` dictionary\n",
    "\n",
    "The dictionary `history` contains four entries this time: one per metric that was being monitored during training and during validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['val_loss', 'val_acc', 'loss', 'acc'])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_val_dict = model_val.history\n",
    "model_val_dict.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7500/7500 [==============================] - 0s 30us/step\n"
     ]
    }
   ],
   "source": [
    "results_train = model.evaluate(train_final, label_train_final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1500/1500 [==============================] - 0s 32us/step\n"
     ]
    }
   ],
   "source": [
    "results_test = model.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.345060640112559, 0.8922666666984558]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.6544460379282634, 0.7566666666666667]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the result isn't exactly the same as before. Note that this because the training set is slightly different! We remove 1000 instances for validation!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plotting the Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's plot the result similarly to what we have done in the previous lab. This time though, let's include the training and the validation loss in the same plot. We'll do the same thing for the training and validation accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.clf()\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "loss_values = model_val_dict['loss']\n",
    "val_loss_values = model_val_dict['val_loss']\n",
    "\n",
    "epochs = range(1, len(loss_values) + 1)\n",
    "plt.plot(epochs, loss_values, 'g', label='Training loss')\n",
    "plt.plot(epochs, val_loss_values, 'blue', label='Validation loss')\n",
    "\n",
    "plt.title('Training & validation loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.clf()\n",
    "\n",
    "acc_values = model_val_dict['acc'] \n",
    "val_acc_values = model_val_dict['val_acc']\n",
    "\n",
    "plt.plot(epochs, acc_values, 'r', label='Training acc')\n",
    "plt.plot(epochs, val_acc_values, 'blue', label='Validation acc')\n",
    "plt.title('Training & validation accuracy')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We observe an interesting pattern here: although the training accuracy keeps increasing when going through more epochs, and the training loss keeps decreasing, the validation accuracy and loss seem to be reaching a status quo around the 60th epoch. This means that we're actually **overfitting** to the train data when we do as many epochs as we were doing. Luckily, you learned how to tackle overfitting in the previous lecture! For starters, it does seem clear that we are training too long. So let's stop training at the 60th epoch first (so-called \"early stopping\") before we move to more advanced regularization techniques!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Early Stopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 7500 samples, validate on 1000 samples\n",
      "Epoch 1/60\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 1.9468 - acc: 0.1603 - val_loss: 1.9379 - val_acc: 0.1780\n",
      "Epoch 2/60\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 1.9314 - acc: 0.1869 - val_loss: 1.9241 - val_acc: 0.1970\n",
      "Epoch 3/60\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 1.9176 - acc: 0.2073 - val_loss: 1.9099 - val_acc: 0.2250\n",
      "Epoch 4/60\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 1.9033 - acc: 0.2293 - val_loss: 1.8946 - val_acc: 0.2390\n",
      "Epoch 5/60\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 1.8876 - acc: 0.2520 - val_loss: 1.8780 - val_acc: 0.2660\n",
      "Epoch 6/60\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 1.8701 - acc: 0.2693 - val_loss: 1.8594 - val_acc: 0.2800\n",
      "Epoch 7/60\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 1.8502 - acc: 0.2917 - val_loss: 1.8382 - val_acc: 0.2910\n",
      "Epoch 8/60\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 1.8271 - acc: 0.3097 - val_loss: 1.8144 - val_acc: 0.3110\n",
      "Epoch 9/60\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 1.8005 - acc: 0.3307 - val_loss: 1.7865 - val_acc: 0.3340\n",
      "Epoch 10/60\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 1.7696 - acc: 0.3539 - val_loss: 1.7550 - val_acc: 0.3660\n",
      "Epoch 11/60\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 1.7351 - acc: 0.3848 - val_loss: 1.7199 - val_acc: 0.3910\n",
      "Epoch 12/60\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 1.6970 - acc: 0.4161 - val_loss: 1.6821 - val_acc: 0.4190\n",
      "Epoch 13/60\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 1.6550 - acc: 0.4477 - val_loss: 1.6407 - val_acc: 0.4430\n",
      "Epoch 14/60\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 1.6103 - acc: 0.4747 - val_loss: 1.5967 - val_acc: 0.4630\n",
      "Epoch 15/60\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 1.5632 - acc: 0.5021 - val_loss: 1.5512 - val_acc: 0.4860\n",
      "Epoch 16/60\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 1.5143 - acc: 0.5280 - val_loss: 1.5047 - val_acc: 0.5170\n",
      "Epoch 17/60\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 1.4642 - acc: 0.5544 - val_loss: 1.4576 - val_acc: 0.5390\n",
      "Epoch 18/60\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 1.4136 - acc: 0.5785 - val_loss: 1.4104 - val_acc: 0.5570\n",
      "Epoch 19/60\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 1.3631 - acc: 0.5993 - val_loss: 1.3649 - val_acc: 0.5750\n",
      "Epoch 20/60\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 1.3134 - acc: 0.6141 - val_loss: 1.3168 - val_acc: 0.5870\n",
      "Epoch 21/60\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 1.2649 - acc: 0.6345 - val_loss: 1.2731 - val_acc: 0.5940\n",
      "Epoch 22/60\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 1.2182 - acc: 0.6477 - val_loss: 1.2286 - val_acc: 0.6220\n",
      "Epoch 23/60\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 1.1736 - acc: 0.6620 - val_loss: 1.1880 - val_acc: 0.6350\n",
      "Epoch 24/60\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 1.1313 - acc: 0.6696 - val_loss: 1.1504 - val_acc: 0.6370\n",
      "Epoch 25/60\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 1.0917 - acc: 0.6801 - val_loss: 1.1141 - val_acc: 0.6470\n",
      "Epoch 26/60\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 1.0547 - acc: 0.6876 - val_loss: 1.0804 - val_acc: 0.6600\n",
      "Epoch 27/60\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 1.0197 - acc: 0.6935 - val_loss: 1.0483 - val_acc: 0.6640\n",
      "Epoch 28/60\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.9875 - acc: 0.7013 - val_loss: 1.0201 - val_acc: 0.6670\n",
      "Epoch 29/60\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.9576 - acc: 0.7081 - val_loss: 0.9939 - val_acc: 0.6760\n",
      "Epoch 30/60\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.9300 - acc: 0.7132 - val_loss: 0.9694 - val_acc: 0.6840\n",
      "Epoch 31/60\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.9042 - acc: 0.7181 - val_loss: 0.9459 - val_acc: 0.6860\n",
      "Epoch 32/60\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.8803 - acc: 0.7229 - val_loss: 0.9234 - val_acc: 0.6910\n",
      "Epoch 33/60\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.8580 - acc: 0.7293 - val_loss: 0.9051 - val_acc: 0.6930\n",
      "Epoch 34/60\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.8371 - acc: 0.7319 - val_loss: 0.8865 - val_acc: 0.6970\n",
      "Epoch 35/60\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.8183 - acc: 0.7395 - val_loss: 0.8710 - val_acc: 0.6970\n",
      "Epoch 36/60\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.8007 - acc: 0.7429 - val_loss: 0.8573 - val_acc: 0.7080\n",
      "Epoch 37/60\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.7834 - acc: 0.7459 - val_loss: 0.8396 - val_acc: 0.7100\n",
      "Epoch 38/60\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 0.7681 - acc: 0.7497 - val_loss: 0.8286 - val_acc: 0.7170\n",
      "Epoch 39/60\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 0.7536 - acc: 0.7529 - val_loss: 0.8158 - val_acc: 0.7140\n",
      "Epoch 40/60\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 0.7398 - acc: 0.7571 - val_loss: 0.8041 - val_acc: 0.7210\n",
      "Epoch 41/60\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.7269 - acc: 0.7588 - val_loss: 0.7918 - val_acc: 0.7240\n",
      "Epoch 42/60\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.7143 - acc: 0.7651 - val_loss: 0.7831 - val_acc: 0.7250\n",
      "Epoch 43/60\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.7025 - acc: 0.7665 - val_loss: 0.7721 - val_acc: 0.7270\n",
      "Epoch 44/60\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 0.6918 - acc: 0.7693 - val_loss: 0.7658 - val_acc: 0.7200\n",
      "Epoch 45/60\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.6811 - acc: 0.7728 - val_loss: 0.7571 - val_acc: 0.7300\n",
      "Epoch 46/60\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.6715 - acc: 0.7733 - val_loss: 0.7516 - val_acc: 0.7300\n",
      "Epoch 47/60\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.6619 - acc: 0.7788 - val_loss: 0.7419 - val_acc: 0.7310\n",
      "Epoch 48/60\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.6528 - acc: 0.7801 - val_loss: 0.7334 - val_acc: 0.7370\n",
      "Epoch 49/60\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.6441 - acc: 0.7815 - val_loss: 0.7290 - val_acc: 0.7330\n",
      "Epoch 50/60\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.6362 - acc: 0.7843 - val_loss: 0.7216 - val_acc: 0.7350\n",
      "Epoch 51/60\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.6281 - acc: 0.7860 - val_loss: 0.7167 - val_acc: 0.7340\n",
      "Epoch 52/60\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.6208 - acc: 0.7900 - val_loss: 0.7126 - val_acc: 0.7370\n",
      "Epoch 53/60\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.6133 - acc: 0.7923 - val_loss: 0.7066 - val_acc: 0.7410\n",
      "Epoch 54/60\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.6064 - acc: 0.7948 - val_loss: 0.7024 - val_acc: 0.7440\n",
      "Epoch 55/60\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.5991 - acc: 0.7971 - val_loss: 0.6974 - val_acc: 0.7450\n",
      "Epoch 56/60\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.5926 - acc: 0.7980 - val_loss: 0.6940 - val_acc: 0.7450\n",
      "Epoch 57/60\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.5857 - acc: 0.8015 - val_loss: 0.6932 - val_acc: 0.7490\n",
      "Epoch 58/60\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.5802 - acc: 0.8021 - val_loss: 0.6884 - val_acc: 0.7510\n",
      "Epoch 59/60\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.5736 - acc: 0.8032 - val_loss: 0.6822 - val_acc: 0.7490\n",
      "Epoch 60/60\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.5681 - acc: 0.8065 - val_loss: 0.6810 - val_acc: 0.7530\n"
     ]
    }
   ],
   "source": [
    "random.seed(123)\n",
    "model = models.Sequential()\n",
    "model.add(layers.Dense(50, activation='relu', input_shape=(2000,))) #2 hidden layers\n",
    "model.add(layers.Dense(25, activation='relu'))\n",
    "model.add(layers.Dense(7, activation='softmax'))\n",
    "\n",
    "model.compile(optimizer='SGD',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "final_model = model.fit(train_final,\n",
    "                    label_train_final,\n",
    "                    epochs=60,\n",
    "                    batch_size=256,\n",
    "                    validation_data=(val, label_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, you can use the test set to make label predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7500/7500 [==============================] - 0s 30us/step\n"
     ]
    }
   ],
   "source": [
    "results_train = model.evaluate(train_final, label_train_final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1500/1500 [==============================] - 0s 29us/step\n"
     ]
    }
   ],
   "source": [
    "results_test = model.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.5629033799012502, 0.8097333333015442]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.7116736135482788, 0.7266666665077209]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We've significantly reduced the variance, so this is already pretty good! Our test set accuracy is slightly worse, but this model will definitely be more robust than the 120 epochs one we fitted before.\n",
    "\n",
    "Now, let's see what else we can do to improve the result!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## L2 Regularization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's include L2 regularization. You can easily do this in keras adding the argument kernel_regulizers.l2 and adding a value for the regularization parameter lambda between parentheses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 7500 samples, validate on 1000 samples\n",
      "Epoch 1/120\n",
      "7500/7500 [==============================] - 0s 41us/step - loss: 2.5842 - acc: 0.1768 - val_loss: 2.5653 - val_acc: 0.1990\n",
      "Epoch 2/120\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 2.5546 - acc: 0.2189 - val_loss: 2.5386 - val_acc: 0.2370\n",
      "Epoch 3/120\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 2.5278 - acc: 0.2551 - val_loss: 2.5115 - val_acc: 0.2700\n",
      "Epoch 4/120\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 2.4979 - acc: 0.2889 - val_loss: 2.4794 - val_acc: 0.2970\n",
      "Epoch 5/120\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 2.4605 - acc: 0.3253 - val_loss: 2.4398 - val_acc: 0.3210\n",
      "Epoch 6/120\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 2.4140 - acc: 0.3645 - val_loss: 2.3919 - val_acc: 0.3580\n",
      "Epoch 7/120\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 2.3607 - acc: 0.3964 - val_loss: 2.3395 - val_acc: 0.3920\n",
      "Epoch 8/120\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 2.3029 - acc: 0.4301 - val_loss: 2.2805 - val_acc: 0.4180\n",
      "Epoch 9/120\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 2.2422 - acc: 0.4613 - val_loss: 2.2226 - val_acc: 0.4390\n",
      "Epoch 10/120\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 2.1808 - acc: 0.4864 - val_loss: 2.1647 - val_acc: 0.4690\n",
      "Epoch 11/120\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 2.1202 - acc: 0.5197 - val_loss: 2.1072 - val_acc: 0.4890\n",
      "Epoch 12/120\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 2.0620 - acc: 0.5432 - val_loss: 2.0513 - val_acc: 0.5170\n",
      "Epoch 13/120\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 2.0054 - acc: 0.5721 - val_loss: 2.0007 - val_acc: 0.5230\n",
      "Epoch 14/120\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 1.9523 - acc: 0.5860 - val_loss: 1.9495 - val_acc: 0.5490\n",
      "Epoch 15/120\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 1.9011 - acc: 0.6057 - val_loss: 1.9033 - val_acc: 0.5680\n",
      "Epoch 16/120\n",
      "7500/7500 [==============================] - 0s 21us/step - loss: 1.8531 - acc: 0.6261 - val_loss: 1.8617 - val_acc: 0.5850\n",
      "Epoch 17/120\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 1.8081 - acc: 0.6356 - val_loss: 1.8198 - val_acc: 0.6020\n",
      "Epoch 18/120\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 1.7664 - acc: 0.6497 - val_loss: 1.7792 - val_acc: 0.6190\n",
      "Epoch 19/120\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 1.7271 - acc: 0.6588 - val_loss: 1.7437 - val_acc: 0.6280\n",
      "Epoch 20/120\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 1.6904 - acc: 0.6725 - val_loss: 1.7131 - val_acc: 0.6330\n",
      "Epoch 21/120\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 1.6561 - acc: 0.6811 - val_loss: 1.6810 - val_acc: 0.6360\n",
      "Epoch 22/120\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 1.6245 - acc: 0.6896 - val_loss: 1.6520 - val_acc: 0.6470\n",
      "Epoch 23/120\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 1.5949 - acc: 0.6981 - val_loss: 1.6242 - val_acc: 0.6570\n",
      "Epoch 24/120\n",
      "7500/7500 [==============================] - 0s 21us/step - loss: 1.5670 - acc: 0.7057 - val_loss: 1.5984 - val_acc: 0.6620\n",
      "Epoch 25/120\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 1.5411 - acc: 0.7115 - val_loss: 1.5734 - val_acc: 0.6680\n",
      "Epoch 26/120\n",
      "7500/7500 [==============================] - 0s 21us/step - loss: 1.5167 - acc: 0.7183 - val_loss: 1.5521 - val_acc: 0.6760\n",
      "Epoch 27/120\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 1.4939 - acc: 0.7252 - val_loss: 1.5317 - val_acc: 0.6740\n",
      "Epoch 28/120\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 1.4728 - acc: 0.7284 - val_loss: 1.5091 - val_acc: 0.6830\n",
      "Epoch 29/120\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 1.4524 - acc: 0.7345 - val_loss: 1.4930 - val_acc: 0.6810\n",
      "Epoch 30/120\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 1.4338 - acc: 0.7361 - val_loss: 1.4755 - val_acc: 0.6880\n",
      "Epoch 31/120\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 1.4155 - acc: 0.7421 - val_loss: 1.4576 - val_acc: 0.6900\n",
      "Epoch 32/120\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 1.3984 - acc: 0.7463 - val_loss: 1.4419 - val_acc: 0.7000\n",
      "Epoch 33/120\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 1.3823 - acc: 0.7519 - val_loss: 1.4326 - val_acc: 0.6890\n",
      "Epoch 34/120\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 1.3675 - acc: 0.7543 - val_loss: 1.4184 - val_acc: 0.6920\n",
      "Epoch 35/120\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 1.3526 - acc: 0.7569 - val_loss: 1.4021 - val_acc: 0.7030\n",
      "Epoch 36/120\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 1.3390 - acc: 0.7596 - val_loss: 1.3883 - val_acc: 0.7090\n",
      "Epoch 37/120\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 1.3256 - acc: 0.7631 - val_loss: 1.3820 - val_acc: 0.7020\n",
      "Epoch 38/120\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 1.3125 - acc: 0.7637 - val_loss: 1.3686 - val_acc: 0.7080\n",
      "Epoch 39/120\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 1.3006 - acc: 0.7685 - val_loss: 1.3560 - val_acc: 0.7130\n",
      "Epoch 40/120\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 1.2890 - acc: 0.7717 - val_loss: 1.3461 - val_acc: 0.7200\n",
      "Epoch 41/120\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 1.2778 - acc: 0.7727 - val_loss: 1.3353 - val_acc: 0.7200\n",
      "Epoch 42/120\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 1.2669 - acc: 0.7765 - val_loss: 1.3291 - val_acc: 0.7160\n",
      "Epoch 43/120\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 1.2565 - acc: 0.7789 - val_loss: 1.3189 - val_acc: 0.7160\n",
      "Epoch 44/120\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 1.2460 - acc: 0.7813 - val_loss: 1.3133 - val_acc: 0.7180\n",
      "Epoch 45/120\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 1.2363 - acc: 0.7832 - val_loss: 1.3027 - val_acc: 0.7250\n",
      "Epoch 46/120\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 1.2264 - acc: 0.7859 - val_loss: 1.2968 - val_acc: 0.7230\n",
      "Epoch 47/120\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 1.2176 - acc: 0.7883 - val_loss: 1.2881 - val_acc: 0.7250\n",
      "Epoch 48/120\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 1.2086 - acc: 0.7892 - val_loss: 1.2818 - val_acc: 0.7350\n",
      "Epoch 49/120\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 1.1998 - acc: 0.7928 - val_loss: 1.2732 - val_acc: 0.7290\n",
      "Epoch 50/120\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 1.1913 - acc: 0.7915 - val_loss: 1.2672 - val_acc: 0.7310\n",
      "Epoch 51/120\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 1.1829 - acc: 0.7949 - val_loss: 1.2602 - val_acc: 0.7310\n",
      "Epoch 52/120\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 1.1747 - acc: 0.7989 - val_loss: 1.2533 - val_acc: 0.7370\n",
      "Epoch 53/120\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 1.1671 - acc: 0.7995 - val_loss: 1.2480 - val_acc: 0.7360\n",
      "Epoch 54/120\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 1.1591 - acc: 0.8005 - val_loss: 1.2434 - val_acc: 0.7330\n",
      "Epoch 55/120\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 1.1518 - acc: 0.8019 - val_loss: 1.2372 - val_acc: 0.7370\n",
      "Epoch 56/120\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 1.1447 - acc: 0.8037 - val_loss: 1.2353 - val_acc: 0.7310\n",
      "Epoch 57/120\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 1.1369 - acc: 0.8085 - val_loss: 1.2269 - val_acc: 0.7400\n",
      "Epoch 58/120\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 1.1300 - acc: 0.8083 - val_loss: 1.2223 - val_acc: 0.7410\n",
      "Epoch 59/120\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 1.1231 - acc: 0.8087 - val_loss: 1.2214 - val_acc: 0.7390\n",
      "Epoch 60/120\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7500/7500 [==============================] - 0s 19us/step - loss: 1.1164 - acc: 0.8112 - val_loss: 1.2148 - val_acc: 0.7420\n",
      "Epoch 61/120\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 1.1101 - acc: 0.8112 - val_loss: 1.2086 - val_acc: 0.7400\n",
      "Epoch 62/120\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 1.1030 - acc: 0.8137 - val_loss: 1.2039 - val_acc: 0.7420\n",
      "Epoch 63/120\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 1.0965 - acc: 0.8161 - val_loss: 1.1978 - val_acc: 0.7470\n",
      "Epoch 64/120\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 1.0899 - acc: 0.8192 - val_loss: 1.1932 - val_acc: 0.7400\n",
      "Epoch 65/120\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 1.0838 - acc: 0.8201 - val_loss: 1.1881 - val_acc: 0.7450\n",
      "Epoch 66/120\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 1.0776 - acc: 0.8207 - val_loss: 1.1864 - val_acc: 0.7440\n",
      "Epoch 67/120\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 1.0711 - acc: 0.8221 - val_loss: 1.1812 - val_acc: 0.7430\n",
      "Epoch 68/120\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 1.0657 - acc: 0.8243 - val_loss: 1.1801 - val_acc: 0.7480\n",
      "Epoch 69/120\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 1.0594 - acc: 0.8272 - val_loss: 1.1734 - val_acc: 0.7470\n",
      "Epoch 70/120\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 1.0539 - acc: 0.8292 - val_loss: 1.1723 - val_acc: 0.7460\n",
      "Epoch 71/120\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 1.0479 - acc: 0.8304 - val_loss: 1.1660 - val_acc: 0.7520\n",
      "Epoch 72/120\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 1.0422 - acc: 0.8295 - val_loss: 1.1643 - val_acc: 0.7510\n",
      "Epoch 73/120\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 1.0368 - acc: 0.8297 - val_loss: 1.1608 - val_acc: 0.7530\n",
      "Epoch 74/120\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 1.0318 - acc: 0.8351 - val_loss: 1.1551 - val_acc: 0.7530\n",
      "Epoch 75/120\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 1.0258 - acc: 0.8341 - val_loss: 1.1523 - val_acc: 0.7520\n",
      "Epoch 76/120\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 1.0202 - acc: 0.8376 - val_loss: 1.1516 - val_acc: 0.7570\n",
      "Epoch 77/120\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 1.0157 - acc: 0.8412 - val_loss: 1.1454 - val_acc: 0.7620\n",
      "Epoch 78/120\n",
      "7500/7500 [==============================] - 0s 28us/step - loss: 1.0102 - acc: 0.8385 - val_loss: 1.1421 - val_acc: 0.7610\n",
      "Epoch 79/120\n",
      "7500/7500 [==============================] - 0s 22us/step - loss: 1.0049 - acc: 0.8407 - val_loss: 1.1464 - val_acc: 0.7510\n",
      "Epoch 80/120\n",
      "7500/7500 [==============================] - 0s 21us/step - loss: 1.0003 - acc: 0.8407 - val_loss: 1.1358 - val_acc: 0.7580\n",
      "Epoch 81/120\n",
      "7500/7500 [==============================] - 0s 27us/step - loss: 0.9953 - acc: 0.8427 - val_loss: 1.1328 - val_acc: 0.7630\n",
      "Epoch 82/120\n",
      "7500/7500 [==============================] - 0s 21us/step - loss: 0.9903 - acc: 0.8432 - val_loss: 1.1300 - val_acc: 0.7630\n",
      "Epoch 83/120\n",
      "7500/7500 [==============================] - 0s 30us/step - loss: 0.9850 - acc: 0.8452 - val_loss: 1.1286 - val_acc: 0.7640\n",
      "Epoch 84/120\n",
      "7500/7500 [==============================] - 0s 21us/step - loss: 0.9805 - acc: 0.8449 - val_loss: 1.1250 - val_acc: 0.7710\n",
      "Epoch 85/120\n",
      "7500/7500 [==============================] - 0s 22us/step - loss: 0.9757 - acc: 0.8487 - val_loss: 1.1238 - val_acc: 0.7610\n",
      "Epoch 86/120\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.9711 - acc: 0.8468 - val_loss: 1.1206 - val_acc: 0.7630\n",
      "Epoch 87/120\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 0.9664 - acc: 0.8480 - val_loss: 1.1180 - val_acc: 0.7660\n",
      "Epoch 88/120\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.9615 - acc: 0.8501 - val_loss: 1.1132 - val_acc: 0.7710\n",
      "Epoch 89/120\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.9567 - acc: 0.8503 - val_loss: 1.1112 - val_acc: 0.7710\n",
      "Epoch 90/120\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.9526 - acc: 0.8527 - val_loss: 1.1098 - val_acc: 0.7700\n",
      "Epoch 91/120\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.9483 - acc: 0.8537 - val_loss: 1.1078 - val_acc: 0.7710\n",
      "Epoch 92/120\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.9437 - acc: 0.8553 - val_loss: 1.1039 - val_acc: 0.7690\n",
      "Epoch 93/120\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.9393 - acc: 0.8543 - val_loss: 1.1032 - val_acc: 0.7720\n",
      "Epoch 94/120\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.9350 - acc: 0.8567 - val_loss: 1.0979 - val_acc: 0.7720\n",
      "Epoch 95/120\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.9307 - acc: 0.8569 - val_loss: 1.0983 - val_acc: 0.7710\n",
      "Epoch 96/120\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.9261 - acc: 0.8595 - val_loss: 1.0930 - val_acc: 0.7740\n",
      "Epoch 97/120\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 0.9222 - acc: 0.8591 - val_loss: 1.0916 - val_acc: 0.7750\n",
      "Epoch 98/120\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.9181 - acc: 0.8589 - val_loss: 1.0896 - val_acc: 0.7760\n",
      "Epoch 99/120\n",
      "7500/7500 [==============================] - 0s 22us/step - loss: 0.9139 - acc: 0.8633 - val_loss: 1.0859 - val_acc: 0.7770\n",
      "Epoch 100/120\n",
      "7500/7500 [==============================] - 0s 22us/step - loss: 0.9099 - acc: 0.8637 - val_loss: 1.0849 - val_acc: 0.7730\n",
      "Epoch 101/120\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 0.9060 - acc: 0.8643 - val_loss: 1.0824 - val_acc: 0.7760\n",
      "Epoch 102/120\n",
      "7500/7500 [==============================] - 0s 21us/step - loss: 0.9020 - acc: 0.8657 - val_loss: 1.0810 - val_acc: 0.7770\n",
      "Epoch 103/120\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 0.8977 - acc: 0.8659 - val_loss: 1.0800 - val_acc: 0.7730\n",
      "Epoch 104/120\n",
      "7500/7500 [==============================] - 0s 21us/step - loss: 0.8938 - acc: 0.8656 - val_loss: 1.0758 - val_acc: 0.7770\n",
      "Epoch 105/120\n",
      "7500/7500 [==============================] - 0s 27us/step - loss: 0.8901 - acc: 0.8681 - val_loss: 1.0718 - val_acc: 0.7780\n",
      "Epoch 106/120\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 0.8864 - acc: 0.8704 - val_loss: 1.0721 - val_acc: 0.7740\n",
      "Epoch 107/120\n",
      "7500/7500 [==============================] - 0s 21us/step - loss: 0.8823 - acc: 0.8687 - val_loss: 1.0694 - val_acc: 0.7810\n",
      "Epoch 108/120\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 0.8787 - acc: 0.8720 - val_loss: 1.0694 - val_acc: 0.7740\n",
      "Epoch 109/120\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 0.8750 - acc: 0.8703 - val_loss: 1.0694 - val_acc: 0.7720\n",
      "Epoch 110/120\n",
      "7500/7500 [==============================] - 0s 22us/step - loss: 0.8710 - acc: 0.8745 - val_loss: 1.0667 - val_acc: 0.7740\n",
      "Epoch 111/120\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.8672 - acc: 0.8755 - val_loss: 1.0652 - val_acc: 0.7710\n",
      "Epoch 112/120\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.8637 - acc: 0.8735 - val_loss: 1.0605 - val_acc: 0.7740\n",
      "Epoch 113/120\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.8600 - acc: 0.8757 - val_loss: 1.0575 - val_acc: 0.7730\n",
      "Epoch 114/120\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.8565 - acc: 0.8775 - val_loss: 1.0555 - val_acc: 0.7790\n",
      "Epoch 115/120\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.8529 - acc: 0.8759 - val_loss: 1.0556 - val_acc: 0.7740\n",
      "Epoch 116/120\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.8497 - acc: 0.8775 - val_loss: 1.0525 - val_acc: 0.7750\n",
      "Epoch 117/120\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.8460 - acc: 0.8809 - val_loss: 1.0517 - val_acc: 0.7770\n",
      "Epoch 118/120\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.8428 - acc: 0.8804 - val_loss: 1.0476 - val_acc: 0.7770\n",
      "Epoch 119/120\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.8390 - acc: 0.8809 - val_loss: 1.0483 - val_acc: 0.7770\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 120/120\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.8360 - acc: 0.8827 - val_loss: 1.0457 - val_acc: 0.7750\n"
     ]
    }
   ],
   "source": [
    "from keras import regularizers\n",
    "random.seed(123)\n",
    "model = models.Sequential()\n",
    "model.add(layers.Dense(50, activation='relu',kernel_regularizer=regularizers.l2(0.005), input_shape=(2000,))) #2 hidden layers\n",
    "model.add(layers.Dense(25, kernel_regularizer=regularizers.l2(0.005), activation='relu'))\n",
    "model.add(layers.Dense(7, activation='softmax'))\n",
    "\n",
    "model.compile(optimizer='SGD',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "L2_model = model.fit(train_final,\n",
    "                    label_train_final,\n",
    "                    epochs=120,\n",
    "                    batch_size=256,\n",
    "                    validation_data=(val, label_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['val_loss', 'val_acc', 'loss', 'acc'])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "L2_model_dict = L2_model.history\n",
    "L2_model_dict.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at the training accuracy as well as the validation accuracy for both the L2 and the model without regularization (for 120 epochs)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAEWCAYAAACJ0YulAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4yLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvOIA7rQAAIABJREFUeJzs3Xd4FVX6wPHve9NJQgotJBASeu8gSJHeIh1dFUGwYFkLu66rsMryU1xXXbtYEFEWFEVcBRGk9957CyWkE1JIb/ee3x/nAkkIEMolCZzP89wnd2bOzD1z72TemXPOnCNKKQzDMAwDwFLaGTAMwzDKDhMUDMMwjAtMUDAMwzAuMEHBMAzDuMAEBcMwDOMCExQMwzCMC0xQKCNExElE0kUk+GamLetEZLaITLa/7yYiB0qS9jo+57b5zoxb70aOvfLGBIXrZD/BnH/ZRCSrwPTIa92eUsqqlPJSSp2+mWmvh4i0E5GdIpImIkdEpJcjPqcopdRqpVSTm7EtEVkvImMKbNuh39mdoOh3WmB+IxH5TUQSRCRJRBaLSL1SyKJxE5igcJ3sJxgvpZQXcBoYWGDed0XTi4jzrc/ldfsMWABUBPoD0aWbHeNyRMQiIqX9f+wD/A9oAFQDdgO/3MoMlNX/rzLy+1yTcpXZ8kREpojIjyIyR0TSgIdFpKOIbBaRFBGJFZGPRcTFnt5ZRJSIhNinZ9uXL7ZfsW8SkdBrTWtf3l9EjorIORH5REQ2FHfFV0AeEKG0E0qpQ1fZ12Mi0q/AtKv9irG5/Z9inojE2fd7tYg0usx2eonIqQLTbURkt32f5gBuBZZVEpFF9qvTZPuVapB92dtAR+AL+53bh8V8Z7727y1BRE6JyAQREfuyx0VkjYh8YM/zCRHpc4X9f9WeJk1EDojIoCLLnxSRw/bl+0WkhX1+LRH51Z6HsyLykX3+FBH5tsD6dUVEFZheLyJviMgmIAMItuf5kP0zjovI40XyMMz+XaaKSLiI9BGRB0VkS5F0fxeRny+3r8VRSm1WSn2jlEpSSuUBHwBNRMSnmO+qk4hEFzxRish9IrLT/r6D6LvUVBGJF5F3i/vM88eKiEwUkTjgK/v8QSKyx/67rReRpgXWaVvgePpBRH6Si0WXj4vI6gJpCx0vRT77sseeffklv8+1fJ+lzQQFxxoKfI++kvoRyAdeACoDnYB+wJNXWP8h4DXAH3038sa1phWRqsBc4CX7554E2l8l39uA986fvEpgDvBggen+QIxSaq99eiFQDwgA9gOzrrZBEXED5gMz0Ps0HxhSIIkFfSIIBmqhA9lHAEqpl4FNwFP2O7fxxXzEZ0AFoDbQA3gMGF1g+d3APqAS+iT39RWyexT9e/oAbwLfi0g1+348CLwKjETfeQ0DkkRf2f4OhAMhQE3071RSo4BH7duMAuKBMPv0E8AnItLcnoe70d/ji4Av0B2IAH4FGkjhop5RwH+vIR/F6QpEKaXOFbNsI/q3uqfAvIfQ/ycAnwDvKqUqAnWBeVf4nBqAF/oYeEZE2qGPicfRv9sMYL79IsUNvb/T0cfTzxQ+nq7FZY+9Aor+PuWHUsq8bvAFnAJ6FZk3BVh5lfX+Bvxkf+8MKCDEPj0b+KJA2kHA/utI+yiwrsAyAWKBMZfJ08PooDAAfTC3sM/vB2y5zDoNgXOAu336R2DiZdJWtufds0DeJ9vf9wJO2d/3ACIBKbDu1vNpi9luWyChwPT6gvtY8DsDXNABun6B5X8GltvfPw4cLrCson3dyiU8HvYDYfb3K4A/F5OmCxAHOBWzbArwbYHpuvpftdC+TbpKHhae/1x0QHv3Mum+Av7P/r4lcBZwuUzaQt/pZdIEAzHAfVdI829gmv29L5AJ1LBPbwQmAZWu8jm9gGzAtci+/LNIuuPogN0DOF1k2eYCx97jwOrijpeix2kJj70r/j5l+WXuFBwrsuCEiDQUkd/tRSmpwOvok+TlxBV4n4m+KrrWtIEF86H0UXulK5cXgHeUUovQJ8ol9juGTsDK4lZQSh1G//OFiYgXcC/2Kz/RrX7esRevpKKvjOHK+30+31H2/J4Xcf6NiHiJyHQROW3f7soSbPO8qoBTwe3Z3wcVmC76fcJlvn8RGVOgyCIFHSTP56Um+rspqiY6AFpLmOeiih5b94rIFtHFdilAnxLkAWAm+i4G9AXBj0oXAV0z+13pUuAjpdRPV0j6PTBcdNHpcPTFxvljcizQGDgiIltFZMAVthOvlMotMF0LePn872D/Hqqjf9dALj3uI7kOJTz2rmvbZYEJCo5VtAvaL9FXkXWVvj2ehL5yd6RY9G02ACIiFD75FeWMvpJGKTUfeBlYji5amXqF9c4XIQ0FdiulTtnnj0bfdfRAF6/UPZ+Va8m3XcGy2ZeAUKC9/bvsUSTtlbr/PQNY0SeRgtu+5gp1EakNfA48jb669QUOc3H/IoE6xawaCdQSEadilmWgi7bOCygmTcE6Bg90MctbQDV7HpaWIA8opdbbt9EJXYxz1aK94ohIJfRxMk8p9faV0ipdrBgH9KVw0RFKqSNKqQfQgfs94GcRcb/cpopMR6LvenwLvCoopeaij6eix33NAu9L8p2fd7Vjr7i8lRsmKNxa3uhilgzRla1Xqk+4WRYCrUVkoL0c+wWgyhXS/wRMFpFm9srAw0AOupL3SifyOei6hHEU+CdH73MOkIj+p3uzhPleD1hE5Fl7pd/9QOsi280Eku0npElF1o9H1xdcwn4lPA/4l/2qLxT4C7qI4Fp5oU8ACeiY+wT6TuG86cDfRaSVaPVEpCa6ziPRnocKIuJhPzGDbr1zj4jUFBFf4JWr5MENcLXnwSoi9wI9Cyz/GnhcRLqLrvivISINCiyfhQ5s6UqpzVf5LBcRcS/wcrFXKC9FF5e+epX1z/se/Z13pEC9gYiMEpHKSikb+n9FAbYSbvMr4M+im1SL/bcdKCKe6OPJWUSeth9Pw4E2BdbdAzS3H/cewD+v8DlXO/bKNRMUbq0XgUeANPRdw4+O/kClVDzwJ+B99EmoDrALfaIuztvoisYF9nx+jg4kc4DfRaTiZT4nCtgOdKBwhek36DLmGOAAusy4JPnOQd91PAEk29//WiDJ++g7j0T7NhcX2cSHwIP2YoT3i/mIZ4BcdH3QGnQxyjVXsNqvej9B13fEoptlbimwfA76O/0RSEU33fRTSuWji9kaoa9wTwMj7Kv9gW7Suc++3QVXyUMK+gT7C5Bk387CAss3or/Hj9En2lUUvkr+L9CUkt0lTAOyCry+sn9ea3TgKfj8TuAVtvM9+gp7mVIqucD8AcAh0S32/gP8qUgR0WXZA9rT6GM2Gd0A4GH7svPH01P2ZfcDi7D/HyilDgL/AlYDR4C1V/ioqx175ZoULrI1bnf24ooYYIRSal1p58coffYr6TNAU6XUydLOz60iIjuAD5VS11Vkdrsydwp3ABHpJ7pdvhu62Woe+grUMEA3KNhwuwcE0d2oVLMXHz2GLuZbUtr5KmvK5FOAxk3XGX277owuwhlqv5027nAiEoW+SBhc2nm5BRqhi/E80a2xhiulzpRulsoeU3xkGIZhXGCKjwzDMIwLyl3xUeXKlVVISEhpZ8MwDKNc2bFjx1ml1JWaowPlMCiEhISwffv20s6GYRhGuSIiEVdPZYqPDMMwjAJMUDAMwzAucGhQsLePPyK6//ZLHtUX3Z/8ChHZK7qf/aJ93RiGYRi3kMOCgv3J2ano/nAao7scaFwk2X+A/yqlmqN7DH3LUfkxDMMwrs6RdwrtgXClR+7KBX7g0gdkGnOxO+ZVxSw3DMMwbiFHBoUgCvcpHsWlXdfuQY9EBbqzKm97r4OGYRhGKSjtiua/obsI3oUeni8a3c99ISIyTkS2i8j2hISEW51HwzCMO4Yjn1OIpnD3vDUoMoiJUioG+52CfcSu4fZugCmSbhq6y17atm1r+uUwDOP2pBQkJkJkJEREwKlTkJR0cfnAgdCunUOz4MigsA2oZx/AJBp4AD3K0gUiUhlIsg+oMQE90LZhGMbtLz0d1q6F9evh0CE4cgROnoTs7EvTin18q8DA8hsUlFL5IvIsumtaJ2CGUuqAiLwObFdKLQC6AW+JiEIPavFnR+XHMAzD4fLy4MQJfYLftw+2boWdOyEzE5ycwMUFXF3168QJyM8HZ2eoVw9b/Xrk9e2JS606WGrWRNWqRUaNqpy2pLEnfi974/cyrFEbHBsSHNzNhX3w90VF5k0q8H4eBYbiMwzDKHOsVjh2DKKjITZWn8h9fMBigc2bYfVqvTwzE7KyCq/boAHccw/4+ZGfm0N2VhqeOCPZOTBkCLk97mF7qBuzjs7jxwM/kpydjJwTKmRWIOtAFjZ1cSRSF4sLtf1q0y6onN4pGIZhlGk5Obr8/vwrKkqX4Z85o6/kXVzgwAFdxHPuXPHbcHYmo1VTTt3TmGTnfFKc83AKrUNA2244N27K+nP7WB+5nl2xKziaeBSrslLBpQKNKjciI28fx7a8h3WzFQ9nD4Y2GkqrgFak5aSRnptOBZcKVHSrSHXv6jSv1pyGlRvi6uTq8K+l3I2n0LZtW2U6xDMMo8Ti42HFCtixQxfrHD2q56WmFp/e1xeVlwfZ2eQFB+Hcsw/SuTMHK2Qw/9xW0lQ2LTxCqCSefJy5koUxqwGwiAUfNx+Ss5MLba66V3XaBbWjedXmVPeuztHEoxxMOIinqydNqjShebXm9K/bH283b4d+DSKyQynV9mrpzJ2CYRjlW3o6LFgAa9bo1jvOzpCRoa/4T5+Ggwd1Ond3qF8f1aI5tsDq2CpVQlWuhFOlquDvxyG3NJbkHWJp7Do2RW4iLdcKnMbdeTa+ZxcSlx6Hm5Mbnq6eJMXoFkGB3oH8u+e/eajZQ1T3ro6zxZkzGWfYFr2NlOwUOtbsSKhvKHK+orgcMHcKhmGUbUrB2bO63P7cOV3sk5ysr/oPHtR3AZmZ4OeHcncnPyeLbFcLmX6epPpV4HijAHY3rcyWqrkcSjrK8eTj5NvyC32ERSwXyu+bVm1K1+CudAruRJ41jz3xe4hNj6VfnX4MaTiEim4ViUmLIeJcBG0D296SIp2bwdwpGIZR9mVlwe7d+oo+MlKf/DMy9NV/fLyu2I2I0EGgCJuLM9nBgaQN60vKsAHsq1eR97Z8wOaozfYU+mrexXICnzwfqqVUo2nVpgxtOBQvVy+cLc4oFLnWXHKtuTSu0pgeoT0I8Aq4araDKgYRVLFoBw23BxMUDMNwLKsV9uyBdetg/36w2fTV/+HDsH27bsZpZ3NxBk9PxMubLD9v4ryFqA7Viapel1NVXdmSeZTI7ARS3eCUbz5Wp9PAadj7C+yFUN9Qpg6Yyr3178XL1QtPF0/cnN1Kb9/LIRMUDMO4dnl5urWOzabb32dl6Sv96Gh9lZ+To6/0t23TFbzp6QCoKlXIc7GQk59Dgr87u/rVYl1QPmtdYjjumUuqez5wDidJx6qiAKhcoTIezh64ObvRKqAbT4b2pE1gG/KseWTkZZBnzSPflo+HiwfdQrrhbDGntRthvj3DMK5MKX2CX7YMlizRJ/nwcN1e/wqsLs5E1a7M9vZebKrpzZbarhxwS73QOsfX3ZeaFasQ7BNMj8rDea5KEzxdPYlKjSI+PZ5m1ZpxT617qOlT84qfY9xcJigYxp0uORm++UaX34O+yo+K0q+4ON2KJycHgEw/byKaBpHctjNnqnuzIXYrCWnx5DpBpA/E+lhIcbGR7QxZLvm4uqbQNrAt1b2qU9PiTCNXL+6ueTfdQroR4htSevtsXJYJCoZxu0tJ0Q9l7dtH9rpVpG1eiyUwEN+7e+KUmob68kskPZ18D132bnN2IsnPg9PeVk5Xzye2toVT7sKqWordAWkoy2HgMABdenTh8dbv0DawLXvtXTFUdKtIiG8Idf3r0rxa83LTOsfQTJNUw7gd5OXpIh13d/D21i165syB+fP107p2qW6wMwAC0qF+EihgblPhrU6KfQUa3bg7u9MqoBW1/Wrj4+aDn4cfLQNa0j6oPUHeQeRac7EpG56unrd+X43rYpqkGsbtKD1dd7B28KB+n56uy/hXr75QmXtelrszu+8KZolXNvsrZHCmhi9te47mgZYjOZgaxdS9CzhzNoJ6jTrxVo2OVPWsCuiA0LByQ1ycXC6bDQ+LhyP30ihFJigYRlmTnw8bN+qyfqsV4uOxbd1CzuYNuB89gdhshZKnBwcQN6ADO0PdWX5sCd7ZNtxC6rKgdh5x1nP0CB3AI81H0a9uvwsn+vZB7RnWaFhxn27c4UxQMIyyYvdumDlTF/vExxdalFgBtgXC1i6wNQj2BwjJbopMF7BZ4oA4AB546AEm9nyLEN8Q/lUKu2CUfyYoGMattns3fPaZPvE3aIDy9ydv7hxcd+0l38WJ7a0DmN7Pj51uydichJBaLQhp3pV2Qe0ZXKUxL/iG4OvuS1JWErHpseRZ83BzdsPX3ZdA78DS3jujnDNBwTAcJScHVq6E+fOxxsdxJieR7IgThB6MIcvVQnQlF2r+vgA3KxyuCl/1h++aWalY3ZmONfvyfJ2+hNULo4pnlWI3X6lCJSpVqHSLd8q43ZmgYBg3w9mzupfOtWvh5ElUXBy2w4dwSksn08OZkxVtiNVGrovw78GVWNurHm5VAvBz9iYox42qtZsxsEpDJlRtaq72jVJlgoJhXIvMTN1nT2wsREZi3bKZ7PWr8QyPACDP3ZXYAE9OuGVyqEEOv9WHQy2q0KNBf4Y3Hk7P0J60dHbjlVLeDcO4HIcGBRHpB3yEHqN5ulLq30WWBwMzAV97mlfsQ3gaRpmglGLVsWWs++lduqw8TsdNUXhkXezALcUDNtWAjT1gVShsD8zFx8ubnrUH0zO0Jx+FdKeuf91y1Z++cWdzWFAQESdgKtAbiAK2icgCpdTBAsleBeYqpT4Xkcbo8ZxDHJUnw7ii3FyIjkZFRHB23xYS9m4iY+Ma2h9LoUceZLgKv7X0ZGUTX85V8iKzqi/Bje+mV53ePF29Nc9anBCEal7VsIiltPfGMK6LI+8U2gPhSqkTACLyAzAYKBgUFFDR/t4HiHFgfgwDgJTsFGLTYklPisN58RLc12yg2pb9+MalYFEgQBXA1wLHqjlzckhXGg4dh+fAwdzv5cX9pb0DhuFAjgwKQUBkgeko4K4iaSYDS0XkOcAT6FXchkRkHDAOIDg4+KZn1Lj9KaVYG7GWaRs+Zse2+YzZbuXJHeCXDSlusCYUTjT3wVojEJdatanavCP1WvWkeVBr03ePcUcp7YrmB4FvlVLviUhHYJaINFVKFXpkUyk1DZgGuu+jUsinUU7kWnNZfGwxUalR1PENJTg2k5gfpuO3dA3tTmdzj723Z5tFiO/bif2PPohP9/6E+dU0/fAbBo4NCtFAwY7Qa9jnFfQY0A9AKbVJRNyBysAZB+bLuE3YlI1DCYc4mniU2PRYDscd4Pgf33H3vnN0Pg2tYqFiLjQGjgZ7ceJPfajfqBOuVQKw9O5N9dBQqpf2ThhGGePIoLANqCcioehg8ADwUJE0p4GewLci0ghwBxIcmCejnItIiWDJ8SUsO7GM1adWczbzLN7Z8MIWeG0LVMkEm5OF/FYtONs1hEN1KhM0bAz1m91d2lk3jHLBYUFBKZUvIs8CS9DNTWcopQ6IyOvAdqXUAuBF4CsR+Qu60nmMKm99eRsOk2vNZVv0NjZEbmBn7E52xO4gJTKcsbvg+Qg3JvpUwde/PTW3HsY5JRUVFgajRmHp2xdXX18CAfMYmHGjcvJz2Bm7kw41OtwRTYvNeApGmZFvy2dr9FZWn1rNqlOr2HB6A1n5WQSmwsiYSgw94Ub7nfE45VtRLVogFgukpkKzZvDaa9C6dWnvglGKrDYrCZkJxKTFUKNijQtdgZ+XnZ9NbFosiVmJNKvaDDdntwvL9p/Zz+Gzh4lO1SXcf2r6JwK8AjiaeJQH5j3ArrhdDG4wmK8HfX1dXYvYlI2vdnzFqlOreLLNk3QP7a4XhIfDsmXYhg7hqPM5EjIuLSjx9/CnYeWGOFmcrvlzCyrpeAomKBilSinFqZRTfLP7G77e9TUxaTGgYExqbR4+7UvbvWfxOXZaJ65eHe67D558Eho3Lt2MG6XCarOy+eQ6th5bw0mVSHRaNNGp0USnRRObFotVWQFwsbgwvPFw/tTkT2yL3savR37lYIJuDe+TBW6VqvBEm3HU9qvNF9u/4OSxbSS7g9V+3nWxuBBWP4xlx5fh5uzG6OajmbptKtW8qvFOh0nck+RN9YgkzrpZ2eeSRGrdYHq0GU5Ft4rkWfNYd3odRxOPUsPiR61VO1m5fS4nk08hri5EeORRq14bHtpto/Xi3TjZFBku8GEH+M/d+oFIAK8cmDEfBh3R0yLC4Ul/pvlrn1zXd2eCglEmpeak8tuR3/jp4E/sittFbFosebY8BKFfnb68nNmau79eisvW7eDqCp07Q9++0L8/NG0Kd8Dte3kQnRrNR1s+4um2TxPqF3rZdPHp8Xy05SOiUqMAcLY4E+AVQJB3EPm2fKLToknOSqZJ1Sa0C2xHZvIZTvzwObJ1K9+1duFYgAvuzu4EegdSN9eLRr+s5+FNGVRLh0+7uPLfIaFUqlyTIK9AQlyrUbVqCAFeAaw/vZ7vtn5NxwOpZLlZCGrYjrCsmnT+bTfVdoeztXU1wnrGk+gBb+2rwt8WJJLbsC45n39KfMOarPn8ZZrP+B0Xz4rUHvtX/Lr0Jvp/M0n6YQaNI3NwKnLazHCBTztYWHN/e/YmHyb0ZApDD8Gju8A3p/jvJtcJZnX0ZGPnYJ7amEe7teHkVfQiYtz9JPTuRNMX3sTz2CmODevGac4Rmx5LyCPj6fqnl67rNzNBwShTYtJieHv920zbOY3s/GyCvIPoHtqd5uc86LY6gqaRWXgcOKq7k65ZEyZOhIcfBi+v0s66UcTCowsZ8+sYss4lMiA9gGlVn8AvMBTGjLkQtM9sXMbOb97iTZeNbKqWR7BvLQByrDn4nIpjwGEbewJgbX1XKrpV5GzGWf61Av6yCdz1xT5ZFVyY9lJPdjT2pctPWxg1/xTueYrYTi2oFNwA1zlzITQU6teHrVvh3Dmdh3/+E06cwDZuHJZjxwpnvk4dfZExfTr5lfzIqFMTn/Xb9bx9+3SfVo0awcGDqNBQxM1N93Vlp+5qT1zHFuyp5coW/ywaugbSyloF37m/UXXBcrJcLbjlKyw2hXJ2Junenhy5rwcNu43Av4K/7jk3JgZrVCSqWVOcQ2pfzNuePfDqq7BwoZ728YG5c6FPn5vyu5mgYJQam7Kx/vR6lh1fxtnMs5zJPMOiY4vIs+YxusVoHm8+hg4HU7F89jksWgTOzrpeoHlz6NoVRo4EN7erf5BxXY4l6hNlUMUgKrhUKDZNvi2fnw78RHhSOK6ZOXhFn+GgTx5Hs6PYs3c57++swoPrU3DKvdgPVNzAHvz3+a6kzJvNq1+HU8H+TEhe9Wq41K2vJ+Lj4ehRAJSLC/z6KzJgAKlTXqPia1OICbuHyn+ZiGud+jB0KOzdq0/8x4/D8OHw1ltQr57e1po18OKLeqS69u31cTRjBthseszq0FB4/33w94foaKhSBXr0AIsFdu2CBx6AiAid5umndf3Uq6/CunV6+tFHwcVFB4Xdu/WxGXiFpgt79sDnn0PVqjo/HTpA5crX/gNt2qQHW3rhBR2gbhITFIxbKs+ax6aoTSw6togf9v9AxLkInMQJfw9/aqmKDFeNeNSvB1WPxcD330NMDFSrBk89pV8BAVf/EKPkDh3SA/n89BN4ekJQEMl+HqzKO8bG/JP45EBQKjTI8aQdQbjGn4WcHBSQZ1FEVsjnVIVcAtKhUQJYgHwLnAj0oNbZfFxzrcgjj3CwS0PCjk3mga1ZvLUCwv2gbjJENg0mf9oXhIafhd9+012Lg85L377QvTuMHq3Hmh4/Hv79b7j/fj3qnMXeb1R6Oowdq8eg/ugjGDjw6vsdGalP8r6+8NJLUKH4oAdAdrYe8rT6nfG0igkKhsOdTD7JkuNLWHJ8CStOrCAtNw0ncaJX7V6MrTOCIdvTcVvwO6xapccaBnByggED9G3+vffqegPj2mzbBtOn6ytkgE6d9AnW2Vlf+T7/PCxYoL/bwYNJtWYSdXgrLnEJBKVBhTz9RHeGvxdH3TNJ8nenU/sRUKECf4T/QfTZk9TP86atCsSvem24qz3Uq4fl8BFdTFO1KvzjH9CgAQDbY7az/MRy+u7JoOUrHyDDhsOXX4K7+5X34+xZuOceHRg6dYLly4tfRylTl3QTmKBgOMSqk6uYuWcmq0+tJuKcHkOglk8t+tbpS7+6/ehRsys+3/8MkybpooL69fVtf5cu+na+Vi3w8CjlvSijoqNh82b9Nz5elykHBenvrWVLfcJ87z3UxInkuFrI86qAi1XhnniOuOreLGvqwYg1Z1EC3/UPYktYS1J93Pnfof/h7uzO+A7j+VvHF/HNtegrdmdnNkZupM+sPtT0qcm57HMkZSXxbu93eabdM9fXBDIvTxe5lFRsrC5yef756ytqMUrMBAXjpopIieBvy/7GvIPz8Pfwp3tId7qFdKN37d7Uz6uIzJ2rryI3bNBXq506wbvv6nLVO/UqTynYsIGszz/B6a4OuD43/tLvwmrVZeOffQa//nrhjspmESy2i/+byskJVT0AS1Q0PzeGxwfamy4qGHgE3l7tTKO4fLa2DmDa2Gac8L7Ysmdks5FM6DLhknb75608uZIB3w0gxDeEH0f8SIuAFo76RoxSZIKCcVPsjd/Lx1s+Zvbe2VjEwsQuE3mx44t4uHjo4otPP9V3BWlpUKMGtGunWw0NHXpnBAOlYPZs+OEHfYUfG3uhDD8vORGXA4fIdQJXK5x5aDBVv5lLSlYyi754kcAV22i7LRqvlAxyfbxJemgIK9tW5vWImRyRJLxyITAN6idCu2hoEQ+/1wPrY2N5u887KKWISYvB38OfGl6BSHS0brl1Hd/7+e24O1+lyMcot0xQMG7IqZRTPLf4ORYeXYiHswejW4xmYpeJBCdZ9WD0W7fqv+Hh+hntJppOAAAgAElEQVSC99+Hhg1LO9s3R2ys3r9t28DbG9vgQSyScHKXLKLe3BV4pWWT3rsbPl164/Kvt6i+9RARVd1IqVUV56BgrGnnIDqatKwUZrUQnB56mEbTf+XZ5anE1PDBK+EcFXMg3U1YVBf+11AxvyFk20tdetfuzevdX6e6V/ULD2fFpMUQnxFPv7r96Fqra+l+P0a5ZIKCcc2sNisnkk/wy+Ff+L81/4cgTOwykaeaP4r/59/Cjz/qpnmgW3e0awfPPAODB5ebu4KsvCy+3vX1hWcl6vrXpXWlpjjt3QeLFpE59zsqHArXiZ2cLhTnJLmDfzac8YRYb2gRp5OkuMG/B/qy89427IjfRVJWEhax0KRKE7oEd+HFu1+ktl9tUrJTmPlSH7rO20Z8o5rUe+wl6gx/AqurC2cyzlw4+Vf1rErHmh1L6dsxbmcmKBglFp4UzvOLn2flyZXkWPXjl4MaDOLT/p9S08lPVxQvXQp33w3DhkFYmK5AtpSdIScXHFnAkwufZFzrcUzoMqHYYpAVJ1bw1O9PEZ4UDgpG7oXntkLLOHCzgk1gfU1Y0AAO1fdj5Kh3WbH9Rzx+X8aT+S0I+dOTeI0cS44zHNr+B+eWL8QvbATNW/ZFRFBKEZkaib+HP16ulz50p5QiLj2O6t53RhNIo2wxQcG4qqy8LD7Z+gn/XP1P3JzceLTVozSr2oyWAS1pVb0VJCbqALB9u24COWZMaWe5MKVgzBiiMuNp2GQlXl7+xGfE08cawijX9iz1S2RlzmFc07NoEZWPR2IqlqAgxnd7hRZTf8Zl5WqSG9RiXQMP5nme4kTzmjzY4wVaBrTkL0v+wraYbQBMHTCVZ9o9U8o7axg3pqRBAaVUuXq1adNGGTdmafhS9dDPDymvf3kpJqOG/jBURadGX0xw6JBS48cr5eenlJubUr/+WnqZLeDAmQNqZ8xOlZufq5RSKmvOLKV0aFBrm3irxLOR6uCkZ1S2Exfmp3q7XXhf6FWxolKffaaU1aqUUspmsxX6rHxrvpq+Y7r65dAvt3w/DcMR0EMWXPUca8YfvINk5GYw/o/xTN81nUoelXiw6YM82PTBi934RkbCK6/oJ45dXHRR0UsvQZs2pZrvPGseU5a+SvQX77CnGuyv5U59tyB+m3KcpGrwa7dq/HPuGaRRa/wTElD9+5M1/lk8Dh3De98+CAnR3Q7UqKErkc+cgW7dCj3JWrSffCeLE4+1fuzW7qhhlAEmKNwhdsft5sGfH+TI2SNM6DyByd0mXxyQPicH3nlH9ytjs+nO6F54QT+56gDRqdGMnT+WNRFrAN1zZli9MJ5t/yxdgrtcOEGfyTjD9ojN7Hn3rzz2v+MEp+pR1RY90pbcs/EEn4PIz9/mb8OfQYYs1N0lvP8+Mn48HiJQXD9ipsttw7giU6dwm7MpGx9u/pAJKyZQyaMSs4fNpkdoj4sJNm2Cxx/XXQ3cd59+4KxWLYfl50IPm/lZjGs9Dndnd5Kzk/nxwI+cy0yhuqs/NndX8m35pJ87y7y5EHYMEpvVodKbH8CsWbo/H9BdO8yc6bC8GsbtxNQpGCo8MVz1+m8vxWTU4DmDVUJGwsWF+flK/eMfSokoVbOmUr//flM+s2jZ/Hn74/eroT8MVUxGtfyipTqccLjQ8oyYCBXTur7K9HBR8x9qo17+9mF1ulVtZRNR2R++p9T57dpsSn35pVLduikVG3tT8mwYdwJKWKfg0BM40A84AoQDrxSz/ANgt/11FEi52jZNULi6zNxMNWnlJOX2hpvy/pe3+nL7l4VP1nFxSvXooX/+xx5TKjX1hj/TZrOpH/b9oALfC1SD5gy6EIDOpJ9RY38dq2SyqIpvVVSvr35dZeVlFV75wAGlatfWldq9e+t8iSjl5KTU99/fcN4MwygDQQFwAo4DtQFXYA/Q+ArpnwNmXG27Jihc2bHEY6rpZ00Vk1EP/fxQ4VZF+flKTZ+uVNWqSrm7K/XNNzflM6NTo1W/2f0Uk1FNpjZRrm+4qsD3AtUba95Qld6upJxfd1YvLnlRnc04q1c4dUqpiROV6t5dqfr1lXJ1VapaNaU2b9bLt25V6qGHlFq48KbkzzCMshEUOgJLCkxPACZcIf1GoPfVtmuCwuUtOrpI+f7bV/m/7a8WH1tceOGuXUq1aqV/8o4dldqzp8TbTclKUZNWTlKnU05fsiw5K1k1mdpEeb7pqT7c9KHKt+arXbG7VP1P6ismo+7++m61P36/Tnz8uFKDBytlsehXhw5K3XefUi++qFRExI3sumEYV1EWgsIIYHqB6VHAp5dJWwuIBZwus3wcsB3YHhwc7KCvrHz7ePPHSiaLavlFS3Ui6UThhcuWKeXlpVRgoC6OuUy5f3HSc9JVp687KSajGk9trJKzki8sy8nPUT1m9lAur7uoFSdWFFovLSlObZv7kbIm2u8O5szRzwb4+Cg1YYIJAoZxi5W3oPAy8ElJtmvuFAqz2Wzq5WUvKyajhvwwRGXkZhRO8OOPSrm4KNW0qVJRUde07ay8LNVzZk9l+T+LemXZK8rldRfVY2YPlZOfo8ITw9UD8x5QTEbN3D3z4krh4frBNx8fdeFBseBg/ffuu3XRkWEYt1xJg4Ijn1OIBmoWmK5hn1ecB4A/OzAvt6Vcay6PL3icWXtn8XTbp/mk/ycXB0bJz4fXXtPDHHbpokfi8vW96jYz8zL5dve3bIraxIbTGziZcpKZQ2YyusVoGlVpxCO/PkLwB8HEZ8QDMKX7FEa3GA1ZWTBlin7eAWDECP3w27FjejjFJ57QD8Y5m0djDKNMK0nkuJ4X+sG4E0AoFyuamxSTriFwCvszE1d7mTsFLTU7VfWZ1UcxGfXGmjcKty6KjlaqSxd9dT5unFJZWZfdTr41/8L7vXF7VaNPGykmo6r/p7oaPGewmndgXqH07218T/Wc2VN9sOkDXUxlsym1YIFSdevqz3vkEaViYm727hqGcYMo7TsFpVS+iDwLLEG3RJqhlDogIq/bM7fAnvQB4Ad7po0SiEuPI+z7MPbE7WHGoBmMbTX24sJDh6BPHz0g+ezZMHJksdtQSvHK8ld4f/P71PWvS7OqzVhwZAF+Hn4sG7WMXrV7FbveXxuM4a9ZLSETWLkT/vOQHkKyfn09xm7Png7YY8MwbhWH3ssrpRYBi4rMm1RkerIj83C7OZZ4jL6z+xKfEc+CBxcwoN6Aiws3bYJ779UDtm/YAC0uDqtotVmJToumRsUaKKV45vdnmLZzGkMbDiXfls/GyI30q9uPaQOnFT9sY2qqHkjn/ff1KGvn1agBX30FjzxybWPzGoZRJpkC3nJka/RWwr4PQxBWP7KadkHtLi7cvFlfpQcF6bEPQkMvLIpPj2foj0PZFLUJHzcfgioGcTDhIBM7T2RKjymXdAZ3iY0b9UA6Z8/qsRWefBLc3PQgNG3a6AHlDcO4LZigUE5Ep0bTb3Y//Dz8WPLwEur617248PRpGDJE9/q5fj1Uq3Zh0d74vQycM5CEjATe6P4GUalR7I3fy4d9P+SFDi9c/YOXLdPbDgqCxYuh7dW7TjEMo/wyQaEcsCkbY+ePJceawx8j/ygcEDIy9FV8VpYeM9keEPJt+Xy85WNeW/Uavu6+rBu7jjaBJegCOztbj018+rRuOfTWW3rs5aVLCwUbwzBuTyYolAOfbfuMZSeW8UXYF9SrVO/iguPH4bHHYO9eWLjwQrfQW6K28MyiZ9gZu5OwemFMGziNQO/AK3/ImjUwdaq+G0hPvzi/a1f49Vfw83PAnhmGUdaYoFDG7Yvfx0vLXmJAvQGMazNOz7RadRfX//d/unL322+hf3/2n9nPqytfZf6R+QR4BTB3xFxGNB5x9TqDiAjo3x+8veGhh3Rldf36EBio5xmGcccwQaEM2xm7k76z++Lr7svXg76+eHJ/7z2YMAGGDoVPPoGgIObsm8PDvzyMt6s3U7pP4YUOLxQ7eHyx/vpXENHFRsHBjtshwzDKPBMUyqiNkRsZ8N0AfNx9WD5qOQFeAXpBRIS+Qxg8GP73PwDmH57PqF9G0Tm4M7/86Rf8PfxL/kFLl+rtvPmmCQiGYZigUBZFpETQb3Y/ArwCWD56OcE+BU7W48frvx9/DMDS40u5f979tA1sy8IHF+LtVoLinqVLdfPSoCB47jmoWxdefNEBe2IYRnljgkIZY1M2Hl3wKABLRy0tHBAWLtSVvm+/DcHB7IrdxbAfh9GwckMWj1x89YCQn69P/vaAcsGiRfq5A8Mw7ngmKJQxn2/7nJUnV/LVwK8I8Q25uGD1ahg7VrcwGj+eyHORhH0fhr+HP4tHLsbP4zKtg6xWiI+H6GjdQd6SJboO4fHHISZG1yX06FH8uoZh3HFMUChDwpPC+fvyv9Ovbj8ea/WYnqmUrlh+5RWoVw/bz/PYdmYXT/z2BBl5Gawfu/7yzU1/+gmeegqSkvS0s7PukuLxx/V0o0aO3ynDMMoVExTKiMhzkfSb3Q9XJ1e+GvjVxZZGkybBlClkDw7jjTG1+XZRL2LSYnB3dmfBAwtoVq3ZpRvLyoK//AW+/BLat9d3GIGB0KxZoe4vDMMwijJBoQw4fe403Wd252zmWZY+vJQaFWvoBb//DlOmsKNvc+5pt4rsvX8wqMEghjUaRli9sOKLjFau1HcHx47Byy/DG2+YjuoMwygxExRKWXpuOt2+7UZSVhLLRi2jfVB7veDUKWyjHuZYzQp0bruXIQ0f4PVurxd+ormg6GiYOBH++1+oU8d0Y20YxnUxQaGUfbXjK06mnGTVI6suBoTcXPKGDyErO5XBY5z44eFfGdxwcPEbSEzU/RNNnaorlSdOhFdfBQ+PW7cThmHcNkxQKEW51lze2/Qe3UK60S2k24X5+a9OxGXnHkaNdGPqs7/Ts/ZlrviPHoVevfRdwqhR8M9/mjoDwzBuiAkKpei7vd8RnRbN14O+vjhz1Sqc/vM+X7aBMZN/vXxA2LsXevcGm02PpdCuXfHpDMMwroGltDNwp7IpG29veJuWAS3pU6ePnpmUhPXhkYRXEpb/uT/96vYrfuX9++Gee3QF8rp1JiAYhnHTODQoiEg/ETkiIuEi8spl0twvIgdF5ICIfO/I/JQl8w/P50jiEV7p9MrF5qd//SvEx/PwCGHKwA+KXzEjA+6/Xz+BvH69HuvAMAzjJnFY8ZGIOAFTgd5AFLBNRBYopQ4WSFMPmAB0Ukoli0gxgwPffpRSvLX+LWr71WZ44+F65saNMHMm73aGuwY9R4PKDYpfefx4OHxY918UEnLL8mwYxp3BkXUK7YFwpdQJABH5ARgMHCyQ5glgqlIqGUApdcaB+Skzlp1YxraYbXw18CucLc661dCf/0yCvzuf9HJlzz2Til/xhx9g+nTdwqhXr1ubacMw7giOLD4KAiILTEfZ5xVUH6gvIhtEZLOIFFuILiLjRGS7iGxPSEhwUHZvnSlrp1CjYg1GtxitZ3z5JezezZ97ZPPX3pOoXKHypSslJMDTT0PHjjB58i3Nr2EYd47Srmh2BuoB3YAHga9ExLdoIqXUNKVUW6VU2ypVqtziLN5cayPWsu70Ov5+999xdXKFhATUq6+yub4nOzqH8mz7Z4tfccIEPUzm11+bJ5QNw3AYRwaFaKBmgeka9nkFRQELlFJ5SqmTwFF0kLhtTVk7haqeVXm8tb1TuldeQaWe49FeGbzT+13cnIvpwnrLFh0Mxo83ndgZhuFQjgwK24B6IhIqIq7AA8CCIml+Rd8lICKV0cVJJxyYp1K14sQKlp1YxosdX8TDxQM2bYIZM/isszuV2nZmWKNhl65kr28gMFB3jmcYhuFADqtoVkrli8izwBLACZihlDogIq8D25VSC+zL+ojIQcAKvKSUSnRUnkpTSnYKY+ePpUGlBrqIyGqFZ57hXGVvJtydxso+719smnqeUrrLih074PvvwbsEo6oZhmHcAIc+0ayUWgQsKjJvUoH3Cvir/XVbe37x88SkxbDxsY1UcKkAn30Gu3fzzP3ODG47knZBRR5As9l0cdEnn+jxDx54oHQybhjGHcV0c3EL/HzwZ2btncWkrpN0p3e5ufCvf3G4URX+1yyVIz3/delKTz2lB8T5y1/0IDtF7yIMwzAcoLRbH932lFL8Y+U/aFGtBa92fVXPnDMHoqMZ3zqBF+/+W+FxmAF27zYBwTCMUmHuFBxs/5n9HEk8wudhn+Pi5KLrCd59l+M1PNndwpOfOr186Uqffaa7vn7tNRMQDMO4pcydgoPNOzgPi1gY2nConrF4MRw4wOR2GUzs+g+83YpUHqekwHffwUMPgV8xI6sZhmE4kLlTcLCfDv5E11pdqeZVDQD1zjsk+LmypoM/01o/cekKM2dCZiY888wtzqlhGIa5U3CoA2cOcOjsIUY0GqFnbN6MrFnD2+1yeemeifpZhYKU0kVHd90FrVvf+gwbhnHHM3cKDjTv4DwEufBQmpo0iRRvFxZ0q8S+NsXcJaxYoUdTmznzFufUMAxDK9GdgojUERE3+/tuIvJ8cX0UGYX9dPAnOgd3prp3dVi3Dlm2jCkd83ih5z9wd3YvnDg1FZ59FqpV0+MlGIZhlIKSFh/9DFhFpC4wDd2n0R0zIM71OHz2MAcSDnBf4/suPJl8zq8CMzt6MKblmMKJbTYYPRrCw3X32O7uxW7TMAzD0UoaFGxKqXxgKPCJUuoloLrjslX+fbPrG5zESQ+is2IFrF3Lm50VA1qMwMvVq3DiN9+E+fP1MwndupVKfg3DMKDkdQp5IvIg8Agw0D7P9N98GTn5OczYPYNBDQYR6B0Ib40iM6ASHzdPZOH5MRTOO3oU/vlPGDkSnn++dDJsGIZhV9I7hbFAR+BNpdRJEQkFZjkuW+Xb/w79j7OZZ3mq7VMQGwurVjH/7kpU9g+ie0j3wok/+wycneE//zEPqhmGUepKdKdgH1f5eQAR8QO8lVJvOzJj5dkXO76gjl8detXuBZ99Dkrx78DjPNz8bzhZnC4mzMiAb7+F4cMhIKDU8msYhnFeSVsfrRaRiiLiD+xEj5D2vmOzVj4dTDjI2oi1PNnmSSxigZ9+IjE0gL2VrYxqPqpw4jlz4Nw5PV6CYRhGGVDS4iMfpVQqMAz4r1LqLsCMHF+ML7d/iauTK2NajsEWE41au5ZptZNoG9iWJlWbXEyoFEydCs2aQadOpZdhwzCMAkoaFJxFpDpwP7DQgfkp16w2K7P3zWZYo2GICO+/1BlRivAerZg7Ym7hxFu26N5Qn3nG1CUYhlFmlLT10evoUdI2KKW2iUht4JjjslU+bY3eSlJWEkMaDOGL7V/QecMpUmoHMn3CpktHVfvgAz2S2sMPl05mDcMwilGiOwWl1E9KqeZKqaft0yeUUsMdm7XyZ9GxRVjEQp86fVi+YRZdT4PvqCcuDQi7d8PcufDcc+DlVfzGDMMwSkFJK5priMgvInLG/vpZRGqUYL1+InJERMJF5JVilo8RkQQR2W1/PX49O1FWLA5fTMcaHYlJi6HhhqNYFDBixKUJJ04Ef3/4+99veR4NwzCupKR1Ct8AC4BA++s3+7zLEhEnYCrQH2gMPCgijYtJ+qNSqqX9Nb3EOS9j4tPj2RG7g/51+/PjgR8ZdBTyQ4KhSZPCCdes0WMqvPIK+PiUTmYNwzAuo6RBoYpS6hulVL799S1Q5SrrtAfC7UVNucAPwOAbyGuZtuT4EgD61+3P/B3f0+uUBedBQwpXIisFEyZAYKDu/M4wDKOMKWlQSBSRh0XEyf56GEi8yjpBQGSB6Sj7vKKGi8heEZknIjWL25CIjBOR7SKyPSEhoYRZvrUWHVtEgJd+AC1kx3Fc82wwcGDhRB98AJs26W4tPDyK2YphGEbpKmlQeBTdHDUOiAVGAGNuwuf/BoQopZoDy4BiBxJQSk1TSrVVSrWtUuVqNyi3Xr4tn6XHl9Kvbj/mHpzL4KOCraI3dO16MdHPP8Pf/qbrGB4v11UnhmHcxkra+ihCKTVIKVVFKVVVKTUEuFrro2h0F9vn1bDPK7jdRKVUjn1yOtCmhPkuU7ZGbyU5O5k+tfswZ893DDnugqVff3B11Qk2b9ZNTzt0gP/+FyxmwDvDMMqmGzk7/fUqy7cB9UQkVERcgQfQldUX2B+IO28QcOgG8lNqfj74My4WF87lnKPa4Sj8z+VeLDqy2eCRR6B6dd09tik2MgyjDLuR4Tiv+BiuUipfRJ5FP/TmBMxQSh0QkdeB7UqpBcDzIjIIyAeSuDlFUrdUnjWPWXtncW/9e/loy0f8JboqynIW6d9fJ/jjD9099nffQRks+jIMwyjoRoKCumoCpRYBi4rMm1Tg/QRgwg3kodT9fux3EjITaFi5Ib8c/oX7ToYgnTpBpUo6wUcf6dZGxT2vYBiGUcZcMSiISBrFn/wFMOUgwIxdMwjwDOCP8D/oLCH4HT4FY5/WCw8dgqVLYcqUi/ULhmEYZdgV6xSUUt5KqYrFvLyVUjdyl3FbiEuPY9GxRXSt1ZVdcbt4K7eLXhAWpv9+/DG4ucG4caWXScMwjGtgmsHcgFl7ZmFVVnKsOfh7+HP3vhSoVQsaN4bkZN3SaORIU5dgGEa5YYLCdVJKMWP3DDrU6MCaiDUMDR2AZcVKfZcgAjNmQGamGXfZMIxyxQSF63Qw4SCHzx6mXfV2pGSn8FhqXT28ZliYbob6+efQpQu0aFHaWTUMwygxExSu04Ij+pGLlJwUPF08abf7DLi7Q7duunL5+HE9gI5hGEY5YoLCdZp/ZD5tqrdh2Yll9K/bD+fFS6BHD6hQAT77DKpVg2HDSjubhmEY18QEhesQlx7HlugttK7emrj0OEa7d9B3BmFhcOoULFwITzxhmqEahlHumKBwHRYe1cNU51pzcbG40Otwrl4QFgZffqkrmk0zVMMwyiETFK7D/CPzCa4YzPrT6+lVuxcey1bpwXRq1oRvv9X9HtUsthdwwzCMMs0EhWuUkZvB8hPLaRXQiuPJxxlVe6geTW3AANi7F+LiTF2CYRjllgkK12j5ieVk52dzOvU0gd6BjIjzg7w8HRT++EMn6tOndDNpGIZxnUxQuEa/Hf0NTxdPdsXt4oW7XsDlj2Xg7Q2dOumg0KoVBASUdjYNwzCuiwkK10ApxR/hf+Dv4Y+3qzdPth4HixZB796QlQUbNkC/fqWdTcMwjOtmgsI12H9mP9Fp0USnRfNE6yfwCY+EqChddLRyJeTnQ9++pZ1NwzCM62aCwjX4I1zXGQjCCx1e0HcJAP3766Ijb2/o2LEUc2gYhnFj7vjur6/F78d+x9niTFi9MIJ9gnVQaNlSD7X5xx/Qs6d5YM0wjHLNoXcKItJPRI6ISLiIvHKFdMNFRIlIW0fm50ak5aSx/vR68m35jG4xGtLTL9YhHD0KERGmPsEwjHLPYUFBRJyAqUB/oDHwoIg0LiadN/ACsMVRebkZVp5ciVVZ8Xb1JqxeGGzdClYr3HMP/PyzTmTqEwzDKOcceafQHghXSp1QSuUCPwCDi0n3BvA2kO3AvNyw872iPtjsQdyc3fRdgojuGvujj3RACAkp3UwahmHcIEcGhSAgssB0lH3eBSLSGqiplPr9ShsSkXEisl1EtickJNz8nF6FUor5R+YDMLblWD1zwwbdtcXPP8OZMzBx4i3Pl2EYxs1Waq2PRMQCvA+8eLW0SqlpSqm2Sqm2VUphaMvDZw+TmJVIVc+q3BV0lx5EZ9Mm6NAB3nkHOneGrl1veb4MwzBuNke2PooGCvYKV8M+7zxvoCmwWkQAAoAFIjJIKbXdgfm6ZrP3zgZgVPNRiAjs3w+pqXphZKTuGdUwDOM24Mg7hW1APREJFRFX4AFgwfmFSqlzSqnKSqkQpVQIsBkocwEBYO7BuQCMaTlGz9iwQf9dsUI3STWtjgzDuE047E5BKZUvIs8CSwAnYIZS6oCIvA5sV0otuPIWyobEzETCk8LxdvWmSZUmeubGjeDrCydPwvff6wpnwzCM24BDH15TSi0CFhWZN+kyabs5Mi/X6/yAOt1DuiPnT/4bNuiH1IKCYMSIUsydYRjGzWWeaL6KWXtnATCy+Ug9Iy4OTpzQ7998E1xcSilnhmEYN5/p++gK8qx5rD+9HkHoU8c+RsL5+gQ3NzPkpmEYtx0TFK5g3el15FhzaFi5Ib7uvnrm8uX678iRULly6WXOMAzDAUzx0RXMPaBbHY1oXKDeYKGuY2D8+FLIkWHcmLy8PKKiosjOLtMdCBg3wN3dnRo1auBynUXbJihcwaJjuo58YP2BekZamh4/ITAQmjUrxZwZxvWJiorC29ubkJCQiw0njNuGUorExESioqIIDQ29rm2Y4qPLSMpKIjI1kgouFWgT2EbP/O03/dd0fGeUU9nZ2VSqVMkEhNuUiFCpUqUbuhM0QeEy1kWsA6BDUAcsYv+aZumWSDz1VCnlyjBunAkIt7cb/X1NULiM8x3gDWs07OLMLVt0q6N27UopV4ZhGI5lgsJlrDy5EoC+de1FRWfOQHIyNG1qnmA2jOuUmJhIy5YtadmyJQEBAQQFBV2Yzs3NLdE2xo4dy5EjR66YZurUqXz33Xc3I8s33auvvsqHH35YaF5ERATdunWjcePGNGnShE8//bSUcmcqmouVlpNGxLkIKrpVpI5fHT3z88/13/9v7+7jqqryxY9/FoihooIcH0aokes0g2jyeAXrqDGmA0ZwQ9PIrqVjvrTUtLq/Jsd8KO3VzTStHH+a5tT8SMbRTJkLOoY06DUf8AEw1HASSyEDQxQ4BsfW7499PIFConI8wPm+X6/zOmev/bQWm9f5nr3W3t89cqTzKiZEC+fr68vhw4cBmDdvHl5eXrzwwgt1ltFao7XGza3+36xr16697n6eeeaZW6/sbeTh4cHSpUsJCQnhwoULhIaGMnz4cH7961/f9ooOQokAAB5gSURBVLpIUKjHrq93ARDpF/lT/9ymTcb7xIlOqpUQTWvG1hkc/vZwk24zpEcIS2OWXn/Bq5w4cYL4+HhCQ0M5dOgQ27dvZ/78+Rw8eBCLxcKYMWOYM8fIkGM2m3n33Xfp168fJpOJyZMnk56eTvv27dm8eTPdunVj9uzZmEwmZsyYgdlsxmw2s2PHDsrLy1m7di333nsvlZWVjBs3jqNHjxIUFERhYSGrV68mJCSkTt3mzp1LWloaFosFs9nMihUrUErx5ZdfMnnyZM6dO4e7uzsff/wxvXr14rXXXmPdunW4ubkRFxfHwoULr9v+nj170rNnTwA6depEYGAgZ86ccUpQkO6jemzI3wDAI0GPGAVWK+Tng48POOF5DkK4gmPHjjFz5kzy8/Px8/Pj9ddfJzs7m5ycHLZv305+fv4165SXlzNkyBBycnIYOHAg77//fr3b1lqzb98+Fi1axCuvvALAO++8Q48ePcjPz+fll1/m0KFD9a777LPPsn//fvLy8igvL2fr1q0AJCUlMXPmTHJycti9ezfdunUjNTWV9PR09u3bR05ODs8/f93HxVzjq6++4siRI/y7k8Yu5UyhHjsKjfGEmF/ZUmLv3Ak1NcZDdYRoJW7mF70j9e7dm4iICPv0unXrWLNmDVarlaKiIvLz8wkKqvuY93bt2hEbGwtAeHg4O3furHfbiYmJ9mUKCwsB2LVrFy+++CIAwcHB9O3bt951MzIyWLRoEZcuXaK0tJTw8HCioqIoLS3loYeMe5g8PT0B+PTTT5kwYQLt2rUDoEuXLjf0N7hw4QIjR47knXfewcvL64bWbSoSFK5iqbFw6vwpfDx9uLOz7RlBq1YZ72PHOq9iQrRyHTp0sH8uKChg2bJl7Nu3D29vbx5//PF6r71v27at/bO7uztWq7Xebd9xxx3XXaY+VVVVTJ06lYMHD+Ln58fs2bMddjd4dXU1iYmJjB8/nvj4eIfsozGk++gqu77ehUYT5V/rrOAf/wA3N0hMbHhFIUSTuXDhAh07dqRTp04UFxezbdu2Jt/Hfffdx/r1RiqbvLy8erunLBYLbm5umEwmLl68yMaNGwHw8fGha9eupNpuaL106RJVVVUMGzaM999/H4vFAsD333/fqLporXnyyScJCQlh+vTpTdG8myZB4Sof5nwIwJi+Y4yCwkL4/nsjrYXtlFAI4VhhYWEEBQURGBjIuHHjuO+++5p8H9OmTePMmTMEBQUxf/58goKC6Ny5c51lfH19eeKJJwgKCiI2NpbIyEj7vOTkZBYvXkz//v0xm82UlJQQFxdHTEwMERERhISE8NZbb9W773nz5uHv74+/vz+9evXin//8J+vWrWP79u32S3QdEQgbQ2mtnbLjmxUREaGzsx33xM6ui7ryveV7yv9QjldbL3juOXjrLeMlSfBEC3f06FH69Onj7Go0C1arFavViqenJwUFBQwfPpyCggLatGn5ver1HWel1AGtdUQDq9i1/NY3oVPnT1FaVUrYL8KMgADGpahKwVNPObdyQogmVVFRwdChQ7FarWitWblyZasICLfKoX8BpVQMsAzjGc2rtdavXzV/MvAMcBmoACZpra/t2LtNXtv5GgCTw225jaqqjO6j3r2h1iCYEKLl8/b25sCBA86uRrPjsDEFpZQ7sByIBYKAJKVU0FWLfaS1vkdrHQK8ASxxVH2uR2vNxqMbUSjGBY8zCv/0J+P9kUecVS0hhLitHDnQPAA4obX+SmtdDaQACbUX0FpfqDXZAXDaAMe+M/s4ZzlHn659uKONcfkaHxqDzjz3nLOqJYQQt5Uju4/8gG9qTZ8GIq9eSCn1DPAc0Bb4bX0bUkpNAiYB3HXXXU1eUYBle5cB8GjfR42CsjI4cgT8/OQuZiGEy3D6Jala6+Va697Ai8DsBpZZpbWO0FpHdHXAF7SlxsKmY0ZuoxF3jzAKX3oJtIann27y/QkhRHPlyKBwBriz1rS/rawhKcB/OLA+Dfr7l3/nkvUSXh5ehPQIAYvF6DpSCqZNc0aVhGiVoqOjr7n+funSpUyZMuVn17uS8qGoqIhRo0bVu8z999/P9S5XX7p0KVVVVfbpESNGcP78+cZU/bb67LPPiIuLu6Z87Nix/OY3v6Ffv35MmDCBmpqaJt+3I4PCfuBupVSAUqot8CiwpfYCSqm7a00+CBQ4sD4NSs5Lxk25Mbz3cNzd3GHNGiMw3HsvdOzojCoJ0SolJSWRkpJSpywlJYWkpKRGrd+zZ082bNhw0/u/OiikpaXh7e1909u73caOHcuxY8fIy8vDYrGwevXqJt+Hw8YUtNZWpdRUYBvGJanva62/UEq9AmRrrbcAU5VSDwA1QBnwhKPq05AySxn/U/A//Kh/JPbuWCPx3ZVUt3JvgmjFnJE6e9SoUcyePZvq6mratm1LYWEhRUVFDBo0iIqKChISEigrK6OmpoYFCxaQkFDn2hQKCwuJi4vjyJEjWCwWxo8fT05ODoGBgfbUEgBTpkxh//79WCwWRo0axfz583n77bcpKioiOjoak8lEZmYmvXr1Ijs7G5PJxJIlS+xZVidOnMiMGTMoLCwkNjYWs9nM7t278fPzY/PmzfaEd1ekpqayYMECqqur8fX1JTk5me7du1NRUcG0adPIzs5GKcXcuXMZOXIkW7duZdasWVy+fBmTyURGRkaj/r4jRoywfx4wYACnT59u1Ho3wqH3KWit04C0q8rm1Pr8rCP33xgb8jdg/dFIkBXzqxhITYVvvwV3d3BiUiohWqMuXbowYMAA0tPTSUhIICUlhdGjR6OUwtPTk02bNtGpUydKS0uJiooiPj6+wWcOr1ixgvbt23P06FFyc3MJCwuzz1u4cCFdunTh8uXLDB06lNzcXKZPn86SJUvIzMzEZDLV2daBAwdYu3Yte/fuRWtNZGQkQ4YMwcfHh4KCAtatW8d7773H6NGj2bhxI48//nid9c1mM3v27EEpxerVq3njjTdYvHgxr776Kp07dyYvLw+AsrIySkpKeOqpp8jKyiIgIKDR+ZFqq6mp4S9/+QvLli274XWvx+Vv30vOS6a9R3sCvAPw7+QPf33eSH43dKjx/AQhWilnpc6+0oV0JSisWbMGMO4VmjVrFllZWbi5uXHmzBnOnj1Ljx496t1OVlaWPXlc//796d+/v33e+vXrWbVqFVarleLiYvLz8+vMv9quXbt4+OGH7ZlaExMT2blzJ/Hx8QQEBNgfvFM79XZtp0+fZsyYMRQXF1NdXU1AQABgpNKu3V3m4+NDamoqgwcPti9zo+m1AZ5++mkGDx7MoEGDbnjd63H61UfO9E35N2SdyuIH6w/E/ioWKithyxb48Ue5YU0IB0lISCAjI4ODBw9SVVVFeHg4YCSYKykp4cCBAxw+fJju3bvfVJrqkydP8uabb5KRkUFubi4PPvjgLaW7vpJ2GxpOvT1t2jSmTp1KXl4eK1eudFh6bYD58+dTUlLCkiWOudfXpYPCpmOb0Ggu68s/dR1dugQeHvIsZiEcxMvLi+joaCZMmFBngLm8vJxu3brh4eFBZmYmp06d+tntDB48mI8++giAI0eOkJubCxhptzt06EDnzp05e/Ys6enp9nU6duzIxYsXr9nWoEGD+OSTT6iqqqKyspJNmzbd0K/w8vJy/Pz8APjggw/s5cOGDWP58uX26bKyMqKiosjKyuLkyZNA49NrA6xevZpt27bZH/fpCC4dFNIK0vD29Ka9R3vMd5nho4+My1ATE6XrSAgHSkpKIicnp05QGDt2LNnZ2dxzzz18+OGHBAYG/uw2pkyZQkVFBX369GHOnDn2M47g4GBCQ0MJDAzkscceq5N2e9KkScTExBAdHV1nW2FhYTz55JMMGDCAyMhIJk6cSGhoaKPbM2/ePB555BHCw8PrjFfMnj2bsrIy+vXrR3BwMJmZmXTt2pVVq1aRmJhIcHAwY8aMqXebGRkZ9vTa/v7+fP7550yePJmzZ88ycOBAQkJC7I8WbUoumzq7sroS3zd8uaPNHQz+5WBSH0wGk8m4+ujTT40xBSFaGUmd7RokdfZNyCzM5IfLP/DDZdt4wubNRkDo0QOu+hUhhBCuwmW7j9IK0rjD3RhA+l3v38Gf/2zMmDzZuPpICCFckEueKWitSStIo0u7Lni28aQ3PvDZZ8bM8eOdWjchhHAml/xJnF+Sz6nyU5y/dJ6hAUNh40bjMtTISHBQFlYhhGgJXDIopBUYN1lbrBYe+LcHYOVKY4btRhghhHBVrhkUTqTRw8u4S3Joh3vg4EFo2xYeftjJNRNCCOdyuaBQWV3J/379v3i4eRDcPRjTpq3GcxPi4+GqJFdCiKZ17tw5QkJCCAkJoUePHvj5+dmnq6urG7WN8ePHc/z48Z9dZvny5SQnJzdFlV2Oyw00f376c2p+rKG4opjRfUfD9FXGjJkznVsxIVyAr68vhw8bmVnnzZuHl5cXL7zwQp1ltNZorRu8Y3ft2rXX3c8zzzxz65V1US4XFD4r/Aw35Yb1RysPdgiF44vB1xcGDnR21YS4vWbMgMNNmzqbkBBYeuOJ9k6cOEF8fDyhoaEcOnSI7du3M3/+fA4ePIjFYmHMmDHMmWMkWDabzbz77rv069cPk8nE5MmTSU9Pp3379mzevJlu3boxe/ZsTCYTM2bMwGw2Yzab2bFjB+Xl5axdu5Z7772XyspKxo0bx9GjRwkKCqKwsJDVq1fbk99dMXfuXNLS0rBYLJjNZlasWIFSii+//JLJkydz7tw53N3d+fjjj+nVqxevvfaaPQ1FXFwcC6+k4m8hXK77KLMwk+4duuPh5sF96V8Yhf/5n0Z6CyGE0xw7doyZM2eSn5+Pn58fr7/+OtnZ2eTk5LB9+3by8/OvWae8vJwhQ4aQk5PDwIED7c9DuJrWmn379rFo0SJ7aoh33nmHHj16kJ+fz8svv8yhQ4fqXffZZ59l//795OXlUV5eztatWwEjVcfMmTPJyclh9+7ddOvWjdTUVNLT09m3bx85OTk8//zzTfTXuX1c6kyhsrqSfWf24dvOl4F3DqTtwr8ZM6TrSLiim/hF70i9e/cmIuKnLAzr1q1jzZo1WK1WioqKyM/PJygoqM467dq1IzY2FjDSWu/cubPebScmJtqXuZL6eteuXbz44ouAkS+pb9++9a6bkZHBokWLuHTpEqWlpYSHhxMVFUVpaSkPPfQQAJ6enoCRKnvChAn2h/DcTFpsZ3OpM4Xd3+zG+qOV7yq/I8Y3Cv71L+jaVe5NEKIZuPIsA4CCggKWLVvGjh07yM3NJSYmpt501G3btrV/biitNfyU/vrnlqlPVVUVU6dOZdOmTeTm5jJhwgSHpsVuDlwqKGQWZuKu3NFoHjpSY1x1NHy4s6slhLjKhQsX6NixI506daK4uJht27Y1+T7uu+8+1q9fD0BeXl693VMWiwU3NzdMJhMXL15k48aNgPGwnK5du5KamgrApUuXqKqqYtiwYbz//vv2R4PezFPVnM2hQUEpFaOUOq6UOqGU+kM9859TSuUrpXKVUhlKqV86sj6fFX5Gz449USgCU3cbhRMmOHKXQoibEBYWRlBQEIGBgYwbN65O+uumMm3aNM6cOUNQUBDz588nKCiIzp0711nG19eXJ554gqCgIGJjY4mMjLTPS05OZvHixfTv3x+z2UxJSQlxcXHExMQQERFBSEgIb731VpPX29EcljpbKeUOfAkMA04D+4EkrXV+rWWigb1a6yql1BTgfq11/cnFbW42dXZFdQU+/+3DnZ3uxNutPQef/9KYUVlpPFRHCBcgqbN/YrVasVqteHp6UlBQwPDhwykoKKBNm5Y/1NpcU2cPAE5orb+yVSgFSADsQUFrnVlr+T1A3adhN6Er4wlnK8/yrPV+qPnCuAxVAoIQLqmiooKhQ4ditVrRWrNy5cpWERBulSP/An7AN7WmTwORDSwL8Hsgvb4ZSqlJwCSAu25yUHjP6T20cWtDVU0V8XvOGYWPOywGCSGaOW9vbw4cOODsajQ7zWKgWSn1OBABLKpvvtZ6ldY6Qmsd0bVr15vax8uDX+aV+43rk+/6/KhRGB9/U9sSQojWypFnCmeAO2tN+9vK6lBKPQD8ERiitf7BUZVRSnH83HGCq7vgXv49/OIX4O/vqN0JIUSL5Mgzhf3A3UqpAKVUW+BRYEvtBZRSocBKIF5r/Z0D6wIY4woTv7PFqd/9ztG7E0KIFsdhQUFrbQWmAtuAo8B6rfUXSqlXlFJX+m0WAV7A35RSh5VSWxrY3C0rqSyh4PsCHjpYYRQ8/bSjdiWEEC2WQ8cUtNZpWutfa617a60X2srmaK232D4/oLXurrUOsb0c1sn/+enPAfA7XmykyI647pVZQogmFh0dfc2NaEuXLmXKlCk/u56XlxcARUVFjBo1qt5l7r//fq53ufrSpUupqqqyT48YMYLz5883puouo1kMNN8OX3z3BXeXueNeWQXh4ZIATwgnSEpKIiUlpU5ZSkoKSUlJjVq/Z8+ebNiw4ab3f3VQSEtLw9vb+6a31xq5zEW5Lw16iRkphShWwWOPObs6QjifE1Jnjxo1itmzZ1NdXU3btm0pLCykqKiIQYMGUVFRQUJCAmVlZdTU1LBgwQISEhLqrF9YWEhcXBxHjhzBYrEwfvx4cnJyCAwMtKeWAJgyZQr79+/HYrEwatQo5s+fz9tvv01RURHR0dGYTCYyMzPp1asX2dnZmEwmlixZYs+yOnHiRGbMmEFhYSGxsbGYzWZ2796Nn58fmzdvtie8uyI1NZUFCxZQXV2Nr68vycnJdO/enYqKCqZNm0Z2djZKKebOncvIkSPZunUrs2bN4vLly5hMJjIyMprwINwalwkKAO3+bqS8ZczP3jQthHCQLl26MGDAANLT00lISCAlJYXRo0ejlMLT05NNmzbRqVMnSktLiYqKIj4+HtXAWf2KFSto3749R48eJTc3l7CwMPu8hQsX0qVLFy5fvszQoUPJzc1l+vTpLFmyhMzMTEwmU51tHThwgLVr17J371601kRGRjJkyBB8fHwoKChg3bp1vPfee4wePZqNGzfy+FX3OJnNZvbs2YNSitWrV/PGG2+wePFiXn31VTp37kxeXh4AZWVllJSU8NRTT5GVlUVAQECzy4/kOkHhm2/g66+NrKgtMJ2tEE3OSamzr3QhXQkKa9asAYxnHsyaNYusrCzc3Nw4c+YMZ8+epUePHvVuJysri+nTpwPQv39/+vfvb5+3fv16Vq1ahdVqpbi4mPz8/Drzr7Zr1y4efvhhe6bWxMREdu7cSXx8PAEBAfYH79ROvV3b6dOnGTNmDMXFxVRXVxMQEAAYqbRrd5f5+PiQmprK4MGD7cs0t/TaLjOmwF//arwPGeLcegjh4hISEsjIyODgwYNUVVURHh4OGAnmSkpKOHDgAIcPH6Z79+43lab65MmTvPnmm2RkZJCbm8uDDz54S+mur6TdhoZTb0+bNo2pU6eSl5fHypUrW3R6bdcJCleSQ9keyCGEcA4vLy+io6OZMGFCnQHm8vJyunXrhoeHB5mZmZw6depntzN48GA++ugjAI4cOUJubi5gpN3u0KEDnTt35uzZs6Sn/5Q9p2PHjly8ePGabQ0aNIhPPvmEqqoqKisr2bRpE4MGDWp0m8rLy/Hz8wPggw8+sJcPGzaM5cuX26fLysqIiooiKyuLkydPAs0vvbbrBIVvvzXe773XufUQQpCUlEROTk6doDB27Fiys7O55557+PDDDwkMDPzZbUyZMoWKigr69OnDnDlz7GccwcHBhIaGEhgYyGOPPVYn7fakSZOIiYkhOjq6zrbCwsJ48sknGTBgAJGRkUycOJHQ0NBGt2fevHk88sgjhIeH1xmvmD17NmVlZfTr14/g4GAyMzPp2rUrq1atIjExkeDgYMY0szFOh6XOdpSbTZ3N5s2wdi18/DG4uU4sFKI2SZ3tGppr6uzmJSHBeAkhhGiQ/GQWQghhJ0FBCBfT0rqMxY251eMrQUEIF+Lp6cm5c+ckMLRSWmvOnTuHp6fnTW/DdcYUhBD4+/tz+vRpSkpKnF0V4SCenp7438KzYiQoCOFCPDw87HfSClEf6T4SQghhJ0FBCCGEnQQFIYQQdi3ujmalVAnw80lRrmUCSh1QHWeQtjRP0pbmqzW151ba8kutddfrLdTigsLNUEplN+b27pZA2tI8SVuar9bUntvRFuk+EkIIYSdBQQghhJ2rBIVVzq5AE5K2NE/SluarNbXH4W1xiTEFIYQQjeMqZwpCCCEaQYKCEEIIu1YdFJRSMUqp40qpE0qpPzi7PjdCKXWnUipTKZWvlPpCKfWsrbyLUmq7UqrA9u7j7Lo2llLKXSl1SCn1d9t0gFJqr+34/FUp1dbZdWwspZS3UmqDUuqYUuqoUmpgSz02SqmZtv+xI0qpdUopz5ZybJRS7yulvlNKHalVVu9xUIa3bW3KVUqFOa/m12qgLYts/2O5SqlNSinvWvNesrXluFLqd01Vj1YbFJRS7sByIBYIApKUUkHOrdUNsQLPa62DgCjgGVv9/wBkaK3vBjJs0y3Fs8DRWtP/Dbyltf4VUAb83im1ujnLgK1a60AgGKNdLe7YKKX8gOlAhNa6H+AOPErLOTZ/BmKuKmvoOMQCd9tek4AVt6mOjfVnrm3LdqCf1ro/8CXwEoDtu+BRoK9tnT/ZvvNuWasNCsAA4ITW+iutdTWQArSY53FqrYu11gdtny9ifOn4YbThA9tiHwD/4Zwa3hillD/wILDaNq2A3wIbbIu0pLZ0BgYDawC01tVa6/O00GODkS25nVKqDdAeKKaFHButdRbw/VXFDR2HBOBDbdgDeCulfnF7anp99bVFa/0PrbXVNrkHuJITOwFI0Vr/oLU+CZzA+M67Za05KPgB39SaPm0ra3GUUr2AUGAv0F1rXWyb9S3Q3UnVulFLgf8D/Gib9gXO1/qHb0nHJwAoAdbausNWK6U60AKPjdb6DPAm8DVGMCgHDtByjw00fBxa+nfCBCDd9tlhbWnNQaFVUEp5ARuBGVrrC7XnaeN64mZ/TbFSKg74Tmt9wNl1aSJtgDBghdY6FKjkqq6iFnRsfDB+dQYAPYEOXNuF0WK1lONwPUqpP2J0KSc7el+tOSicAe6sNe1vK2sxlFIeGAEhWWv9sa347JVTXtv7d86q3w24D4hXShVidOP9FqNP3tvWZQEt6/icBk5rrffapjdgBImWeGweAE5qrUu01jXAxxjHq6UeG2j4OLTI7wSl1JNAHDBW/3RjmcPa0pqDwn7gbttVFG0xBmW2OLlOjWbrc18DHNVaL6k1awvwhO3zE8Dm2123G6W1fklr7a+17oVxHHZorccCmcAo22Itoi0AWutvgW+UUr+xFQ0F8mmBxwaj2yhKKdXe9j93pS0t8tjYNHQctgDjbFchRQHltbqZmiWlVAxGt2u81rqq1qwtwKNKqTuUUgEYg+f7mmSnWutW+wJGYIzY/wv4o7Prc4N1N2Oc9uYCh22vERh98RlAAfAp0MXZdb3Bdt0P/N32+d9s/8gngL8Bdzi7fjfQjhAg23Z8PgF8WuqxAeYDx4AjwF+AO1rKsQHWYYyF1GCcwf2+oeMAKIwrEv8F5GFcceX0NlynLScwxg6ufAf831rL/9HWluNAbFPVQ9JcCCGEsGvN3UdCCCFukAQFIYQQdhIUhBBC2ElQEEIIYSdBQQghhJ0EBSFslFKXlVKHa72aLKGdUqpX7eyXQjRXba6/iBAuw6K1DnF2JYRwJjlTEOI6lFKFSqk3lFJ5Sql9Sqlf2cp7KaV22HLdZyil7rKVd7flvs+xve61bcpdKfWe7dkF/1BKtbMtP10Zz83IVUqlOKmZQgASFISord1V3Udjas0r11rfA7yLkfEV4B3gA23kuk8G3raVvw38U2sdjJET6Qtb+d3Acq11X+A8MNJW/gcg1LadyY5qnBCNIXc0C2GjlKrQWnvVU14I/FZr/ZUtSeG3WmtfpVQp8AutdY2tvFhrbVJKlQD+Wusfam2jF7BdGw9+QSn1IuChtV6glNoKVGCky/hEa13h4KYK0SA5UxCicXQDn2/ED7U+X+anMb0HMXLyhAH7a2UnFeK2k6AgROOMqfX+ue3zboysrwBjgZ22zxnAFLA/l7pzQxtVSrkBd2qtM4EXgc7ANWcrQtwu8otEiJ+0U0odrjW9VWt95bJUH6VULsav/SRb2TSMp6/9F8aT2Mbbyp8FVimlfo9xRjAFI/tlfdyB/2cLHAp4WxuP9hTCKWRMQYjrsI0pRGitS51dFyEcTbqPhBBC2MmZghBCCDs5UxBCCGEnQUEIIYSdBAUhhBB2EhSEEELYSVAQQghh9/8Bcli+jmZtrbsAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.clf()\n",
    "\n",
    "acc_values = L2_model_dict['acc'] \n",
    "val_acc_values = L2_model_dict['val_acc']\n",
    "model_acc = model_val_dict['acc']\n",
    "model_val_acc = model_val_dict['val_acc']\n",
    "\n",
    "epochs = range(1, len(acc_values) + 1)\n",
    "plt.plot(epochs, acc_values, 'g', label='Training acc L2')\n",
    "plt.plot(epochs, val_acc_values, 'g', label='Validation acc L2')\n",
    "plt.plot(epochs, model_acc, 'r', label='Training acc')\n",
    "plt.plot(epochs, model_val_acc, 'r', label='Validation acc')\n",
    "plt.title('Training & validation accuracy L2 vs regular')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results of L2 regularization are quite disappointing here. We notice the discrepancy between validation and training accuracy seems to have decreased slightly, but the end result is definitely not getting better. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## L1 Regularization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's have a look at L1 regularization. Will this work better?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 7500 samples, validate on 1000 samples\n",
      "Epoch 1/120\n",
      "7500/7500 [==============================] - 0s 42us/step - loss: 16.0058 - acc: 0.1349 - val_loss: 15.5881 - val_acc: 0.1850\n",
      "Epoch 2/120\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 15.2314 - acc: 0.1893 - val_loss: 14.8398 - val_acc: 0.2080\n",
      "Epoch 3/120\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 14.4970 - acc: 0.2076 - val_loss: 14.1204 - val_acc: 0.2220\n",
      "Epoch 4/120\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 13.7880 - acc: 0.2133 - val_loss: 13.4244 - val_acc: 0.2290\n",
      "Epoch 5/120\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 13.1007 - acc: 0.2237 - val_loss: 12.7483 - val_acc: 0.2430\n",
      "Epoch 6/120\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 12.4327 - acc: 0.2389 - val_loss: 12.0904 - val_acc: 0.2540\n",
      "Epoch 7/120\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 11.7830 - acc: 0.2577 - val_loss: 11.4509 - val_acc: 0.2840\n",
      "Epoch 8/120\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 11.1525 - acc: 0.2823 - val_loss: 10.8310 - val_acc: 0.3010\n",
      "Epoch 9/120\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 10.5412 - acc: 0.3040 - val_loss: 10.2316 - val_acc: 0.3220\n",
      "Epoch 10/120\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 9.9511 - acc: 0.3240 - val_loss: 9.6537 - val_acc: 0.3480\n",
      "Epoch 11/120\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 9.3820 - acc: 0.3555 - val_loss: 9.0966 - val_acc: 0.3610\n",
      "Epoch 12/120\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 8.8335 - acc: 0.3803 - val_loss: 8.5605 - val_acc: 0.3780\n",
      "Epoch 13/120\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 8.3060 - acc: 0.4016 - val_loss: 8.0449 - val_acc: 0.4070\n",
      "Epoch 14/120\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 7.7987 - acc: 0.4327 - val_loss: 7.5509 - val_acc: 0.4480\n",
      "Epoch 15/120\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 7.3126 - acc: 0.4631 - val_loss: 7.0761 - val_acc: 0.4610\n",
      "Epoch 16/120\n",
      "7500/7500 [==============================] - 0s 21us/step - loss: 6.8482 - acc: 0.4881 - val_loss: 6.6243 - val_acc: 0.4800\n",
      "Epoch 17/120\n",
      "7500/7500 [==============================] - 0s 21us/step - loss: 6.4057 - acc: 0.5136 - val_loss: 6.1944 - val_acc: 0.5120\n",
      "Epoch 18/120\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 5.9859 - acc: 0.5344 - val_loss: 5.7878 - val_acc: 0.5120\n",
      "Epoch 19/120\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 5.5888 - acc: 0.5512 - val_loss: 5.4021 - val_acc: 0.5380\n",
      "Epoch 20/120\n",
      "7500/7500 [==============================] - 0s 21us/step - loss: 5.2136 - acc: 0.5732 - val_loss: 5.0388 - val_acc: 0.5840\n",
      "Epoch 21/120\n",
      "7500/7500 [==============================] - 0s 21us/step - loss: 4.8608 - acc: 0.5923 - val_loss: 4.6965 - val_acc: 0.5910\n",
      "Epoch 22/120\n",
      "7500/7500 [==============================] - 0s 21us/step - loss: 4.5309 - acc: 0.6057 - val_loss: 4.3802 - val_acc: 0.6030\n",
      "Epoch 23/120\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 4.2240 - acc: 0.6213 - val_loss: 4.0852 - val_acc: 0.5980\n",
      "Epoch 24/120\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 3.9389 - acc: 0.6269 - val_loss: 3.8115 - val_acc: 0.6170\n",
      "Epoch 25/120\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 3.6751 - acc: 0.6373 - val_loss: 3.5587 - val_acc: 0.6260\n",
      "Epoch 26/120\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 3.4332 - acc: 0.6436 - val_loss: 3.3287 - val_acc: 0.6310\n",
      "Epoch 27/120\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 3.2131 - acc: 0.6516 - val_loss: 3.1202 - val_acc: 0.6370\n",
      "Epoch 28/120\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 3.0142 - acc: 0.6548 - val_loss: 2.9301 - val_acc: 0.6530\n",
      "Epoch 29/120\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 2.8363 - acc: 0.6624 - val_loss: 2.7637 - val_acc: 0.6490\n",
      "Epoch 30/120\n",
      "7500/7500 [==============================] - 0s 24us/step - loss: 2.6792 - acc: 0.6627 - val_loss: 2.6170 - val_acc: 0.6600\n",
      "Epoch 31/120\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 2.5428 - acc: 0.6691 - val_loss: 2.4932 - val_acc: 0.6480\n",
      "Epoch 32/120\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 2.4263 - acc: 0.6699 - val_loss: 2.3836 - val_acc: 0.6630\n",
      "Epoch 33/120\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 2.3289 - acc: 0.6727 - val_loss: 2.2967 - val_acc: 0.6580\n",
      "Epoch 34/120\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 2.2492 - acc: 0.6700 - val_loss: 2.2268 - val_acc: 0.6840\n",
      "Epoch 35/120\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 2.1867 - acc: 0.6773 - val_loss: 2.1697 - val_acc: 0.6780\n",
      "Epoch 36/120\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 2.1390 - acc: 0.6791 - val_loss: 2.1309 - val_acc: 0.6640\n",
      "Epoch 37/120\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 2.1034 - acc: 0.6784 - val_loss: 2.0966 - val_acc: 0.6790\n",
      "Epoch 38/120\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 2.0731 - acc: 0.6792 - val_loss: 2.0699 - val_acc: 0.6790\n",
      "Epoch 39/120\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 2.0466 - acc: 0.6827 - val_loss: 2.0472 - val_acc: 0.6710\n",
      "Epoch 40/120\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 2.0223 - acc: 0.6825 - val_loss: 2.0221 - val_acc: 0.6730\n",
      "Epoch 41/120\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 1.9999 - acc: 0.6849 - val_loss: 1.9994 - val_acc: 0.6880\n",
      "Epoch 42/120\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 1.9785 - acc: 0.6859 - val_loss: 1.9764 - val_acc: 0.6890\n",
      "Epoch 43/120\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 1.9576 - acc: 0.6868 - val_loss: 1.9589 - val_acc: 0.6860\n",
      "Epoch 44/120\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 1.9387 - acc: 0.6887 - val_loss: 1.9385 - val_acc: 0.6860\n",
      "Epoch 45/120\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 1.9200 - acc: 0.6916 - val_loss: 1.9236 - val_acc: 0.6870\n",
      "Epoch 46/120\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 1.9029 - acc: 0.6915 - val_loss: 1.9019 - val_acc: 0.6920\n",
      "Epoch 47/120\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 1.8855 - acc: 0.6936 - val_loss: 1.8879 - val_acc: 0.6930\n",
      "Epoch 48/120\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 1.8692 - acc: 0.6959 - val_loss: 1.8716 - val_acc: 0.6920\n",
      "Epoch 49/120\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 1.8534 - acc: 0.6972 - val_loss: 1.8567 - val_acc: 0.6960\n",
      "Epoch 50/120\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 1.8387 - acc: 0.6964 - val_loss: 1.8398 - val_acc: 0.6930\n",
      "Epoch 51/120\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 1.8237 - acc: 0.6985 - val_loss: 1.8298 - val_acc: 0.6900\n",
      "Epoch 52/120\n",
      "7500/7500 [==============================] - 0s 21us/step - loss: 1.8097 - acc: 0.6991 - val_loss: 1.8148 - val_acc: 0.6950\n",
      "Epoch 53/120\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 1.7962 - acc: 0.6989 - val_loss: 1.7988 - val_acc: 0.6940\n",
      "Epoch 54/120\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 1.7832 - acc: 0.7024 - val_loss: 1.7888 - val_acc: 0.6990\n",
      "Epoch 55/120\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 1.7703 - acc: 0.7011 - val_loss: 1.7749 - val_acc: 0.6960\n",
      "Epoch 56/120\n",
      "7500/7500 [==============================] - 0s 21us/step - loss: 1.7580 - acc: 0.7033 - val_loss: 1.7628 - val_acc: 0.6950\n",
      "Epoch 57/120\n",
      "7500/7500 [==============================] - 0s 21us/step - loss: 1.7458 - acc: 0.7031 - val_loss: 1.7519 - val_acc: 0.6970\n",
      "Epoch 58/120\n",
      "7500/7500 [==============================] - 0s 21us/step - loss: 1.7342 - acc: 0.7031 - val_loss: 1.7377 - val_acc: 0.6960\n",
      "Epoch 59/120\n",
      "7500/7500 [==============================] - 0s 21us/step - loss: 1.7226 - acc: 0.7039 - val_loss: 1.7299 - val_acc: 0.6940\n",
      "Epoch 60/120\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7500/7500 [==============================] - 0s 21us/step - loss: 1.7120 - acc: 0.7045 - val_loss: 1.7166 - val_acc: 0.6970\n",
      "Epoch 61/120\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 1.7009 - acc: 0.7048 - val_loss: 1.7056 - val_acc: 0.7000\n",
      "Epoch 62/120\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 1.6902 - acc: 0.7080 - val_loss: 1.6966 - val_acc: 0.6950\n",
      "Epoch 63/120\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 1.6803 - acc: 0.7081 - val_loss: 1.6860 - val_acc: 0.7030\n",
      "Epoch 64/120\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 1.6700 - acc: 0.7084 - val_loss: 1.6797 - val_acc: 0.7000\n",
      "Epoch 65/120\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 1.6600 - acc: 0.7080 - val_loss: 1.6723 - val_acc: 0.6920\n",
      "Epoch 66/120\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 1.6507 - acc: 0.7091 - val_loss: 1.6573 - val_acc: 0.7020\n",
      "Epoch 67/120\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 1.6411 - acc: 0.7108 - val_loss: 1.6481 - val_acc: 0.7000\n",
      "Epoch 68/120\n",
      "7500/7500 [==============================] - 0s 21us/step - loss: 1.6321 - acc: 0.7095 - val_loss: 1.6387 - val_acc: 0.7050\n",
      "Epoch 69/120\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 1.6228 - acc: 0.7101 - val_loss: 1.6314 - val_acc: 0.7060\n",
      "Epoch 70/120\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 1.6143 - acc: 0.7136 - val_loss: 1.6224 - val_acc: 0.7060\n",
      "Epoch 71/120\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 1.6051 - acc: 0.7116 - val_loss: 1.6103 - val_acc: 0.7040\n",
      "Epoch 72/120\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 1.5969 - acc: 0.7129 - val_loss: 1.6053 - val_acc: 0.7060\n",
      "Epoch 73/120\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 1.5883 - acc: 0.7137 - val_loss: 1.5957 - val_acc: 0.7060\n",
      "Epoch 74/120\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 1.5802 - acc: 0.7147 - val_loss: 1.5877 - val_acc: 0.7070\n",
      "Epoch 75/120\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 1.5717 - acc: 0.7148 - val_loss: 1.5813 - val_acc: 0.7120\n",
      "Epoch 76/120\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 1.5639 - acc: 0.7144 - val_loss: 1.5749 - val_acc: 0.7050\n",
      "Epoch 77/120\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 1.5561 - acc: 0.7155 - val_loss: 1.5641 - val_acc: 0.7040\n",
      "Epoch 78/120\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 1.5482 - acc: 0.7161 - val_loss: 1.5579 - val_acc: 0.6990\n",
      "Epoch 79/120\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 1.5407 - acc: 0.7169 - val_loss: 1.5494 - val_acc: 0.7080\n",
      "Epoch 80/120\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 1.5327 - acc: 0.7140 - val_loss: 1.5438 - val_acc: 0.7080\n",
      "Epoch 81/120\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 1.5255 - acc: 0.7171 - val_loss: 1.5364 - val_acc: 0.7120\n",
      "Epoch 82/120\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 1.5189 - acc: 0.7183 - val_loss: 1.5297 - val_acc: 0.7060\n",
      "Epoch 83/120\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 1.5113 - acc: 0.7164 - val_loss: 1.5205 - val_acc: 0.7090\n",
      "Epoch 84/120\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 1.5040 - acc: 0.7185 - val_loss: 1.5133 - val_acc: 0.7110\n",
      "Epoch 85/120\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 1.4965 - acc: 0.7179 - val_loss: 1.5074 - val_acc: 0.7090\n",
      "Epoch 86/120\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 1.4896 - acc: 0.7179 - val_loss: 1.5004 - val_acc: 0.7120\n",
      "Epoch 87/120\n",
      "7500/7500 [==============================] - 0s 21us/step - loss: 1.4834 - acc: 0.7205 - val_loss: 1.4979 - val_acc: 0.7100\n",
      "Epoch 88/120\n",
      "7500/7500 [==============================] - 0s 21us/step - loss: 1.4768 - acc: 0.7204 - val_loss: 1.4872 - val_acc: 0.7140\n",
      "Epoch 89/120\n",
      "7500/7500 [==============================] - 0s 22us/step - loss: 1.4699 - acc: 0.7196 - val_loss: 1.4811 - val_acc: 0.7130\n",
      "Epoch 90/120\n",
      "7500/7500 [==============================] - 0s 21us/step - loss: 1.4635 - acc: 0.7213 - val_loss: 1.4753 - val_acc: 0.7120\n",
      "Epoch 91/120\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 1.4567 - acc: 0.7223 - val_loss: 1.4712 - val_acc: 0.7080\n",
      "Epoch 92/120\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 1.4507 - acc: 0.7216 - val_loss: 1.4647 - val_acc: 0.7100\n",
      "Epoch 93/120\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 1.4441 - acc: 0.7240 - val_loss: 1.4557 - val_acc: 0.7110\n",
      "Epoch 94/120\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 1.4377 - acc: 0.7233 - val_loss: 1.4496 - val_acc: 0.7090\n",
      "Epoch 95/120\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 1.4313 - acc: 0.7235 - val_loss: 1.4432 - val_acc: 0.7120\n",
      "Epoch 96/120\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 1.4253 - acc: 0.7248 - val_loss: 1.4378 - val_acc: 0.7140\n",
      "Epoch 97/120\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 1.4201 - acc: 0.7239 - val_loss: 1.4331 - val_acc: 0.7120\n",
      "Epoch 98/120\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 1.4134 - acc: 0.7257 - val_loss: 1.4271 - val_acc: 0.7120\n",
      "Epoch 99/120\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 1.4076 - acc: 0.7251 - val_loss: 1.4207 - val_acc: 0.7150\n",
      "Epoch 100/120\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 1.4017 - acc: 0.7272 - val_loss: 1.4152 - val_acc: 0.7180\n",
      "Epoch 101/120\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 1.3956 - acc: 0.7263 - val_loss: 1.4107 - val_acc: 0.7150\n",
      "Epoch 102/120\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 1.3900 - acc: 0.7272 - val_loss: 1.4045 - val_acc: 0.7170\n",
      "Epoch 103/120\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 1.3844 - acc: 0.7275 - val_loss: 1.4007 - val_acc: 0.7140\n",
      "Epoch 104/120\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 1.3795 - acc: 0.7287 - val_loss: 1.3948 - val_acc: 0.7150\n",
      "Epoch 105/120\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 1.3734 - acc: 0.7269 - val_loss: 1.3886 - val_acc: 0.7120\n",
      "Epoch 106/120\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 1.3678 - acc: 0.7300 - val_loss: 1.3857 - val_acc: 0.7140\n",
      "Epoch 107/120\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 1.3630 - acc: 0.7291 - val_loss: 1.3793 - val_acc: 0.7180\n",
      "Epoch 108/120\n",
      "7500/7500 [==============================] - 0s 21us/step - loss: 1.3576 - acc: 0.7289 - val_loss: 1.3737 - val_acc: 0.7150\n",
      "Epoch 109/120\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 1.3522 - acc: 0.7315 - val_loss: 1.3764 - val_acc: 0.7100\n",
      "Epoch 110/120\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 1.3474 - acc: 0.7319 - val_loss: 1.3641 - val_acc: 0.7150\n",
      "Epoch 111/120\n",
      "7500/7500 [==============================] - 0s 21us/step - loss: 1.3421 - acc: 0.7312 - val_loss: 1.3609 - val_acc: 0.7130\n",
      "Epoch 112/120\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 1.3373 - acc: 0.7309 - val_loss: 1.3527 - val_acc: 0.7140\n",
      "Epoch 113/120\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 1.3325 - acc: 0.7325 - val_loss: 1.3529 - val_acc: 0.7130\n",
      "Epoch 114/120\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 1.3277 - acc: 0.7327 - val_loss: 1.3458 - val_acc: 0.7160\n",
      "Epoch 115/120\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 1.3218 - acc: 0.7329 - val_loss: 1.3419 - val_acc: 0.7140\n",
      "Epoch 116/120\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 1.3179 - acc: 0.7344 - val_loss: 1.3369 - val_acc: 0.7200\n",
      "Epoch 117/120\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 1.3123 - acc: 0.7339 - val_loss: 1.3335 - val_acc: 0.7150\n",
      "Epoch 118/120\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 1.3085 - acc: 0.7339 - val_loss: 1.3368 - val_acc: 0.7130\n",
      "Epoch 119/120\n",
      "7500/7500 [==============================] - 0s 21us/step - loss: 1.3039 - acc: 0.7336 - val_loss: 1.3232 - val_acc: 0.7140\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 120/120\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 1.2992 - acc: 0.7371 - val_loss: 1.3188 - val_acc: 0.7180\n"
     ]
    }
   ],
   "source": [
    "random.seed(123)\n",
    "model = models.Sequential()\n",
    "model.add(layers.Dense(50, activation='relu',kernel_regularizer=regularizers.l1(0.005), input_shape=(2000,))) #2 hidden layers\n",
    "model.add(layers.Dense(25, kernel_regularizer=regularizers.l1(0.005), activation='relu'))\n",
    "model.add(layers.Dense(7, activation='softmax'))\n",
    "\n",
    "model.compile(optimizer='SGD',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "L1_model = model.fit(train_final,\n",
    "                    label_train_final,\n",
    "                    epochs=120,\n",
    "                    batch_size=256,\n",
    "                    validation_data=(val, label_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "L1_model_dict = L1_model.history\n",
    "plt.clf()\n",
    "\n",
    "acc_values = L1_model_dict['acc'] \n",
    "val_acc_values = L1_model_dict['val_acc']\n",
    "\n",
    "epochs = range(1, len(acc_values) + 1)\n",
    "plt.plot(epochs, acc_values, 'g', label='Training acc L1')\n",
    "plt.plot(epochs, val_acc_values, 'g.', label='Validation acc L1')\n",
    "plt.title('Training & validation accuracy with L1 regularization')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice how The training and validation accuracy don't diverge as much as before! Unfortunately, the validation accuracy doesn't reach rates much higher than 70%. It does seem like we can still improve the model by training much longer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 7500 samples, validate on 1000 samples\n",
      "Epoch 1/1000\n",
      "7500/7500 [==============================] - 0s 45us/step - loss: 16.0735 - acc: 0.1592 - val_loss: 15.6452 - val_acc: 0.2090\n",
      "Epoch 2/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 15.2865 - acc: 0.2105 - val_loss: 14.8832 - val_acc: 0.2430\n",
      "Epoch 3/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 14.5401 - acc: 0.2307 - val_loss: 14.1501 - val_acc: 0.2610\n",
      "Epoch 4/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 13.8171 - acc: 0.2436 - val_loss: 13.4388 - val_acc: 0.2830\n",
      "Epoch 5/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 13.1144 - acc: 0.2624 - val_loss: 12.7476 - val_acc: 0.3080\n",
      "Epoch 6/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 12.4316 - acc: 0.2895 - val_loss: 12.0771 - val_acc: 0.3220\n",
      "Epoch 7/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 11.7690 - acc: 0.3128 - val_loss: 11.4256 - val_acc: 0.3350\n",
      "Epoch 8/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 11.1258 - acc: 0.3375 - val_loss: 10.7947 - val_acc: 0.3680\n",
      "Epoch 9/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 10.5021 - acc: 0.3683 - val_loss: 10.1831 - val_acc: 0.3910\n",
      "Epoch 10/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 9.8989 - acc: 0.3988 - val_loss: 9.5924 - val_acc: 0.4160\n",
      "Epoch 11/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 9.3167 - acc: 0.4379 - val_loss: 9.0241 - val_acc: 0.4440\n",
      "Epoch 12/1000\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 8.7566 - acc: 0.4719 - val_loss: 8.4769 - val_acc: 0.4510\n",
      "Epoch 13/1000\n",
      "7500/7500 [==============================] - 0s 21us/step - loss: 8.2188 - acc: 0.5020 - val_loss: 7.9518 - val_acc: 0.4850\n",
      "Epoch 14/1000\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 7.7039 - acc: 0.5343 - val_loss: 7.4516 - val_acc: 0.5180\n",
      "Epoch 15/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 7.2120 - acc: 0.5593 - val_loss: 6.9721 - val_acc: 0.5370\n",
      "Epoch 16/1000\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 6.7430 - acc: 0.5796 - val_loss: 6.5184 - val_acc: 0.5630\n",
      "Epoch 17/1000\n",
      "7500/7500 [==============================] - 0s 21us/step - loss: 6.2974 - acc: 0.5988 - val_loss: 6.0849 - val_acc: 0.5760\n",
      "Epoch 18/1000\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 5.8757 - acc: 0.6127 - val_loss: 5.6749 - val_acc: 0.5970\n",
      "Epoch 19/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 5.4778 - acc: 0.6272 - val_loss: 5.2889 - val_acc: 0.6100\n",
      "Epoch 20/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 5.1027 - acc: 0.6345 - val_loss: 4.9256 - val_acc: 0.6270\n",
      "Epoch 21/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 4.7505 - acc: 0.6504 - val_loss: 4.5845 - val_acc: 0.6320\n",
      "Epoch 22/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 4.4213 - acc: 0.6565 - val_loss: 4.2671 - val_acc: 0.6360\n",
      "Epoch 23/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 4.1155 - acc: 0.6596 - val_loss: 3.9741 - val_acc: 0.6540\n",
      "Epoch 24/1000\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 3.8324 - acc: 0.6657 - val_loss: 3.7016 - val_acc: 0.6590\n",
      "Epoch 25/1000\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 3.5715 - acc: 0.6715 - val_loss: 3.4511 - val_acc: 0.6550\n",
      "Epoch 26/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 3.3328 - acc: 0.6741 - val_loss: 3.2265 - val_acc: 0.6600\n",
      "Epoch 27/1000\n",
      "7500/7500 [==============================] - 0s 21us/step - loss: 3.1168 - acc: 0.6773 - val_loss: 3.0203 - val_acc: 0.6610\n",
      "Epoch 28/1000\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 2.9225 - acc: 0.6768 - val_loss: 2.8355 - val_acc: 0.6660\n",
      "Epoch 29/1000\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 2.7494 - acc: 0.6785 - val_loss: 2.6743 - val_acc: 0.6650\n",
      "Epoch 30/1000\n",
      "7500/7500 [==============================] - 0s 21us/step - loss: 2.5979 - acc: 0.6784 - val_loss: 2.5328 - val_acc: 0.6600\n",
      "Epoch 31/1000\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 2.4669 - acc: 0.6797 - val_loss: 2.4108 - val_acc: 0.6580\n",
      "Epoch 32/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 2.3556 - acc: 0.6797 - val_loss: 2.3092 - val_acc: 0.6590\n",
      "Epoch 33/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 2.2630 - acc: 0.6800 - val_loss: 2.2253 - val_acc: 0.6670\n",
      "Epoch 34/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 2.1878 - acc: 0.6800 - val_loss: 2.1595 - val_acc: 0.6630\n",
      "Epoch 35/1000\n",
      "7500/7500 [==============================] - 0s 21us/step - loss: 2.1299 - acc: 0.6816 - val_loss: 2.1102 - val_acc: 0.6650\n",
      "Epoch 36/1000\n",
      "7500/7500 [==============================] - 0s 22us/step - loss: 2.0858 - acc: 0.6805 - val_loss: 2.0711 - val_acc: 0.6690\n",
      "Epoch 37/1000\n",
      "7500/7500 [==============================] - 0s 21us/step - loss: 2.0529 - acc: 0.6831 - val_loss: 2.0456 - val_acc: 0.6750\n",
      "Epoch 38/1000\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 2.0264 - acc: 0.6836 - val_loss: 2.0163 - val_acc: 0.6680\n",
      "Epoch 39/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 2.0037 - acc: 0.6815 - val_loss: 1.9946 - val_acc: 0.6670\n",
      "Epoch 40/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 1.9830 - acc: 0.6820 - val_loss: 1.9752 - val_acc: 0.6700\n",
      "Epoch 41/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 1.9643 - acc: 0.6832 - val_loss: 1.9599 - val_acc: 0.6680\n",
      "Epoch 42/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 1.9469 - acc: 0.6827 - val_loss: 1.9424 - val_acc: 0.6740\n",
      "Epoch 43/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 1.9299 - acc: 0.6849 - val_loss: 1.9254 - val_acc: 0.6730\n",
      "Epoch 44/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 1.9147 - acc: 0.6867 - val_loss: 1.9079 - val_acc: 0.6730\n",
      "Epoch 45/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 1.8993 - acc: 0.6861 - val_loss: 1.8949 - val_acc: 0.6810\n",
      "Epoch 46/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 1.8855 - acc: 0.6869 - val_loss: 1.8796 - val_acc: 0.6820\n",
      "Epoch 47/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 1.8714 - acc: 0.6861 - val_loss: 1.8679 - val_acc: 0.6810\n",
      "Epoch 48/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 1.8577 - acc: 0.6864 - val_loss: 1.8553 - val_acc: 0.6710\n",
      "Epoch 49/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 1.8450 - acc: 0.6884 - val_loss: 1.8482 - val_acc: 0.6830\n",
      "Epoch 50/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 1.8329 - acc: 0.6899 - val_loss: 1.8298 - val_acc: 0.6830\n",
      "Epoch 51/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 1.8198 - acc: 0.6905 - val_loss: 1.8165 - val_acc: 0.6880\n",
      "Epoch 52/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 1.8083 - acc: 0.6903 - val_loss: 1.8061 - val_acc: 0.6820\n",
      "Epoch 53/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 1.7966 - acc: 0.6900 - val_loss: 1.7936 - val_acc: 0.6870\n",
      "Epoch 54/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 1.7855 - acc: 0.6903 - val_loss: 1.7854 - val_acc: 0.6820\n",
      "Epoch 55/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 1.7746 - acc: 0.6915 - val_loss: 1.7738 - val_acc: 0.6840\n",
      "Epoch 56/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 1.7642 - acc: 0.6929 - val_loss: 1.7642 - val_acc: 0.6790\n",
      "Epoch 57/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 1.7535 - acc: 0.6937 - val_loss: 1.7529 - val_acc: 0.6910\n",
      "Epoch 58/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 1.7429 - acc: 0.6931 - val_loss: 1.7467 - val_acc: 0.6900\n",
      "Epoch 59/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 1.7333 - acc: 0.6940 - val_loss: 1.7321 - val_acc: 0.6880\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 60/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 1.7230 - acc: 0.6949 - val_loss: 1.7289 - val_acc: 0.6820\n",
      "Epoch 61/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 1.7134 - acc: 0.6939 - val_loss: 1.7167 - val_acc: 0.6920\n",
      "Epoch 62/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 1.7043 - acc: 0.6952 - val_loss: 1.7104 - val_acc: 0.6820\n",
      "Epoch 63/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 1.6948 - acc: 0.6948 - val_loss: 1.6993 - val_acc: 0.6860\n",
      "Epoch 64/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 1.6861 - acc: 0.6951 - val_loss: 1.6887 - val_acc: 0.6860\n",
      "Epoch 65/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 1.6770 - acc: 0.6979 - val_loss: 1.6797 - val_acc: 0.6910\n",
      "Epoch 66/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 1.6680 - acc: 0.6983 - val_loss: 1.6683 - val_acc: 0.6930\n",
      "Epoch 67/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 1.6588 - acc: 0.6983 - val_loss: 1.6620 - val_acc: 0.6920\n",
      "Epoch 68/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 1.6505 - acc: 0.6995 - val_loss: 1.6552 - val_acc: 0.6970\n",
      "Epoch 69/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 1.6420 - acc: 0.7001 - val_loss: 1.6443 - val_acc: 0.7000\n",
      "Epoch 70/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 1.6341 - acc: 0.6999 - val_loss: 1.6369 - val_acc: 0.7030\n",
      "Epoch 71/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 1.6251 - acc: 0.6996 - val_loss: 1.6340 - val_acc: 0.7010\n",
      "Epoch 72/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 1.6177 - acc: 0.7021 - val_loss: 1.6229 - val_acc: 0.6920\n",
      "Epoch 73/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 1.6094 - acc: 0.7011 - val_loss: 1.6129 - val_acc: 0.7030\n",
      "Epoch 74/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 1.6014 - acc: 0.7036 - val_loss: 1.6043 - val_acc: 0.7040\n",
      "Epoch 75/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 1.5936 - acc: 0.7031 - val_loss: 1.6039 - val_acc: 0.6920\n",
      "Epoch 76/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 1.5861 - acc: 0.7045 - val_loss: 1.5915 - val_acc: 0.7030\n",
      "Epoch 77/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 1.5785 - acc: 0.7052 - val_loss: 1.5846 - val_acc: 0.6990\n",
      "Epoch 78/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 1.5707 - acc: 0.7073 - val_loss: 1.5768 - val_acc: 0.7050\n",
      "Epoch 79/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 1.5631 - acc: 0.7055 - val_loss: 1.5676 - val_acc: 0.7020\n",
      "Epoch 80/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 1.5560 - acc: 0.7061 - val_loss: 1.5614 - val_acc: 0.7100\n",
      "Epoch 81/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 1.5487 - acc: 0.7085 - val_loss: 1.5564 - val_acc: 0.7080\n",
      "Epoch 82/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 1.5418 - acc: 0.7076 - val_loss: 1.5525 - val_acc: 0.6950\n",
      "Epoch 83/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 1.5343 - acc: 0.7085 - val_loss: 1.5422 - val_acc: 0.7080\n",
      "Epoch 84/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 1.5273 - acc: 0.7120 - val_loss: 1.5377 - val_acc: 0.7040\n",
      "Epoch 85/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 1.5209 - acc: 0.7099 - val_loss: 1.5324 - val_acc: 0.7140\n",
      "Epoch 86/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 1.5143 - acc: 0.7133 - val_loss: 1.5264 - val_acc: 0.7070\n",
      "Epoch 87/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 1.5073 - acc: 0.7121 - val_loss: 1.5224 - val_acc: 0.7160\n",
      "Epoch 88/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 1.5013 - acc: 0.7147 - val_loss: 1.5140 - val_acc: 0.7120\n",
      "Epoch 89/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 1.4947 - acc: 0.7135 - val_loss: 1.5081 - val_acc: 0.7090\n",
      "Epoch 90/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 1.4885 - acc: 0.7144 - val_loss: 1.4982 - val_acc: 0.7100\n",
      "Epoch 91/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 1.4815 - acc: 0.7155 - val_loss: 1.4935 - val_acc: 0.7080\n",
      "Epoch 92/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 1.4755 - acc: 0.7157 - val_loss: 1.4875 - val_acc: 0.7110\n",
      "Epoch 93/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 1.4688 - acc: 0.7153 - val_loss: 1.4807 - val_acc: 0.7080\n",
      "Epoch 94/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 1.4625 - acc: 0.7169 - val_loss: 1.4729 - val_acc: 0.7110\n",
      "Epoch 95/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 1.4562 - acc: 0.7176 - val_loss: 1.4740 - val_acc: 0.7060\n",
      "Epoch 96/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 1.4511 - acc: 0.7187 - val_loss: 1.4624 - val_acc: 0.7090\n",
      "Epoch 97/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 1.4448 - acc: 0.7188 - val_loss: 1.4583 - val_acc: 0.7130\n",
      "Epoch 98/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 1.4395 - acc: 0.7171 - val_loss: 1.4528 - val_acc: 0.7150\n",
      "Epoch 99/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 1.4331 - acc: 0.7196 - val_loss: 1.4463 - val_acc: 0.7100\n",
      "Epoch 100/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 1.4267 - acc: 0.7196 - val_loss: 1.4469 - val_acc: 0.7050\n",
      "Epoch 101/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 1.4220 - acc: 0.7200 - val_loss: 1.4350 - val_acc: 0.7140\n",
      "Epoch 102/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 1.4155 - acc: 0.7197 - val_loss: 1.4297 - val_acc: 0.7140\n",
      "Epoch 103/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 1.4100 - acc: 0.7215 - val_loss: 1.4253 - val_acc: 0.7100\n",
      "Epoch 104/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 1.4045 - acc: 0.7205 - val_loss: 1.4213 - val_acc: 0.7130\n",
      "Epoch 105/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 1.3995 - acc: 0.7204 - val_loss: 1.4134 - val_acc: 0.7180\n",
      "Epoch 106/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 1.3933 - acc: 0.7224 - val_loss: 1.4071 - val_acc: 0.7180\n",
      "Epoch 107/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 1.3880 - acc: 0.7220 - val_loss: 1.4054 - val_acc: 0.7160\n",
      "Epoch 108/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 1.3823 - acc: 0.7223 - val_loss: 1.3971 - val_acc: 0.7220\n",
      "Epoch 109/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 1.3775 - acc: 0.7228 - val_loss: 1.3923 - val_acc: 0.7190\n",
      "Epoch 110/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 1.3719 - acc: 0.7249 - val_loss: 1.3886 - val_acc: 0.7220\n",
      "Epoch 111/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 1.3669 - acc: 0.7235 - val_loss: 1.3826 - val_acc: 0.7120\n",
      "Epoch 112/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 1.3619 - acc: 0.7251 - val_loss: 1.3789 - val_acc: 0.7210\n",
      "Epoch 113/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 1.3571 - acc: 0.7243 - val_loss: 1.3759 - val_acc: 0.7180\n",
      "Epoch 114/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 1.3512 - acc: 0.7276 - val_loss: 1.3681 - val_acc: 0.7200\n",
      "Epoch 115/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 1.3468 - acc: 0.7248 - val_loss: 1.3708 - val_acc: 0.7190\n",
      "Epoch 116/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 1.3425 - acc: 0.7248 - val_loss: 1.3632 - val_acc: 0.7130\n",
      "Epoch 117/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 1.3372 - acc: 0.7257 - val_loss: 1.3518 - val_acc: 0.7220\n",
      "Epoch 118/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 1.3316 - acc: 0.7265 - val_loss: 1.3476 - val_acc: 0.7220\n",
      "Epoch 119/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7500/7500 [==============================] - 0s 18us/step - loss: 1.3264 - acc: 0.7293 - val_loss: 1.3439 - val_acc: 0.7220\n",
      "Epoch 120/1000\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 1.3221 - acc: 0.7281 - val_loss: 1.3410 - val_acc: 0.7190\n",
      "Epoch 121/1000\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 1.3172 - acc: 0.7287 - val_loss: 1.3340 - val_acc: 0.7200\n",
      "Epoch 122/1000\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 1.3127 - acc: 0.7276 - val_loss: 1.3350 - val_acc: 0.7140\n",
      "Epoch 123/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 1.3086 - acc: 0.7299 - val_loss: 1.3282 - val_acc: 0.7200\n",
      "Epoch 124/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 1.3034 - acc: 0.7292 - val_loss: 1.3206 - val_acc: 0.7220\n",
      "Epoch 125/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 1.2990 - acc: 0.7299 - val_loss: 1.3273 - val_acc: 0.7150\n",
      "Epoch 126/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 1.2948 - acc: 0.7295 - val_loss: 1.3130 - val_acc: 0.7230\n",
      "Epoch 127/1000\n",
      "7500/7500 [==============================] - 0s 21us/step - loss: 1.2898 - acc: 0.7303 - val_loss: 1.3116 - val_acc: 0.7220\n",
      "Epoch 128/1000\n",
      "7500/7500 [==============================] - 0s 21us/step - loss: 1.2855 - acc: 0.7317 - val_loss: 1.3073 - val_acc: 0.7250\n",
      "Epoch 129/1000\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 1.2809 - acc: 0.7311 - val_loss: 1.2998 - val_acc: 0.7260\n",
      "Epoch 130/1000\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 1.2763 - acc: 0.7315 - val_loss: 1.2960 - val_acc: 0.7290\n",
      "Epoch 131/1000\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 1.2721 - acc: 0.7320 - val_loss: 1.2925 - val_acc: 0.7240\n",
      "Epoch 132/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 1.2678 - acc: 0.7311 - val_loss: 1.3001 - val_acc: 0.7230\n",
      "Epoch 133/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 1.2641 - acc: 0.7321 - val_loss: 1.2847 - val_acc: 0.7240\n",
      "Epoch 134/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 1.2596 - acc: 0.7320 - val_loss: 1.2820 - val_acc: 0.7270\n",
      "Epoch 135/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 1.2556 - acc: 0.7339 - val_loss: 1.2768 - val_acc: 0.7250\n",
      "Epoch 136/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 1.2511 - acc: 0.7336 - val_loss: 1.2747 - val_acc: 0.7240\n",
      "Epoch 137/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 1.2477 - acc: 0.7325 - val_loss: 1.2709 - val_acc: 0.7230\n",
      "Epoch 138/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 1.2437 - acc: 0.7335 - val_loss: 1.2678 - val_acc: 0.7260\n",
      "Epoch 139/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 1.2402 - acc: 0.7325 - val_loss: 1.2620 - val_acc: 0.7280\n",
      "Epoch 140/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 1.2363 - acc: 0.7343 - val_loss: 1.2567 - val_acc: 0.7270\n",
      "Epoch 141/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 1.2319 - acc: 0.7335 - val_loss: 1.2526 - val_acc: 0.7270\n",
      "Epoch 142/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 1.2286 - acc: 0.7343 - val_loss: 1.2536 - val_acc: 0.7240\n",
      "Epoch 143/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 1.2251 - acc: 0.7353 - val_loss: 1.2490 - val_acc: 0.7210\n",
      "Epoch 144/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 1.2206 - acc: 0.7360 - val_loss: 1.2452 - val_acc: 0.7280\n",
      "Epoch 145/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 1.2177 - acc: 0.7356 - val_loss: 1.2446 - val_acc: 0.7250\n",
      "Epoch 146/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 1.2145 - acc: 0.7355 - val_loss: 1.2364 - val_acc: 0.7310\n",
      "Epoch 147/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 1.2103 - acc: 0.7363 - val_loss: 1.2356 - val_acc: 0.7270\n",
      "Epoch 148/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 1.2070 - acc: 0.7353 - val_loss: 1.2345 - val_acc: 0.7230\n",
      "Epoch 149/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 1.2039 - acc: 0.7368 - val_loss: 1.2309 - val_acc: 0.7270\n",
      "Epoch 150/1000\n",
      "7500/7500 [==============================] - 0s 21us/step - loss: 1.2004 - acc: 0.7341 - val_loss: 1.2245 - val_acc: 0.7330\n",
      "Epoch 151/1000\n",
      "7500/7500 [==============================] - 0s 21us/step - loss: 1.1971 - acc: 0.7349 - val_loss: 1.2282 - val_acc: 0.7290\n",
      "Epoch 152/1000\n",
      "7500/7500 [==============================] - ETA: 0s - loss: 1.1970 - acc: 0.734 - 0s 21us/step - loss: 1.1935 - acc: 0.7371 - val_loss: 1.2168 - val_acc: 0.7310\n",
      "Epoch 153/1000\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 1.1903 - acc: 0.7371 - val_loss: 1.2156 - val_acc: 0.7310\n",
      "Epoch 154/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 1.1868 - acc: 0.7364 - val_loss: 1.2147 - val_acc: 0.7280\n",
      "Epoch 155/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 1.1838 - acc: 0.7360 - val_loss: 1.2104 - val_acc: 0.7320\n",
      "Epoch 156/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 1.1808 - acc: 0.7368 - val_loss: 1.2078 - val_acc: 0.7330\n",
      "Epoch 157/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 1.1776 - acc: 0.7377 - val_loss: 1.2046 - val_acc: 0.7330\n",
      "Epoch 158/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 1.1748 - acc: 0.7360 - val_loss: 1.2049 - val_acc: 0.7290\n",
      "Epoch 159/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 1.1715 - acc: 0.7376 - val_loss: 1.1968 - val_acc: 0.7310\n",
      "Epoch 160/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 1.1683 - acc: 0.7387 - val_loss: 1.1977 - val_acc: 0.7270\n",
      "Epoch 161/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 1.1658 - acc: 0.7376 - val_loss: 1.1911 - val_acc: 0.7310\n",
      "Epoch 162/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 1.1627 - acc: 0.7373 - val_loss: 1.1957 - val_acc: 0.7270\n",
      "Epoch 163/1000\n",
      "7500/7500 [==============================] - 0s 21us/step - loss: 1.1599 - acc: 0.7387 - val_loss: 1.1858 - val_acc: 0.7330\n",
      "Epoch 164/1000\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 1.1571 - acc: 0.7389 - val_loss: 1.1894 - val_acc: 0.7360\n",
      "Epoch 165/1000\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 1.1545 - acc: 0.7373 - val_loss: 1.1812 - val_acc: 0.7330\n",
      "Epoch 166/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 1.1510 - acc: 0.7392 - val_loss: 1.1778 - val_acc: 0.7330\n",
      "Epoch 167/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 1.1487 - acc: 0.7389 - val_loss: 1.1778 - val_acc: 0.7340\n",
      "Epoch 168/1000\n",
      "7500/7500 [==============================] - 0s 21us/step - loss: 1.1467 - acc: 0.7391 - val_loss: 1.1730 - val_acc: 0.7300\n",
      "Epoch 169/1000\n",
      "7500/7500 [==============================] - 0s 21us/step - loss: 1.1430 - acc: 0.7387 - val_loss: 1.1724 - val_acc: 0.7330\n",
      "Epoch 170/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 1.1405 - acc: 0.7403 - val_loss: 1.1786 - val_acc: 0.7220\n",
      "Epoch 171/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 1.1383 - acc: 0.7395 - val_loss: 1.1692 - val_acc: 0.7340\n",
      "Epoch 172/1000\n",
      "7500/7500 [==============================] - 0s 21us/step - loss: 1.1356 - acc: 0.7389 - val_loss: 1.1667 - val_acc: 0.7330\n",
      "Epoch 173/1000\n",
      "7500/7500 [==============================] - 0s 21us/step - loss: 1.1329 - acc: 0.7408 - val_loss: 1.1727 - val_acc: 0.7320\n",
      "Epoch 174/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 1.1313 - acc: 0.7415 - val_loss: 1.1607 - val_acc: 0.7310\n",
      "Epoch 175/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 1.1284 - acc: 0.7423 - val_loss: 1.1615 - val_acc: 0.7340\n",
      "Epoch 176/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 1.1263 - acc: 0.7407 - val_loss: 1.1545 - val_acc: 0.7340\n",
      "Epoch 177/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 1.1231 - acc: 0.7409 - val_loss: 1.1540 - val_acc: 0.7350\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 178/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 1.1210 - acc: 0.7419 - val_loss: 1.1501 - val_acc: 0.7320\n",
      "Epoch 179/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 1.1185 - acc: 0.7432 - val_loss: 1.1491 - val_acc: 0.7290\n",
      "Epoch 180/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 1.1167 - acc: 0.7436 - val_loss: 1.1478 - val_acc: 0.7320\n",
      "Epoch 181/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 1.1151 - acc: 0.7429 - val_loss: 1.1487 - val_acc: 0.7320\n",
      "Epoch 182/1000\n",
      "7500/7500 [==============================] - 0s 21us/step - loss: 1.1127 - acc: 0.7451 - val_loss: 1.1433 - val_acc: 0.7330\n",
      "Epoch 183/1000\n",
      "7500/7500 [==============================] - 0s 21us/step - loss: 1.1100 - acc: 0.7432 - val_loss: 1.1406 - val_acc: 0.7350\n",
      "Epoch 184/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 1.1082 - acc: 0.7436 - val_loss: 1.1408 - val_acc: 0.7300\n",
      "Epoch 185/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 1.1063 - acc: 0.7440 - val_loss: 1.1445 - val_acc: 0.7390\n",
      "Epoch 186/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 1.1045 - acc: 0.7447 - val_loss: 1.1446 - val_acc: 0.7200\n",
      "Epoch 187/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 1.1027 - acc: 0.7428 - val_loss: 1.1408 - val_acc: 0.7390\n",
      "Epoch 188/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 1.1011 - acc: 0.7449 - val_loss: 1.1325 - val_acc: 0.7350\n",
      "Epoch 189/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 1.0985 - acc: 0.7452 - val_loss: 1.1355 - val_acc: 0.7360\n",
      "Epoch 190/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 1.0962 - acc: 0.7451 - val_loss: 1.1301 - val_acc: 0.7300\n",
      "Epoch 191/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 1.0952 - acc: 0.7452 - val_loss: 1.1277 - val_acc: 0.7320\n",
      "Epoch 192/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 1.0928 - acc: 0.7464 - val_loss: 1.1280 - val_acc: 0.7330\n",
      "Epoch 193/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 1.0922 - acc: 0.7448 - val_loss: 1.1242 - val_acc: 0.7300\n",
      "Epoch 194/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 1.0893 - acc: 0.7451 - val_loss: 1.1279 - val_acc: 0.7340\n",
      "Epoch 195/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 1.0881 - acc: 0.7456 - val_loss: 1.1206 - val_acc: 0.7340\n",
      "Epoch 196/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 1.0859 - acc: 0.7473 - val_loss: 1.1195 - val_acc: 0.7290\n",
      "Epoch 197/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 1.0842 - acc: 0.7465 - val_loss: 1.1218 - val_acc: 0.7290\n",
      "Epoch 198/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 1.0830 - acc: 0.7460 - val_loss: 1.1176 - val_acc: 0.7330\n",
      "Epoch 199/1000\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 1.0810 - acc: 0.7475 - val_loss: 1.1180 - val_acc: 0.7310\n",
      "Epoch 200/1000\n",
      "7500/7500 [==============================] - 0s 21us/step - loss: 1.0791 - acc: 0.7477 - val_loss: 1.1142 - val_acc: 0.7310\n",
      "Epoch 201/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 1.0775 - acc: 0.7481 - val_loss: 1.1188 - val_acc: 0.7300\n",
      "Epoch 202/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 1.0765 - acc: 0.7477 - val_loss: 1.1189 - val_acc: 0.7270\n",
      "Epoch 203/1000\n",
      "7500/7500 [==============================] - 0s 30us/step - loss: 1.0752 - acc: 0.7473 - val_loss: 1.1106 - val_acc: 0.7310\n",
      "Epoch 204/1000\n",
      "7500/7500 [==============================] - 0s 21us/step - loss: 1.0724 - acc: 0.7479 - val_loss: 1.1072 - val_acc: 0.7300\n",
      "Epoch 205/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 1.0706 - acc: 0.7480 - val_loss: 1.1076 - val_acc: 0.7340\n",
      "Epoch 206/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 1.0699 - acc: 0.7480 - val_loss: 1.1058 - val_acc: 0.7310\n",
      "Epoch 207/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 1.0676 - acc: 0.7492 - val_loss: 1.1024 - val_acc: 0.7320\n",
      "Epoch 208/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 1.0667 - acc: 0.7483 - val_loss: 1.1036 - val_acc: 0.7300\n",
      "Epoch 209/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 1.0646 - acc: 0.7485 - val_loss: 1.1026 - val_acc: 0.7350\n",
      "Epoch 210/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 1.0643 - acc: 0.7496 - val_loss: 1.1026 - val_acc: 0.7330\n",
      "Epoch 211/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 1.0621 - acc: 0.7500 - val_loss: 1.1019 - val_acc: 0.7370\n",
      "Epoch 212/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 1.0611 - acc: 0.7472 - val_loss: 1.0955 - val_acc: 0.7300\n",
      "Epoch 213/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 1.0592 - acc: 0.7485 - val_loss: 1.1015 - val_acc: 0.7300\n",
      "Epoch 214/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 1.0586 - acc: 0.7491 - val_loss: 1.0951 - val_acc: 0.7350\n",
      "Epoch 215/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 1.0559 - acc: 0.7485 - val_loss: 1.0955 - val_acc: 0.7300\n",
      "Epoch 216/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 1.0543 - acc: 0.7519 - val_loss: 1.0987 - val_acc: 0.7330\n",
      "Epoch 217/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 1.0534 - acc: 0.7492 - val_loss: 1.0938 - val_acc: 0.7350\n",
      "Epoch 218/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 1.0521 - acc: 0.7500 - val_loss: 1.0902 - val_acc: 0.7310\n",
      "Epoch 219/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 1.0505 - acc: 0.7509 - val_loss: 1.0943 - val_acc: 0.7360\n",
      "Epoch 220/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 1.0493 - acc: 0.7497 - val_loss: 1.0918 - val_acc: 0.7270\n",
      "Epoch 221/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 1.0476 - acc: 0.7497 - val_loss: 1.0921 - val_acc: 0.7340\n",
      "Epoch 222/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 1.0470 - acc: 0.7521 - val_loss: 1.0851 - val_acc: 0.7320\n",
      "Epoch 223/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 1.0454 - acc: 0.7505 - val_loss: 1.0857 - val_acc: 0.7350\n",
      "Epoch 224/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 1.0432 - acc: 0.7508 - val_loss: 1.0839 - val_acc: 0.7320\n",
      "Epoch 225/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 1.0421 - acc: 0.7520 - val_loss: 1.0806 - val_acc: 0.7350\n",
      "Epoch 226/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 1.0410 - acc: 0.7517 - val_loss: 1.0823 - val_acc: 0.7360\n",
      "Epoch 227/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 1.0406 - acc: 0.7540 - val_loss: 1.0861 - val_acc: 0.7260\n",
      "Epoch 228/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 1.0388 - acc: 0.7523 - val_loss: 1.0787 - val_acc: 0.7340\n",
      "Epoch 229/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 1.0374 - acc: 0.7512 - val_loss: 1.0771 - val_acc: 0.7330\n",
      "Epoch 230/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 1.0359 - acc: 0.7519 - val_loss: 1.0788 - val_acc: 0.7370\n",
      "Epoch 231/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 1.0346 - acc: 0.7516 - val_loss: 1.0821 - val_acc: 0.7320\n",
      "Epoch 232/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 1.0341 - acc: 0.7507 - val_loss: 1.0773 - val_acc: 0.7330\n",
      "Epoch 233/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 1.0317 - acc: 0.7523 - val_loss: 1.0759 - val_acc: 0.7310\n",
      "Epoch 234/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 1.0310 - acc: 0.7524 - val_loss: 1.0712 - val_acc: 0.7320\n",
      "Epoch 235/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 1.0297 - acc: 0.7525 - val_loss: 1.0723 - val_acc: 0.7340\n",
      "Epoch 236/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 1.0294 - acc: 0.7540 - val_loss: 1.0806 - val_acc: 0.7260\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 237/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 1.0277 - acc: 0.7524 - val_loss: 1.0756 - val_acc: 0.7360\n",
      "Epoch 238/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 1.0263 - acc: 0.7528 - val_loss: 1.0674 - val_acc: 0.7360\n",
      "Epoch 239/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 1.0257 - acc: 0.7532 - val_loss: 1.0722 - val_acc: 0.7290\n",
      "Epoch 240/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 1.0246 - acc: 0.7565 - val_loss: 1.0657 - val_acc: 0.7330\n",
      "Epoch 241/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 1.0228 - acc: 0.7540 - val_loss: 1.0757 - val_acc: 0.7290\n",
      "Epoch 242/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 1.0222 - acc: 0.7531 - val_loss: 1.0644 - val_acc: 0.7320\n",
      "Epoch 243/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 1.0211 - acc: 0.7535 - val_loss: 1.0624 - val_acc: 0.7330\n",
      "Epoch 244/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 1.0199 - acc: 0.7541 - val_loss: 1.0667 - val_acc: 0.7310\n",
      "Epoch 245/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 1.0190 - acc: 0.7553 - val_loss: 1.0666 - val_acc: 0.7310\n",
      "Epoch 246/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 1.0179 - acc: 0.7553 - val_loss: 1.0654 - val_acc: 0.7310\n",
      "Epoch 247/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 1.0162 - acc: 0.7544 - val_loss: 1.0631 - val_acc: 0.7350\n",
      "Epoch 248/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 1.0153 - acc: 0.7543 - val_loss: 1.0632 - val_acc: 0.7370\n",
      "Epoch 249/1000\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 1.0145 - acc: 0.7563 - val_loss: 1.0570 - val_acc: 0.7330\n",
      "Epoch 250/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 1.0134 - acc: 0.7552 - val_loss: 1.0559 - val_acc: 0.7350\n",
      "Epoch 251/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 1.0119 - acc: 0.7545 - val_loss: 1.0656 - val_acc: 0.7360\n",
      "Epoch 252/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 1.0113 - acc: 0.7551 - val_loss: 1.0575 - val_acc: 0.7290\n",
      "Epoch 253/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 1.0096 - acc: 0.7555 - val_loss: 1.0531 - val_acc: 0.7310\n",
      "Epoch 254/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 1.0084 - acc: 0.7557 - val_loss: 1.0655 - val_acc: 0.7430\n",
      "Epoch 255/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 1.0081 - acc: 0.7567 - val_loss: 1.0542 - val_acc: 0.7380\n",
      "Epoch 256/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 1.0073 - acc: 0.7555 - val_loss: 1.0545 - val_acc: 0.7320\n",
      "Epoch 257/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 1.0064 - acc: 0.7556 - val_loss: 1.0496 - val_acc: 0.7330\n",
      "Epoch 258/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 1.0050 - acc: 0.7559 - val_loss: 1.0521 - val_acc: 0.7370\n",
      "Epoch 259/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 1.0038 - acc: 0.7569 - val_loss: 1.0525 - val_acc: 0.7350\n",
      "Epoch 260/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 1.0029 - acc: 0.7579 - val_loss: 1.0481 - val_acc: 0.7290\n",
      "Epoch 261/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 1.0020 - acc: 0.7573 - val_loss: 1.0496 - val_acc: 0.7360\n",
      "Epoch 262/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 1.0015 - acc: 0.7564 - val_loss: 1.0459 - val_acc: 0.7370\n",
      "Epoch 263/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.9993 - acc: 0.7581 - val_loss: 1.0496 - val_acc: 0.7320\n",
      "Epoch 264/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.9993 - acc: 0.7599 - val_loss: 1.0457 - val_acc: 0.7330\n",
      "Epoch 265/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.9983 - acc: 0.7568 - val_loss: 1.0446 - val_acc: 0.7360\n",
      "Epoch 266/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.9974 - acc: 0.7589 - val_loss: 1.0479 - val_acc: 0.7400\n",
      "Epoch 267/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.9969 - acc: 0.7577 - val_loss: 1.0489 - val_acc: 0.7280\n",
      "Epoch 268/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.9960 - acc: 0.7589 - val_loss: 1.0449 - val_acc: 0.7350\n",
      "Epoch 269/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.9955 - acc: 0.7585 - val_loss: 1.0498 - val_acc: 0.7390\n",
      "Epoch 270/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.9936 - acc: 0.7588 - val_loss: 1.0416 - val_acc: 0.7350\n",
      "Epoch 271/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.9924 - acc: 0.7585 - val_loss: 1.0415 - val_acc: 0.7330\n",
      "Epoch 272/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.9912 - acc: 0.7596 - val_loss: 1.0447 - val_acc: 0.7400\n",
      "Epoch 273/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.9911 - acc: 0.7579 - val_loss: 1.0490 - val_acc: 0.7220\n",
      "Epoch 274/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.9910 - acc: 0.7592 - val_loss: 1.0425 - val_acc: 0.7350\n",
      "Epoch 275/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.9892 - acc: 0.7581 - val_loss: 1.0419 - val_acc: 0.7420\n",
      "Epoch 276/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.9883 - acc: 0.7621 - val_loss: 1.0436 - val_acc: 0.7380\n",
      "Epoch 277/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.9869 - acc: 0.7585 - val_loss: 1.0434 - val_acc: 0.7320\n",
      "Epoch 278/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.9874 - acc: 0.7611 - val_loss: 1.0439 - val_acc: 0.7410\n",
      "Epoch 279/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.9862 - acc: 0.7585 - val_loss: 1.0418 - val_acc: 0.7310\n",
      "Epoch 280/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.9844 - acc: 0.7593 - val_loss: 1.0344 - val_acc: 0.7340\n",
      "Epoch 281/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.9840 - acc: 0.7613 - val_loss: 1.0355 - val_acc: 0.7320\n",
      "Epoch 282/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.9829 - acc: 0.7601 - val_loss: 1.0350 - val_acc: 0.7450\n",
      "Epoch 283/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.9822 - acc: 0.7623 - val_loss: 1.0339 - val_acc: 0.7360\n",
      "Epoch 284/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.9807 - acc: 0.7597 - val_loss: 1.0342 - val_acc: 0.7420\n",
      "Epoch 285/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.9816 - acc: 0.7584 - val_loss: 1.0340 - val_acc: 0.7440\n",
      "Epoch 286/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.9797 - acc: 0.7601 - val_loss: 1.0323 - val_acc: 0.7360\n",
      "Epoch 287/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.9795 - acc: 0.7592 - val_loss: 1.0345 - val_acc: 0.7410\n",
      "Epoch 288/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.9785 - acc: 0.7619 - val_loss: 1.0341 - val_acc: 0.7330\n",
      "Epoch 289/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.9773 - acc: 0.7628 - val_loss: 1.0258 - val_acc: 0.7390\n",
      "Epoch 290/1000\n",
      "7500/7500 [==============================] - 0s 21us/step - loss: 0.9759 - acc: 0.7624 - val_loss: 1.0304 - val_acc: 0.7380\n",
      "Epoch 291/1000\n",
      "7500/7500 [==============================] - 0s 22us/step - loss: 0.9755 - acc: 0.7615 - val_loss: 1.0263 - val_acc: 0.7370\n",
      "Epoch 292/1000\n",
      "7500/7500 [==============================] - 0s 21us/step - loss: 0.9740 - acc: 0.7620 - val_loss: 1.0252 - val_acc: 0.7370\n",
      "Epoch 293/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.9740 - acc: 0.7604 - val_loss: 1.0275 - val_acc: 0.7320\n",
      "Epoch 294/1000\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 0.9731 - acc: 0.7621 - val_loss: 1.0309 - val_acc: 0.7350\n",
      "Epoch 295/1000\n",
      "7500/7500 [==============================] - 0s 22us/step - loss: 0.9722 - acc: 0.7624 - val_loss: 1.0283 - val_acc: 0.7350\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 296/1000\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 0.9721 - acc: 0.7649 - val_loss: 1.0387 - val_acc: 0.7360\n",
      "Epoch 297/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.9719 - acc: 0.7640 - val_loss: 1.0328 - val_acc: 0.7460\n",
      "Epoch 298/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.9697 - acc: 0.7636 - val_loss: 1.0264 - val_acc: 0.7430\n",
      "Epoch 299/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.9686 - acc: 0.7652 - val_loss: 1.0244 - val_acc: 0.7360\n",
      "Epoch 300/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.9683 - acc: 0.7651 - val_loss: 1.0214 - val_acc: 0.7390\n",
      "Epoch 301/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.9681 - acc: 0.7620 - val_loss: 1.0269 - val_acc: 0.7360\n",
      "Epoch 302/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.9672 - acc: 0.7639 - val_loss: 1.0216 - val_acc: 0.7380\n",
      "Epoch 303/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.9665 - acc: 0.7635 - val_loss: 1.0263 - val_acc: 0.7400\n",
      "Epoch 304/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.9655 - acc: 0.7643 - val_loss: 1.0214 - val_acc: 0.7420\n",
      "Epoch 305/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.9651 - acc: 0.7636 - val_loss: 1.0176 - val_acc: 0.7450\n",
      "Epoch 306/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.9641 - acc: 0.7661 - val_loss: 1.0168 - val_acc: 0.7420\n",
      "Epoch 307/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.9628 - acc: 0.7657 - val_loss: 1.0261 - val_acc: 0.7370\n",
      "Epoch 308/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.9630 - acc: 0.7649 - val_loss: 1.0190 - val_acc: 0.7420\n",
      "Epoch 309/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.9634 - acc: 0.7648 - val_loss: 1.0241 - val_acc: 0.7340\n",
      "Epoch 310/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.9618 - acc: 0.7664 - val_loss: 1.0221 - val_acc: 0.7370\n",
      "Epoch 311/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.9608 - acc: 0.7651 - val_loss: 1.0214 - val_acc: 0.7310\n",
      "Epoch 312/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.9600 - acc: 0.7636 - val_loss: 1.0164 - val_acc: 0.7420\n",
      "Epoch 313/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.9603 - acc: 0.7644 - val_loss: 1.0223 - val_acc: 0.7310\n",
      "Epoch 314/1000\n",
      "7500/7500 [==============================] - 0s 21us/step - loss: 0.9602 - acc: 0.7649 - val_loss: 1.0120 - val_acc: 0.7460\n",
      "Epoch 315/1000\n",
      "7500/7500 [==============================] - 0s 22us/step - loss: 0.9588 - acc: 0.7668 - val_loss: 1.0212 - val_acc: 0.7400\n",
      "Epoch 316/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.9578 - acc: 0.7661 - val_loss: 1.0127 - val_acc: 0.7420\n",
      "Epoch 317/1000\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 0.9578 - acc: 0.7675 - val_loss: 1.0155 - val_acc: 0.7350\n",
      "Epoch 318/1000\n",
      "7500/7500 [==============================] - 0s 21us/step - loss: 0.9562 - acc: 0.7665 - val_loss: 1.0106 - val_acc: 0.7420\n",
      "Epoch 319/1000\n",
      "7500/7500 [==============================] - 0s 22us/step - loss: 0.9558 - acc: 0.7673 - val_loss: 1.0272 - val_acc: 0.7340\n",
      "Epoch 320/1000\n",
      "7500/7500 [==============================] - 0s 21us/step - loss: 0.9551 - acc: 0.7663 - val_loss: 1.0174 - val_acc: 0.7290\n",
      "Epoch 321/1000\n",
      "7500/7500 [==============================] - 0s 21us/step - loss: 0.9550 - acc: 0.7669 - val_loss: 1.0224 - val_acc: 0.7400\n",
      "Epoch 322/1000\n",
      "7500/7500 [==============================] - 0s 21us/step - loss: 0.9546 - acc: 0.7687 - val_loss: 1.0326 - val_acc: 0.7250\n",
      "Epoch 323/1000\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 0.9555 - acc: 0.7661 - val_loss: 1.0103 - val_acc: 0.7430\n",
      "Epoch 324/1000\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 0.9523 - acc: 0.7660 - val_loss: 1.0133 - val_acc: 0.7460\n",
      "Epoch 325/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.9518 - acc: 0.7676 - val_loss: 1.0095 - val_acc: 0.7450\n",
      "Epoch 326/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.9518 - acc: 0.7663 - val_loss: 1.0230 - val_acc: 0.7380\n",
      "Epoch 327/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.9512 - acc: 0.7680 - val_loss: 1.0078 - val_acc: 0.7390\n",
      "Epoch 328/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.9508 - acc: 0.7659 - val_loss: 1.0092 - val_acc: 0.7430\n",
      "Epoch 329/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.9491 - acc: 0.7689 - val_loss: 1.0174 - val_acc: 0.7320\n",
      "Epoch 330/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.9498 - acc: 0.7681 - val_loss: 1.0076 - val_acc: 0.7350\n",
      "Epoch 331/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.9495 - acc: 0.7659 - val_loss: 1.0110 - val_acc: 0.7350\n",
      "Epoch 332/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.9475 - acc: 0.7697 - val_loss: 1.0060 - val_acc: 0.7420\n",
      "Epoch 333/1000\n",
      "7500/7500 [==============================] - 0s 21us/step - loss: 0.9469 - acc: 0.7693 - val_loss: 1.0135 - val_acc: 0.7370\n",
      "Epoch 334/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.9464 - acc: 0.7689 - val_loss: 1.0182 - val_acc: 0.7280\n",
      "Epoch 335/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.9479 - acc: 0.7673 - val_loss: 1.0045 - val_acc: 0.7460\n",
      "Epoch 336/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.9460 - acc: 0.7677 - val_loss: 1.0085 - val_acc: 0.7450\n",
      "Epoch 337/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.9446 - acc: 0.7693 - val_loss: 1.0043 - val_acc: 0.7390\n",
      "Epoch 338/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.9438 - acc: 0.7677 - val_loss: 1.0031 - val_acc: 0.7470\n",
      "Epoch 339/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.9436 - acc: 0.7700 - val_loss: 1.0078 - val_acc: 0.7320\n",
      "Epoch 340/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.9433 - acc: 0.7695 - val_loss: 1.0446 - val_acc: 0.7380\n",
      "Epoch 341/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.9445 - acc: 0.7700 - val_loss: 1.0060 - val_acc: 0.7360\n",
      "Epoch 342/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.9418 - acc: 0.7689 - val_loss: 1.0092 - val_acc: 0.7510\n",
      "Epoch 343/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.9413 - acc: 0.7688 - val_loss: 1.0355 - val_acc: 0.7320\n",
      "Epoch 344/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.9425 - acc: 0.7707 - val_loss: 1.0078 - val_acc: 0.7330\n",
      "Epoch 345/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.9400 - acc: 0.7725 - val_loss: 1.0105 - val_acc: 0.7440\n",
      "Epoch 346/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.9405 - acc: 0.7699 - val_loss: 0.9973 - val_acc: 0.7470\n",
      "Epoch 347/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.9387 - acc: 0.7705 - val_loss: 1.0056 - val_acc: 0.7380\n",
      "Epoch 348/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.9388 - acc: 0.7715 - val_loss: 1.0031 - val_acc: 0.7500\n",
      "Epoch 349/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.9389 - acc: 0.7689 - val_loss: 0.9984 - val_acc: 0.7500\n",
      "Epoch 350/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.9365 - acc: 0.7704 - val_loss: 0.9959 - val_acc: 0.7410\n",
      "Epoch 351/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.9351 - acc: 0.7705 - val_loss: 1.0033 - val_acc: 0.7380\n",
      "Epoch 352/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.9363 - acc: 0.7720 - val_loss: 0.9974 - val_acc: 0.7390\n",
      "Epoch 353/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.9354 - acc: 0.7711 - val_loss: 0.9984 - val_acc: 0.7480\n",
      "Epoch 354/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.9345 - acc: 0.7707 - val_loss: 0.9975 - val_acc: 0.7470\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 355/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.9352 - acc: 0.7717 - val_loss: 0.9988 - val_acc: 0.7530\n",
      "Epoch 356/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.9327 - acc: 0.7716 - val_loss: 0.9955 - val_acc: 0.7480\n",
      "Epoch 357/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.9328 - acc: 0.7701 - val_loss: 0.9952 - val_acc: 0.7510\n",
      "Epoch 358/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.9324 - acc: 0.7740 - val_loss: 1.0084 - val_acc: 0.7350\n",
      "Epoch 359/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.9329 - acc: 0.7731 - val_loss: 0.9994 - val_acc: 0.7470\n",
      "Epoch 360/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.9322 - acc: 0.7693 - val_loss: 0.9927 - val_acc: 0.7500\n",
      "Epoch 361/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.9306 - acc: 0.7696 - val_loss: 0.9913 - val_acc: 0.7450\n",
      "Epoch 362/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.9306 - acc: 0.7707 - val_loss: 0.9933 - val_acc: 0.7480\n",
      "Epoch 363/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.9301 - acc: 0.7724 - val_loss: 0.9933 - val_acc: 0.7500\n",
      "Epoch 364/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.9295 - acc: 0.7713 - val_loss: 0.9940 - val_acc: 0.7440\n",
      "Epoch 365/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.9304 - acc: 0.7712 - val_loss: 0.9935 - val_acc: 0.7420\n",
      "Epoch 366/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.9291 - acc: 0.7720 - val_loss: 0.9894 - val_acc: 0.7490\n",
      "Epoch 367/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.9279 - acc: 0.7731 - val_loss: 0.9942 - val_acc: 0.7450\n",
      "Epoch 368/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.9271 - acc: 0.7713 - val_loss: 1.0014 - val_acc: 0.7500\n",
      "Epoch 369/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.9278 - acc: 0.7732 - val_loss: 0.9977 - val_acc: 0.7440\n",
      "Epoch 370/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.9280 - acc: 0.7736 - val_loss: 1.0149 - val_acc: 0.7410\n",
      "Epoch 371/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.9269 - acc: 0.7721 - val_loss: 0.9964 - val_acc: 0.7350\n",
      "Epoch 372/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.9270 - acc: 0.7715 - val_loss: 0.9927 - val_acc: 0.7510\n",
      "Epoch 373/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.9254 - acc: 0.7753 - val_loss: 0.9975 - val_acc: 0.7400\n",
      "Epoch 374/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.9255 - acc: 0.7741 - val_loss: 0.9890 - val_acc: 0.7480\n",
      "Epoch 375/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.9244 - acc: 0.7733 - val_loss: 0.9991 - val_acc: 0.7430\n",
      "Epoch 376/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.9227 - acc: 0.7736 - val_loss: 0.9913 - val_acc: 0.7400\n",
      "Epoch 377/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.9225 - acc: 0.7740 - val_loss: 0.9960 - val_acc: 0.7380\n",
      "Epoch 378/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.9234 - acc: 0.7727 - val_loss: 0.9969 - val_acc: 0.7530\n",
      "Epoch 379/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.9235 - acc: 0.7747 - val_loss: 0.9875 - val_acc: 0.7450\n",
      "Epoch 380/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.9215 - acc: 0.7752 - val_loss: 0.9906 - val_acc: 0.7520\n",
      "Epoch 381/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.9231 - acc: 0.7739 - val_loss: 1.0056 - val_acc: 0.7460\n",
      "Epoch 382/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.9228 - acc: 0.7732 - val_loss: 0.9881 - val_acc: 0.7460\n",
      "Epoch 383/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.9219 - acc: 0.7737 - val_loss: 0.9979 - val_acc: 0.7430\n",
      "Epoch 384/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.9203 - acc: 0.7727 - val_loss: 0.9866 - val_acc: 0.7500\n",
      "Epoch 385/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.9201 - acc: 0.7757 - val_loss: 0.9855 - val_acc: 0.7520\n",
      "Epoch 386/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.9199 - acc: 0.7751 - val_loss: 1.0121 - val_acc: 0.7320\n",
      "Epoch 387/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.9209 - acc: 0.7736 - val_loss: 0.9846 - val_acc: 0.7500\n",
      "Epoch 388/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.9191 - acc: 0.7732 - val_loss: 0.9825 - val_acc: 0.7510\n",
      "Epoch 389/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.9184 - acc: 0.7733 - val_loss: 0.9827 - val_acc: 0.7520\n",
      "Epoch 390/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.9179 - acc: 0.7744 - val_loss: 0.9925 - val_acc: 0.7390\n",
      "Epoch 391/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.9181 - acc: 0.7740 - val_loss: 0.9847 - val_acc: 0.7480\n",
      "Epoch 392/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.9167 - acc: 0.7745 - val_loss: 0.9811 - val_acc: 0.7490\n",
      "Epoch 393/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.9179 - acc: 0.7741 - val_loss: 1.0292 - val_acc: 0.7310\n",
      "Epoch 394/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.9183 - acc: 0.7741 - val_loss: 0.9899 - val_acc: 0.7530\n",
      "Epoch 395/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.9158 - acc: 0.7748 - val_loss: 0.9817 - val_acc: 0.7510\n",
      "Epoch 396/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.9161 - acc: 0.7741 - val_loss: 0.9841 - val_acc: 0.7420\n",
      "Epoch 397/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.9159 - acc: 0.7727 - val_loss: 0.9840 - val_acc: 0.7420\n",
      "Epoch 398/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.9156 - acc: 0.7736 - val_loss: 0.9920 - val_acc: 0.7370\n",
      "Epoch 399/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.9153 - acc: 0.7743 - val_loss: 0.9984 - val_acc: 0.7520\n",
      "Epoch 400/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.9154 - acc: 0.7741 - val_loss: 0.9841 - val_acc: 0.7430\n",
      "Epoch 401/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.9142 - acc: 0.7753 - val_loss: 0.9828 - val_acc: 0.7460\n",
      "Epoch 402/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.9143 - acc: 0.7755 - val_loss: 0.9804 - val_acc: 0.7510\n",
      "Epoch 403/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.9123 - acc: 0.7745 - val_loss: 0.9815 - val_acc: 0.7480\n",
      "Epoch 404/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.9120 - acc: 0.7763 - val_loss: 0.9810 - val_acc: 0.7490\n",
      "Epoch 405/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.9122 - acc: 0.7767 - val_loss: 0.9921 - val_acc: 0.7470\n",
      "Epoch 406/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.9126 - acc: 0.7736 - val_loss: 0.9786 - val_acc: 0.7520\n",
      "Epoch 407/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.9122 - acc: 0.7749 - val_loss: 0.9841 - val_acc: 0.7420\n",
      "Epoch 408/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.9105 - acc: 0.7752 - val_loss: 0.9834 - val_acc: 0.7460\n",
      "Epoch 409/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.9107 - acc: 0.7756 - val_loss: 0.9809 - val_acc: 0.7390\n",
      "Epoch 410/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.9092 - acc: 0.7771 - val_loss: 0.9903 - val_acc: 0.7390\n",
      "Epoch 411/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.9090 - acc: 0.7755 - val_loss: 0.9867 - val_acc: 0.7410\n",
      "Epoch 412/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.9083 - acc: 0.7756 - val_loss: 0.9753 - val_acc: 0.7500\n",
      "Epoch 413/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.9084 - acc: 0.7772 - val_loss: 0.9987 - val_acc: 0.7540\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 414/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.9084 - acc: 0.7755 - val_loss: 0.9802 - val_acc: 0.7460\n",
      "Epoch 415/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.9083 - acc: 0.7791 - val_loss: 1.0070 - val_acc: 0.7500\n",
      "Epoch 416/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.9092 - acc: 0.7753 - val_loss: 0.9808 - val_acc: 0.7410\n",
      "Epoch 417/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.9065 - acc: 0.7755 - val_loss: 0.9779 - val_acc: 0.7450\n",
      "Epoch 418/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.9062 - acc: 0.7755 - val_loss: 0.9764 - val_acc: 0.7540\n",
      "Epoch 419/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.9070 - acc: 0.7767 - val_loss: 0.9852 - val_acc: 0.7420\n",
      "Epoch 420/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.9064 - acc: 0.7775 - val_loss: 0.9768 - val_acc: 0.7480\n",
      "Epoch 421/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.9041 - acc: 0.7775 - val_loss: 0.9765 - val_acc: 0.7540\n",
      "Epoch 422/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.9053 - acc: 0.7784 - val_loss: 0.9744 - val_acc: 0.7500\n",
      "Epoch 423/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.9056 - acc: 0.7805 - val_loss: 0.9784 - val_acc: 0.7440\n",
      "Epoch 424/1000\n",
      "7500/7500 [==============================] - 0s 21us/step - loss: 0.9053 - acc: 0.7777 - val_loss: 0.9744 - val_acc: 0.7510\n",
      "Epoch 425/1000\n",
      "7500/7500 [==============================] - 0s 21us/step - loss: 0.9034 - acc: 0.7771 - val_loss: 0.9739 - val_acc: 0.7450\n",
      "Epoch 426/1000\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 0.9050 - acc: 0.7780 - val_loss: 0.9769 - val_acc: 0.7420\n",
      "Epoch 427/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.9036 - acc: 0.7787 - val_loss: 0.9841 - val_acc: 0.7540\n",
      "Epoch 428/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.9039 - acc: 0.7768 - val_loss: 0.9713 - val_acc: 0.7510\n",
      "Epoch 429/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.9030 - acc: 0.7791 - val_loss: 0.9821 - val_acc: 0.7480\n",
      "Epoch 430/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.9028 - acc: 0.7772 - val_loss: 0.9869 - val_acc: 0.7460\n",
      "Epoch 431/1000\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 0.9032 - acc: 0.7769 - val_loss: 0.9821 - val_acc: 0.7460\n",
      "Epoch 432/1000\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 0.9016 - acc: 0.7747 - val_loss: 0.9769 - val_acc: 0.7510\n",
      "Epoch 433/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.9016 - acc: 0.7769 - val_loss: 0.9777 - val_acc: 0.7430\n",
      "Epoch 434/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.9042 - acc: 0.7771 - val_loss: 0.9696 - val_acc: 0.7500\n",
      "Epoch 435/1000\n",
      "7500/7500 [==============================] - 0s 21us/step - loss: 0.9012 - acc: 0.7781 - val_loss: 0.9782 - val_acc: 0.7420\n",
      "Epoch 436/1000\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 0.9017 - acc: 0.7779 - val_loss: 0.9752 - val_acc: 0.7470\n",
      "Epoch 437/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.8997 - acc: 0.7788 - val_loss: 0.9684 - val_acc: 0.7500\n",
      "Epoch 438/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.8999 - acc: 0.7761 - val_loss: 0.9754 - val_acc: 0.7500\n",
      "Epoch 439/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.8995 - acc: 0.7785 - val_loss: 0.9796 - val_acc: 0.7390\n",
      "Epoch 440/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.8995 - acc: 0.7776 - val_loss: 0.9807 - val_acc: 0.7510\n",
      "Epoch 441/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.8993 - acc: 0.7781 - val_loss: 0.9905 - val_acc: 0.7390\n",
      "Epoch 442/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.8983 - acc: 0.7777 - val_loss: 0.9767 - val_acc: 0.7410\n",
      "Epoch 443/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.8971 - acc: 0.7775 - val_loss: 0.9947 - val_acc: 0.7440\n",
      "Epoch 444/1000\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 0.8990 - acc: 0.7775 - val_loss: 0.9667 - val_acc: 0.7540\n",
      "Epoch 445/1000\n",
      "7500/7500 [==============================] - 0s 21us/step - loss: 0.8977 - acc: 0.7780 - val_loss: 0.9705 - val_acc: 0.7450\n",
      "Epoch 446/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.8963 - acc: 0.7791 - val_loss: 0.9759 - val_acc: 0.7520\n",
      "Epoch 447/1000\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 0.8984 - acc: 0.7783 - val_loss: 0.9703 - val_acc: 0.7550\n",
      "Epoch 448/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.8982 - acc: 0.7776 - val_loss: 0.9694 - val_acc: 0.7470\n",
      "Epoch 449/1000\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 0.8996 - acc: 0.7781 - val_loss: 0.9732 - val_acc: 0.7440\n",
      "Epoch 450/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.8958 - acc: 0.7783 - val_loss: 0.9669 - val_acc: 0.7510\n",
      "Epoch 451/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.8955 - acc: 0.7784 - val_loss: 0.9705 - val_acc: 0.7500\n",
      "Epoch 452/1000\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 0.8954 - acc: 0.7781 - val_loss: 0.9693 - val_acc: 0.7430\n",
      "Epoch 453/1000\n",
      "7500/7500 [==============================] - 0s 24us/step - loss: 0.8938 - acc: 0.7793 - val_loss: 0.9706 - val_acc: 0.7410\n",
      "Epoch 454/1000\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 0.8958 - acc: 0.7776 - val_loss: 0.9661 - val_acc: 0.7470\n",
      "Epoch 455/1000\n",
      "7500/7500 [==============================] - 0s 22us/step - loss: 0.8943 - acc: 0.7783 - val_loss: 0.9881 - val_acc: 0.7370\n",
      "Epoch 456/1000\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 0.8958 - acc: 0.7787 - val_loss: 0.9689 - val_acc: 0.7560\n",
      "Epoch 457/1000\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 0.8926 - acc: 0.7797 - val_loss: 0.9706 - val_acc: 0.7490\n",
      "Epoch 458/1000\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 0.8923 - acc: 0.7795 - val_loss: 0.9658 - val_acc: 0.7500\n",
      "Epoch 459/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.8933 - acc: 0.7775 - val_loss: 0.9953 - val_acc: 0.7370\n",
      "Epoch 460/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.8919 - acc: 0.7796 - val_loss: 0.9670 - val_acc: 0.7510\n",
      "Epoch 461/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.8918 - acc: 0.7791 - val_loss: 0.9694 - val_acc: 0.7510\n",
      "Epoch 462/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.8919 - acc: 0.7805 - val_loss: 0.9646 - val_acc: 0.7530\n",
      "Epoch 463/1000\n",
      "7500/7500 [==============================] - 0s 21us/step - loss: 0.8917 - acc: 0.7791 - val_loss: 0.9638 - val_acc: 0.7550\n",
      "Epoch 464/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.8914 - acc: 0.7789 - val_loss: 0.9754 - val_acc: 0.7560\n",
      "Epoch 465/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.8935 - acc: 0.7779 - val_loss: 0.9641 - val_acc: 0.7480\n",
      "Epoch 466/1000\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 0.8905 - acc: 0.7797 - val_loss: 0.9662 - val_acc: 0.7550\n",
      "Epoch 467/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.8904 - acc: 0.7815 - val_loss: 0.9684 - val_acc: 0.7600\n",
      "Epoch 468/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.8910 - acc: 0.7799 - val_loss: 0.9795 - val_acc: 0.7470\n",
      "Epoch 469/1000\n",
      "7500/7500 [==============================] - 0s 21us/step - loss: 0.8925 - acc: 0.7797 - val_loss: 0.9654 - val_acc: 0.7500\n",
      "Epoch 470/1000\n",
      "7500/7500 [==============================] - 0s 21us/step - loss: 0.8901 - acc: 0.7797 - val_loss: 0.9656 - val_acc: 0.7500\n",
      "Epoch 471/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.8898 - acc: 0.7803 - val_loss: 0.9749 - val_acc: 0.7450\n",
      "Epoch 472/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.8906 - acc: 0.7788 - val_loss: 0.9781 - val_acc: 0.7420\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 473/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.8904 - acc: 0.7804 - val_loss: 1.0116 - val_acc: 0.7370\n",
      "Epoch 474/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.8904 - acc: 0.7809 - val_loss: 0.9612 - val_acc: 0.7510\n",
      "Epoch 475/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.8888 - acc: 0.7793 - val_loss: 0.9675 - val_acc: 0.7510\n",
      "Epoch 476/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.8880 - acc: 0.7816 - val_loss: 0.9619 - val_acc: 0.7550\n",
      "Epoch 477/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.8888 - acc: 0.7791 - val_loss: 0.9663 - val_acc: 0.7530\n",
      "Epoch 478/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.8886 - acc: 0.7817 - val_loss: 0.9736 - val_acc: 0.7440\n",
      "Epoch 479/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.8882 - acc: 0.7800 - val_loss: 0.9632 - val_acc: 0.7530\n",
      "Epoch 480/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.8886 - acc: 0.7801 - val_loss: 0.9627 - val_acc: 0.7510\n",
      "Epoch 481/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.8862 - acc: 0.7827 - val_loss: 0.9779 - val_acc: 0.7460\n",
      "Epoch 482/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.8874 - acc: 0.7799 - val_loss: 0.9711 - val_acc: 0.7450\n",
      "Epoch 483/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.8861 - acc: 0.7820 - val_loss: 0.9635 - val_acc: 0.7520\n",
      "Epoch 484/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.8862 - acc: 0.7791 - val_loss: 0.9644 - val_acc: 0.7510\n",
      "Epoch 485/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.8869 - acc: 0.7812 - val_loss: 0.9601 - val_acc: 0.7550\n",
      "Epoch 486/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.8865 - acc: 0.7804 - val_loss: 0.9710 - val_acc: 0.7610\n",
      "Epoch 487/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.8859 - acc: 0.7805 - val_loss: 0.9762 - val_acc: 0.7430\n",
      "Epoch 488/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.8858 - acc: 0.7812 - val_loss: 0.9659 - val_acc: 0.7460\n",
      "Epoch 489/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.8861 - acc: 0.7819 - val_loss: 0.9720 - val_acc: 0.7540\n",
      "Epoch 490/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.8864 - acc: 0.7805 - val_loss: 0.9585 - val_acc: 0.7560\n",
      "Epoch 491/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.8845 - acc: 0.7801 - val_loss: 0.9668 - val_acc: 0.7490\n",
      "Epoch 492/1000\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 0.8849 - acc: 0.7816 - val_loss: 0.9686 - val_acc: 0.7450\n",
      "Epoch 493/1000\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 0.8833 - acc: 0.7799 - val_loss: 0.9623 - val_acc: 0.7520\n",
      "Epoch 494/1000\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 0.8839 - acc: 0.7825 - val_loss: 0.9969 - val_acc: 0.7400\n",
      "Epoch 495/1000\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 0.8835 - acc: 0.7805 - val_loss: 0.9670 - val_acc: 0.7620\n",
      "Epoch 496/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.8839 - acc: 0.7797 - val_loss: 0.9611 - val_acc: 0.7540\n",
      "Epoch 497/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.8827 - acc: 0.7827 - val_loss: 0.9858 - val_acc: 0.7580\n",
      "Epoch 498/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.8860 - acc: 0.7824 - val_loss: 0.9776 - val_acc: 0.7420\n",
      "Epoch 499/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.8825 - acc: 0.7835 - val_loss: 0.9790 - val_acc: 0.7470\n",
      "Epoch 500/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.8829 - acc: 0.7825 - val_loss: 0.9683 - val_acc: 0.7520\n",
      "Epoch 501/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.8817 - acc: 0.7819 - val_loss: 0.9634 - val_acc: 0.7490\n",
      "Epoch 502/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.8829 - acc: 0.7803 - val_loss: 0.9764 - val_acc: 0.7560\n",
      "Epoch 503/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.8815 - acc: 0.7825 - val_loss: 0.9594 - val_acc: 0.7520\n",
      "Epoch 504/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.8816 - acc: 0.7817 - val_loss: 0.9555 - val_acc: 0.7530\n",
      "Epoch 505/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.8818 - acc: 0.7824 - val_loss: 0.9569 - val_acc: 0.7560\n",
      "Epoch 506/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.8808 - acc: 0.7819 - val_loss: 0.9577 - val_acc: 0.7470\n",
      "Epoch 507/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.8806 - acc: 0.7816 - val_loss: 0.9598 - val_acc: 0.7530\n",
      "Epoch 508/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.8794 - acc: 0.7827 - val_loss: 0.9671 - val_acc: 0.7520\n",
      "Epoch 509/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.8802 - acc: 0.7825 - val_loss: 0.9575 - val_acc: 0.7540\n",
      "Epoch 510/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.8806 - acc: 0.7819 - val_loss: 0.9628 - val_acc: 0.7480\n",
      "Epoch 511/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.8808 - acc: 0.7821 - val_loss: 0.9740 - val_acc: 0.7370\n",
      "Epoch 512/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.8800 - acc: 0.7831 - val_loss: 0.9846 - val_acc: 0.7460\n",
      "Epoch 513/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.8832 - acc: 0.7805 - val_loss: 0.9559 - val_acc: 0.7530\n",
      "Epoch 514/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.8770 - acc: 0.7820 - val_loss: 0.9615 - val_acc: 0.7560\n",
      "Epoch 515/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.8778 - acc: 0.7836 - val_loss: 0.9576 - val_acc: 0.7540\n",
      "Epoch 516/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.8794 - acc: 0.7828 - val_loss: 0.9538 - val_acc: 0.7580\n",
      "Epoch 517/1000\n",
      "7500/7500 [==============================] - 0s 22us/step - loss: 0.8790 - acc: 0.7835 - val_loss: 0.9639 - val_acc: 0.7470\n",
      "Epoch 518/1000\n",
      "7500/7500 [==============================] - 0s 21us/step - loss: 0.8777 - acc: 0.7857 - val_loss: 0.9623 - val_acc: 0.7540\n",
      "Epoch 519/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.8789 - acc: 0.7815 - val_loss: 0.9558 - val_acc: 0.7510\n",
      "Epoch 520/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.8772 - acc: 0.7848 - val_loss: 0.9634 - val_acc: 0.7490\n",
      "Epoch 521/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.8831 - acc: 0.7809 - val_loss: 0.9587 - val_acc: 0.7480\n",
      "Epoch 522/1000\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 0.8789 - acc: 0.7811 - val_loss: 0.9587 - val_acc: 0.7550\n",
      "Epoch 523/1000\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 0.8782 - acc: 0.7844 - val_loss: 0.9548 - val_acc: 0.7580\n",
      "Epoch 524/1000\n",
      "7500/7500 [==============================] - 0s 22us/step - loss: 0.8774 - acc: 0.7837 - val_loss: 0.9634 - val_acc: 0.7520\n",
      "Epoch 525/1000\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 0.8776 - acc: 0.7835 - val_loss: 0.9612 - val_acc: 0.7520\n",
      "Epoch 526/1000\n",
      "7500/7500 [==============================] - 0s 21us/step - loss: 0.8771 - acc: 0.7833 - val_loss: 0.9566 - val_acc: 0.7550\n",
      "Epoch 527/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.8772 - acc: 0.7811 - val_loss: 0.9577 - val_acc: 0.7530\n",
      "Epoch 528/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.8776 - acc: 0.7839 - val_loss: 0.9564 - val_acc: 0.7530\n",
      "Epoch 529/1000\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 0.8769 - acc: 0.7825 - val_loss: 0.9592 - val_acc: 0.7550\n",
      "Epoch 530/1000\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 0.8750 - acc: 0.7841 - val_loss: 0.9551 - val_acc: 0.7540\n",
      "Epoch 531/1000\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 0.8753 - acc: 0.7851 - val_loss: 0.9524 - val_acc: 0.7530\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 532/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.8758 - acc: 0.7837 - val_loss: 0.9556 - val_acc: 0.7570\n",
      "Epoch 533/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.8753 - acc: 0.7844 - val_loss: 0.9683 - val_acc: 0.7560\n",
      "Epoch 534/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.8737 - acc: 0.7855 - val_loss: 0.9551 - val_acc: 0.7500\n",
      "Epoch 535/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.8732 - acc: 0.7845 - val_loss: 0.9585 - val_acc: 0.7490\n",
      "Epoch 536/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.8737 - acc: 0.7852 - val_loss: 0.9526 - val_acc: 0.7550\n",
      "Epoch 537/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.8750 - acc: 0.7855 - val_loss: 0.9539 - val_acc: 0.7600\n",
      "Epoch 538/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.8727 - acc: 0.7847 - val_loss: 0.9512 - val_acc: 0.7580\n",
      "Epoch 539/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.8738 - acc: 0.7845 - val_loss: 0.9732 - val_acc: 0.7420\n",
      "Epoch 540/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.8762 - acc: 0.7841 - val_loss: 0.9493 - val_acc: 0.7550\n",
      "Epoch 541/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.8734 - acc: 0.7840 - val_loss: 0.9575 - val_acc: 0.7590\n",
      "Epoch 542/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.8733 - acc: 0.7847 - val_loss: 0.9539 - val_acc: 0.7480\n",
      "Epoch 543/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.8757 - acc: 0.7848 - val_loss: 0.9803 - val_acc: 0.7450\n",
      "Epoch 544/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.8750 - acc: 0.7832 - val_loss: 0.9639 - val_acc: 0.7510\n",
      "Epoch 545/1000\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 0.8717 - acc: 0.7849 - val_loss: 0.9526 - val_acc: 0.7570\n",
      "Epoch 546/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.8738 - acc: 0.7847 - val_loss: 0.9591 - val_acc: 0.7560\n",
      "Epoch 547/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.8716 - acc: 0.7844 - val_loss: 0.9614 - val_acc: 0.7500\n",
      "Epoch 548/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.8739 - acc: 0.7833 - val_loss: 0.9580 - val_acc: 0.7560\n",
      "Epoch 549/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.8740 - acc: 0.7849 - val_loss: 0.9602 - val_acc: 0.7510\n",
      "Epoch 550/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.8714 - acc: 0.7872 - val_loss: 0.9508 - val_acc: 0.7530\n",
      "Epoch 551/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.8706 - acc: 0.7860 - val_loss: 0.9565 - val_acc: 0.7530\n",
      "Epoch 552/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.8701 - acc: 0.7865 - val_loss: 0.9556 - val_acc: 0.7500\n",
      "Epoch 553/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.8714 - acc: 0.7860 - val_loss: 0.9541 - val_acc: 0.7520\n",
      "Epoch 554/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.8716 - acc: 0.7872 - val_loss: 0.9832 - val_acc: 0.7380\n",
      "Epoch 555/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.8698 - acc: 0.7844 - val_loss: 0.9494 - val_acc: 0.7620\n",
      "Epoch 556/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.8703 - acc: 0.7859 - val_loss: 0.9712 - val_acc: 0.7510\n",
      "Epoch 557/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.8700 - acc: 0.7847 - val_loss: 0.9523 - val_acc: 0.7500\n",
      "Epoch 558/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.8697 - acc: 0.7857 - val_loss: 0.9524 - val_acc: 0.7600\n",
      "Epoch 559/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.8694 - acc: 0.7861 - val_loss: 0.9538 - val_acc: 0.7520\n",
      "Epoch 560/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.8722 - acc: 0.7860 - val_loss: 0.9609 - val_acc: 0.7520\n",
      "Epoch 561/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.8729 - acc: 0.7839 - val_loss: 0.9711 - val_acc: 0.7500\n",
      "Epoch 562/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.8716 - acc: 0.7869 - val_loss: 0.9492 - val_acc: 0.7580\n",
      "Epoch 563/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.8687 - acc: 0.7857 - val_loss: 0.9512 - val_acc: 0.7560\n",
      "Epoch 564/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.8662 - acc: 0.7865 - val_loss: 0.9605 - val_acc: 0.7650\n",
      "Epoch 565/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.8686 - acc: 0.7857 - val_loss: 0.9451 - val_acc: 0.7580\n",
      "Epoch 566/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.8677 - acc: 0.7857 - val_loss: 0.9519 - val_acc: 0.7580\n",
      "Epoch 567/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.8679 - acc: 0.7876 - val_loss: 0.9493 - val_acc: 0.7540\n",
      "Epoch 568/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.8675 - acc: 0.7857 - val_loss: 0.9578 - val_acc: 0.7440\n",
      "Epoch 569/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.8680 - acc: 0.7857 - val_loss: 0.9474 - val_acc: 0.7560\n",
      "Epoch 570/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.8658 - acc: 0.7871 - val_loss: 0.9521 - val_acc: 0.7500\n",
      "Epoch 571/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.8662 - acc: 0.7857 - val_loss: 0.9634 - val_acc: 0.7480\n",
      "Epoch 572/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.8671 - acc: 0.7848 - val_loss: 0.9785 - val_acc: 0.7490\n",
      "Epoch 573/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.8680 - acc: 0.7867 - val_loss: 0.9707 - val_acc: 0.7450\n",
      "Epoch 574/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.8669 - acc: 0.7845 - val_loss: 0.9459 - val_acc: 0.7610\n",
      "Epoch 575/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.8669 - acc: 0.7859 - val_loss: 0.9680 - val_acc: 0.7620\n",
      "Epoch 576/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.8651 - acc: 0.7869 - val_loss: 0.9582 - val_acc: 0.7570\n",
      "Epoch 577/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.8668 - acc: 0.7883 - val_loss: 0.9602 - val_acc: 0.7460\n",
      "Epoch 578/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.8649 - acc: 0.7865 - val_loss: 0.9561 - val_acc: 0.7620\n",
      "Epoch 579/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.8651 - acc: 0.7872 - val_loss: 0.9547 - val_acc: 0.7480\n",
      "Epoch 580/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.8648 - acc: 0.7867 - val_loss: 0.9810 - val_acc: 0.7430\n",
      "Epoch 581/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.8664 - acc: 0.7879 - val_loss: 0.9538 - val_acc: 0.7550\n",
      "Epoch 582/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.8653 - acc: 0.7887 - val_loss: 0.9948 - val_acc: 0.7350\n",
      "Epoch 583/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.8657 - acc: 0.7875 - val_loss: 0.9539 - val_acc: 0.7540\n",
      "Epoch 584/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.8663 - acc: 0.7860 - val_loss: 0.9681 - val_acc: 0.7550\n",
      "Epoch 585/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.8654 - acc: 0.7848 - val_loss: 0.9455 - val_acc: 0.7570\n",
      "Epoch 586/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.8630 - acc: 0.7885 - val_loss: 0.9728 - val_acc: 0.7520\n",
      "Epoch 587/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.8647 - acc: 0.7865 - val_loss: 0.9460 - val_acc: 0.7590\n",
      "Epoch 588/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.8647 - acc: 0.7885 - val_loss: 0.9450 - val_acc: 0.7570\n",
      "Epoch 589/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.8641 - acc: 0.7881 - val_loss: 0.9702 - val_acc: 0.7470\n",
      "Epoch 590/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.8640 - acc: 0.7865 - val_loss: 0.9864 - val_acc: 0.7420\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 591/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.8622 - acc: 0.7881 - val_loss: 0.9530 - val_acc: 0.7530\n",
      "Epoch 592/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.8612 - acc: 0.7899 - val_loss: 0.9556 - val_acc: 0.7540\n",
      "Epoch 593/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.8643 - acc: 0.7883 - val_loss: 0.9528 - val_acc: 0.7540\n",
      "Epoch 594/1000\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 0.8621 - acc: 0.7880 - val_loss: 1.0255 - val_acc: 0.7280\n",
      "Epoch 595/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.8672 - acc: 0.7869 - val_loss: 0.9639 - val_acc: 0.7530\n",
      "Epoch 596/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.8635 - acc: 0.7880 - val_loss: 0.9506 - val_acc: 0.7610\n",
      "Epoch 597/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.8618 - acc: 0.7875 - val_loss: 0.9601 - val_acc: 0.7500\n",
      "Epoch 598/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.8661 - acc: 0.7864 - val_loss: 0.9409 - val_acc: 0.7580\n",
      "Epoch 599/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.8615 - acc: 0.7875 - val_loss: 0.9466 - val_acc: 0.7550\n",
      "Epoch 600/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.8605 - acc: 0.7905 - val_loss: 0.9535 - val_acc: 0.7550\n",
      "Epoch 601/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.8591 - acc: 0.7904 - val_loss: 0.9405 - val_acc: 0.7610\n",
      "Epoch 602/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.8605 - acc: 0.7880 - val_loss: 0.9570 - val_acc: 0.7480\n",
      "Epoch 603/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.8633 - acc: 0.7873 - val_loss: 0.9659 - val_acc: 0.7560\n",
      "Epoch 604/1000\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 0.8614 - acc: 0.7885 - val_loss: 0.9498 - val_acc: 0.7600\n",
      "Epoch 605/1000\n",
      "7500/7500 [==============================] - 0s 21us/step - loss: 0.8616 - acc: 0.7872 - val_loss: 0.9675 - val_acc: 0.7550\n",
      "Epoch 606/1000\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 0.8587 - acc: 0.7893 - val_loss: 0.9482 - val_acc: 0.7500\n",
      "Epoch 607/1000\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 0.8623 - acc: 0.7856 - val_loss: 0.9403 - val_acc: 0.7610\n",
      "Epoch 608/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.8586 - acc: 0.7888 - val_loss: 0.9422 - val_acc: 0.7630\n",
      "Epoch 609/1000\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 0.8596 - acc: 0.7880 - val_loss: 0.9430 - val_acc: 0.7590\n",
      "Epoch 610/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.8598 - acc: 0.7917 - val_loss: 0.9501 - val_acc: 0.7520\n",
      "Epoch 611/1000\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 0.8590 - acc: 0.7904 - val_loss: 0.9475 - val_acc: 0.7550\n",
      "Epoch 612/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.8568 - acc: 0.7897 - val_loss: 0.9663 - val_acc: 0.7500\n",
      "Epoch 613/1000\n",
      "7500/7500 [==============================] - 0s 21us/step - loss: 0.8579 - acc: 0.7903 - val_loss: 0.9448 - val_acc: 0.7610\n",
      "Epoch 614/1000\n",
      "7500/7500 [==============================] - 0s 21us/step - loss: 0.8569 - acc: 0.7912 - val_loss: 0.9540 - val_acc: 0.7430\n",
      "Epoch 615/1000\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 0.8613 - acc: 0.7883 - val_loss: 0.9510 - val_acc: 0.7540\n",
      "Epoch 616/1000\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 0.8580 - acc: 0.7893 - val_loss: 0.9485 - val_acc: 0.7640\n",
      "Epoch 617/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.8609 - acc: 0.7901 - val_loss: 0.9510 - val_acc: 0.7530\n",
      "Epoch 618/1000\n",
      "7500/7500 [==============================] - 0s 21us/step - loss: 0.8599 - acc: 0.7871 - val_loss: 0.9559 - val_acc: 0.7540\n",
      "Epoch 619/1000\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 0.8587 - acc: 0.7896 - val_loss: 0.9526 - val_acc: 0.7580\n",
      "Epoch 620/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.8569 - acc: 0.7901 - val_loss: 0.9385 - val_acc: 0.7620\n",
      "Epoch 621/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.8574 - acc: 0.7880 - val_loss: 0.9454 - val_acc: 0.7590\n",
      "Epoch 622/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.8617 - acc: 0.7865 - val_loss: 0.9620 - val_acc: 0.7450\n",
      "Epoch 623/1000\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 0.8592 - acc: 0.7883 - val_loss: 0.9454 - val_acc: 0.7520\n",
      "Epoch 624/1000\n",
      "7500/7500 [==============================] - 0s 22us/step - loss: 0.8571 - acc: 0.7887 - val_loss: 0.9501 - val_acc: 0.7570\n",
      "Epoch 625/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.8565 - acc: 0.7905 - val_loss: 0.9519 - val_acc: 0.7610\n",
      "Epoch 626/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.8577 - acc: 0.7905 - val_loss: 0.9379 - val_acc: 0.7630\n",
      "Epoch 627/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.8560 - acc: 0.7899 - val_loss: 0.9478 - val_acc: 0.7600\n",
      "Epoch 628/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.8560 - acc: 0.7872 - val_loss: 0.9476 - val_acc: 0.7560\n",
      "Epoch 629/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.8567 - acc: 0.7901 - val_loss: 0.9394 - val_acc: 0.7590\n",
      "Epoch 630/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.8536 - acc: 0.7917 - val_loss: 0.9416 - val_acc: 0.7620\n",
      "Epoch 631/1000\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 0.8540 - acc: 0.7896 - val_loss: 0.9402 - val_acc: 0.7620\n",
      "Epoch 632/1000\n",
      "7500/7500 [==============================] - 0s 21us/step - loss: 0.8551 - acc: 0.7893 - val_loss: 0.9394 - val_acc: 0.7590\n",
      "Epoch 633/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.8552 - acc: 0.7912 - val_loss: 0.9605 - val_acc: 0.7470\n",
      "Epoch 634/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.8551 - acc: 0.7931 - val_loss: 0.9521 - val_acc: 0.7500\n",
      "Epoch 635/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.8541 - acc: 0.7900 - val_loss: 0.9387 - val_acc: 0.7620\n",
      "Epoch 636/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.8559 - acc: 0.7888 - val_loss: 0.9365 - val_acc: 0.7630\n",
      "Epoch 637/1000\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 0.8536 - acc: 0.7899 - val_loss: 0.9389 - val_acc: 0.7590\n",
      "Epoch 638/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.8527 - acc: 0.7936 - val_loss: 0.9393 - val_acc: 0.7570\n",
      "Epoch 639/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.8517 - acc: 0.7913 - val_loss: 0.9489 - val_acc: 0.7550\n",
      "Epoch 640/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.8533 - acc: 0.7916 - val_loss: 0.9477 - val_acc: 0.7640\n",
      "Epoch 641/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.8557 - acc: 0.7892 - val_loss: 0.9462 - val_acc: 0.7540\n",
      "Epoch 642/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.8586 - acc: 0.7873 - val_loss: 0.9381 - val_acc: 0.7600\n",
      "Epoch 643/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.8532 - acc: 0.7899 - val_loss: 0.9587 - val_acc: 0.7440\n",
      "Epoch 644/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.8538 - acc: 0.7901 - val_loss: 0.9362 - val_acc: 0.7600\n",
      "Epoch 645/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.8534 - acc: 0.7889 - val_loss: 0.9375 - val_acc: 0.7590\n",
      "Epoch 646/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.8520 - acc: 0.7899 - val_loss: 0.9752 - val_acc: 0.7440\n",
      "Epoch 647/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.8520 - acc: 0.7909 - val_loss: 0.9452 - val_acc: 0.7560\n",
      "Epoch 648/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.8525 - acc: 0.7920 - val_loss: 0.9352 - val_acc: 0.7620\n",
      "Epoch 649/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.8506 - acc: 0.7920 - val_loss: 0.9556 - val_acc: 0.7520\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 650/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.8521 - acc: 0.7904 - val_loss: 0.9408 - val_acc: 0.7620\n",
      "Epoch 651/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.8522 - acc: 0.7932 - val_loss: 0.9605 - val_acc: 0.7450\n",
      "Epoch 652/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.8545 - acc: 0.7927 - val_loss: 0.9460 - val_acc: 0.7560\n",
      "Epoch 653/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.8508 - acc: 0.7911 - val_loss: 0.9768 - val_acc: 0.7450\n",
      "Epoch 654/1000\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 0.8512 - acc: 0.7895 - val_loss: 0.9491 - val_acc: 0.7610\n",
      "Epoch 655/1000\n",
      "7500/7500 [==============================] - 0s 21us/step - loss: 0.8517 - acc: 0.7936 - val_loss: 0.9432 - val_acc: 0.7620\n",
      "Epoch 656/1000\n",
      "7500/7500 [==============================] - 0s 21us/step - loss: 0.8515 - acc: 0.7931 - val_loss: 0.9628 - val_acc: 0.7520\n",
      "Epoch 657/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.8518 - acc: 0.7919 - val_loss: 0.9752 - val_acc: 0.7480\n",
      "Epoch 658/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.8528 - acc: 0.7909 - val_loss: 0.9409 - val_acc: 0.7620\n",
      "Epoch 659/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.8547 - acc: 0.7896 - val_loss: 0.9456 - val_acc: 0.7630\n",
      "Epoch 660/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.8529 - acc: 0.7901 - val_loss: 0.9412 - val_acc: 0.7580\n",
      "Epoch 661/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.8536 - acc: 0.7895 - val_loss: 0.9394 - val_acc: 0.7640\n",
      "Epoch 662/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.8516 - acc: 0.7903 - val_loss: 0.9524 - val_acc: 0.7540\n",
      "Epoch 663/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.8527 - acc: 0.7916 - val_loss: 0.9551 - val_acc: 0.7520\n",
      "Epoch 664/1000\n",
      "7500/7500 [==============================] - 0s 21us/step - loss: 0.8494 - acc: 0.7924 - val_loss: 0.9744 - val_acc: 0.7480\n",
      "Epoch 665/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.8505 - acc: 0.7892 - val_loss: 0.9470 - val_acc: 0.7630\n",
      "Epoch 666/1000\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 0.8509 - acc: 0.7903 - val_loss: 0.9388 - val_acc: 0.7560\n",
      "Epoch 667/1000\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 0.8479 - acc: 0.7929 - val_loss: 0.9500 - val_acc: 0.7600\n",
      "Epoch 668/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.8483 - acc: 0.7917 - val_loss: 0.9459 - val_acc: 0.7520\n",
      "Epoch 669/1000\n",
      "7500/7500 [==============================] - 0s 21us/step - loss: 0.8529 - acc: 0.7912 - val_loss: 0.9655 - val_acc: 0.7470\n",
      "Epoch 670/1000\n",
      "7500/7500 [==============================] - 0s 22us/step - loss: 0.8517 - acc: 0.7913 - val_loss: 0.9808 - val_acc: 0.7440\n",
      "Epoch 671/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.8506 - acc: 0.7903 - val_loss: 0.9339 - val_acc: 0.7650\n",
      "Epoch 672/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.8501 - acc: 0.7909 - val_loss: 0.9329 - val_acc: 0.7640\n",
      "Epoch 673/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.8460 - acc: 0.7919 - val_loss: 0.9355 - val_acc: 0.7600\n",
      "Epoch 674/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.8507 - acc: 0.7948 - val_loss: 0.9379 - val_acc: 0.7600\n",
      "Epoch 675/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.8480 - acc: 0.7913 - val_loss: 0.9359 - val_acc: 0.7600\n",
      "Epoch 676/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.8497 - acc: 0.7937 - val_loss: 0.9455 - val_acc: 0.7540\n",
      "Epoch 677/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.8481 - acc: 0.7927 - val_loss: 0.9443 - val_acc: 0.7630\n",
      "Epoch 678/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.8492 - acc: 0.7917 - val_loss: 0.9510 - val_acc: 0.7610\n",
      "Epoch 679/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.8476 - acc: 0.7929 - val_loss: 0.9512 - val_acc: 0.7540\n",
      "Epoch 680/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.8489 - acc: 0.7936 - val_loss: 0.9534 - val_acc: 0.7540\n",
      "Epoch 681/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.8481 - acc: 0.7917 - val_loss: 0.9471 - val_acc: 0.7580\n",
      "Epoch 682/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.8471 - acc: 0.7908 - val_loss: 0.9437 - val_acc: 0.7570\n",
      "Epoch 683/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.8469 - acc: 0.7939 - val_loss: 0.9519 - val_acc: 0.7590\n",
      "Epoch 684/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.8494 - acc: 0.7915 - val_loss: 0.9389 - val_acc: 0.7590\n",
      "Epoch 685/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.8468 - acc: 0.7945 - val_loss: 0.9456 - val_acc: 0.7520\n",
      "Epoch 686/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.8474 - acc: 0.7961 - val_loss: 0.9373 - val_acc: 0.7590\n",
      "Epoch 687/1000\n",
      "7500/7500 [==============================] - 0s 21us/step - loss: 0.8481 - acc: 0.7931 - val_loss: 0.9366 - val_acc: 0.7600\n",
      "Epoch 688/1000\n",
      "7500/7500 [==============================] - 0s 21us/step - loss: 0.8483 - acc: 0.7920 - val_loss: 0.9323 - val_acc: 0.7630\n",
      "Epoch 689/1000\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 0.8478 - acc: 0.7912 - val_loss: 0.9475 - val_acc: 0.7620\n",
      "Epoch 690/1000\n",
      "7500/7500 [==============================] - 0s 22us/step - loss: 0.8479 - acc: 0.7913 - val_loss: 0.9390 - val_acc: 0.7540\n",
      "Epoch 691/1000\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 0.8481 - acc: 0.7936 - val_loss: 0.9349 - val_acc: 0.7510\n",
      "Epoch 692/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.8460 - acc: 0.7927 - val_loss: 0.9360 - val_acc: 0.7650\n",
      "Epoch 693/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.8478 - acc: 0.7911 - val_loss: 0.9469 - val_acc: 0.7660\n",
      "Epoch 694/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.8443 - acc: 0.7932 - val_loss: 0.9424 - val_acc: 0.7520\n",
      "Epoch 695/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.8465 - acc: 0.7925 - val_loss: 0.9363 - val_acc: 0.7620\n",
      "Epoch 696/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.8468 - acc: 0.7929 - val_loss: 0.9412 - val_acc: 0.7590\n",
      "Epoch 697/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.8437 - acc: 0.7943 - val_loss: 0.9441 - val_acc: 0.7620\n",
      "Epoch 698/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.8456 - acc: 0.7952 - val_loss: 0.9539 - val_acc: 0.7500\n",
      "Epoch 699/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.8449 - acc: 0.7939 - val_loss: 0.9580 - val_acc: 0.7480\n",
      "Epoch 700/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.8440 - acc: 0.7951 - val_loss: 0.9358 - val_acc: 0.7560\n",
      "Epoch 701/1000\n",
      "7500/7500 [==============================] - 0s 22us/step - loss: 0.8452 - acc: 0.7943 - val_loss: 0.9542 - val_acc: 0.7480\n",
      "Epoch 702/1000\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 0.8466 - acc: 0.7927 - val_loss: 0.9356 - val_acc: 0.7580\n",
      "Epoch 703/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.8435 - acc: 0.7948 - val_loss: 0.9473 - val_acc: 0.7560\n",
      "Epoch 704/1000\n",
      "7500/7500 [==============================] - 0s 21us/step - loss: 0.8485 - acc: 0.7940 - val_loss: 0.9499 - val_acc: 0.7550\n",
      "Epoch 705/1000\n",
      "7500/7500 [==============================] - 0s 21us/step - loss: 0.8499 - acc: 0.7920 - val_loss: 0.9361 - val_acc: 0.7630\n",
      "Epoch 706/1000\n",
      "7500/7500 [==============================] - 0s 21us/step - loss: 0.8456 - acc: 0.7932 - val_loss: 0.9358 - val_acc: 0.7580\n",
      "Epoch 707/1000\n",
      "7500/7500 [==============================] - 0s 21us/step - loss: 0.8440 - acc: 0.7944 - val_loss: 0.9397 - val_acc: 0.7620\n",
      "Epoch 708/1000\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 0.8445 - acc: 0.7956 - val_loss: 0.9468 - val_acc: 0.7520\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 709/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.8456 - acc: 0.7931 - val_loss: 0.9353 - val_acc: 0.7620\n",
      "Epoch 710/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.8433 - acc: 0.7940 - val_loss: 0.9325 - val_acc: 0.7660\n",
      "Epoch 711/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.8420 - acc: 0.7943 - val_loss: 0.9504 - val_acc: 0.7550\n",
      "Epoch 712/1000\n",
      "7500/7500 [==============================] - 0s 22us/step - loss: 0.8464 - acc: 0.7948 - val_loss: 0.9401 - val_acc: 0.7540\n",
      "Epoch 713/1000\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 0.8462 - acc: 0.7937 - val_loss: 0.9580 - val_acc: 0.7500\n",
      "Epoch 714/1000\n",
      "7500/7500 [==============================] - 0s 21us/step - loss: 0.8427 - acc: 0.7949 - val_loss: 0.9347 - val_acc: 0.7620\n",
      "Epoch 715/1000\n",
      "7500/7500 [==============================] - 0s 22us/step - loss: 0.8411 - acc: 0.7960 - val_loss: 0.9306 - val_acc: 0.7630\n",
      "Epoch 716/1000\n",
      "7500/7500 [==============================] - 0s 21us/step - loss: 0.8439 - acc: 0.7940 - val_loss: 0.9411 - val_acc: 0.7540\n",
      "Epoch 717/1000\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 0.8462 - acc: 0.7916 - val_loss: 0.9353 - val_acc: 0.7610\n",
      "Epoch 718/1000\n",
      "7500/7500 [==============================] - 0s 22us/step - loss: 0.8432 - acc: 0.7944 - val_loss: 0.9346 - val_acc: 0.7620\n",
      "Epoch 719/1000\n",
      "7500/7500 [==============================] - 0s 21us/step - loss: 0.8415 - acc: 0.7944 - val_loss: 0.9391 - val_acc: 0.7610\n",
      "Epoch 720/1000\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 0.8457 - acc: 0.7956 - val_loss: 0.9314 - val_acc: 0.7580\n",
      "Epoch 721/1000\n",
      "7500/7500 [==============================] - 0s 22us/step - loss: 0.8462 - acc: 0.7932 - val_loss: 0.9312 - val_acc: 0.7670\n",
      "Epoch 722/1000\n",
      "7500/7500 [==============================] - 0s 22us/step - loss: 0.8408 - acc: 0.7947 - val_loss: 0.9406 - val_acc: 0.7600\n",
      "Epoch 723/1000\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 0.8426 - acc: 0.7932 - val_loss: 0.9630 - val_acc: 0.7510\n",
      "Epoch 724/1000\n",
      "7500/7500 [==============================] - 0s 22us/step - loss: 0.8433 - acc: 0.7948 - val_loss: 0.9373 - val_acc: 0.7590\n",
      "Epoch 725/1000\n",
      "7500/7500 [==============================] - 0s 22us/step - loss: 0.8426 - acc: 0.7943 - val_loss: 0.9378 - val_acc: 0.7590\n",
      "Epoch 726/1000\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 0.8446 - acc: 0.7941 - val_loss: 0.9466 - val_acc: 0.7560\n",
      "Epoch 727/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.8431 - acc: 0.7947 - val_loss: 0.9409 - val_acc: 0.7670\n",
      "Epoch 728/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.8428 - acc: 0.7949 - val_loss: 0.9553 - val_acc: 0.7520\n",
      "Epoch 729/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.8451 - acc: 0.7924 - val_loss: 0.9440 - val_acc: 0.7540\n",
      "Epoch 730/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.8466 - acc: 0.7913 - val_loss: 0.9616 - val_acc: 0.7450\n",
      "Epoch 731/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.8421 - acc: 0.7972 - val_loss: 0.9692 - val_acc: 0.7480\n",
      "Epoch 732/1000\n",
      "7500/7500 [==============================] - 0s 21us/step - loss: 0.8444 - acc: 0.7945 - val_loss: 0.9302 - val_acc: 0.7620\n",
      "Epoch 733/1000\n",
      "7500/7500 [==============================] - 0s 22us/step - loss: 0.8444 - acc: 0.7951 - val_loss: 0.9315 - val_acc: 0.7560\n",
      "Epoch 734/1000\n",
      "7500/7500 [==============================] - 0s 21us/step - loss: 0.8447 - acc: 0.7929 - val_loss: 1.0043 - val_acc: 0.7380\n",
      "Epoch 735/1000\n",
      "7500/7500 [==============================] - 0s 22us/step - loss: 0.8509 - acc: 0.7919 - val_loss: 0.9390 - val_acc: 0.7590\n",
      "Epoch 736/1000\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 0.8464 - acc: 0.7933 - val_loss: 0.9334 - val_acc: 0.7660\n",
      "Epoch 737/1000\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 0.8424 - acc: 0.7956 - val_loss: 0.9361 - val_acc: 0.7660\n",
      "Epoch 738/1000\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 0.8406 - acc: 0.7955 - val_loss: 0.9665 - val_acc: 0.7400\n",
      "Epoch 739/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.8456 - acc: 0.7921 - val_loss: 0.9328 - val_acc: 0.7600\n",
      "Epoch 740/1000\n",
      "7500/7500 [==============================] - 0s 21us/step - loss: 0.8423 - acc: 0.7940 - val_loss: 0.9342 - val_acc: 0.7620\n",
      "Epoch 741/1000\n",
      "7500/7500 [==============================] - 0s 21us/step - loss: 0.8417 - acc: 0.7981 - val_loss: 0.9440 - val_acc: 0.7590\n",
      "Epoch 742/1000\n",
      "7500/7500 [==============================] - 0s 22us/step - loss: 0.8403 - acc: 0.7956 - val_loss: 0.9307 - val_acc: 0.7600\n",
      "Epoch 743/1000\n",
      "7500/7500 [==============================] - 0s 22us/step - loss: 0.8373 - acc: 0.7979 - val_loss: 0.9408 - val_acc: 0.7670\n",
      "Epoch 744/1000\n",
      "7500/7500 [==============================] - 0s 21us/step - loss: 0.8404 - acc: 0.7952 - val_loss: 0.9307 - val_acc: 0.7600\n",
      "Epoch 745/1000\n",
      "7500/7500 [==============================] - 0s 21us/step - loss: 0.8396 - acc: 0.7969 - val_loss: 0.9507 - val_acc: 0.7560\n",
      "Epoch 746/1000\n",
      "7500/7500 [==============================] - 0s 21us/step - loss: 0.8378 - acc: 0.7941 - val_loss: 0.9796 - val_acc: 0.7450\n",
      "Epoch 747/1000\n",
      "7500/7500 [==============================] - 0s 22us/step - loss: 0.8410 - acc: 0.7939 - val_loss: 0.9415 - val_acc: 0.7570\n",
      "Epoch 748/1000\n",
      "7500/7500 [==============================] - 0s 21us/step - loss: 0.8446 - acc: 0.7956 - val_loss: 0.9356 - val_acc: 0.7610\n",
      "Epoch 749/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.8392 - acc: 0.7972 - val_loss: 0.9588 - val_acc: 0.7540\n",
      "Epoch 750/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.8424 - acc: 0.7948 - val_loss: 0.9304 - val_acc: 0.7610\n",
      "Epoch 751/1000\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 0.8397 - acc: 0.7948 - val_loss: 0.9612 - val_acc: 0.7540\n",
      "Epoch 752/1000\n",
      "7500/7500 [==============================] - 0s 22us/step - loss: 0.8407 - acc: 0.7925 - val_loss: 0.9288 - val_acc: 0.7580\n",
      "Epoch 753/1000\n",
      "7500/7500 [==============================] - 0s 21us/step - loss: 0.8402 - acc: 0.7949 - val_loss: 0.9437 - val_acc: 0.7570\n",
      "Epoch 754/1000\n",
      "7500/7500 [==============================] - 0s 21us/step - loss: 0.8384 - acc: 0.7944 - val_loss: 0.9304 - val_acc: 0.7640\n",
      "Epoch 755/1000\n",
      "7500/7500 [==============================] - 0s 22us/step - loss: 0.8410 - acc: 0.7945 - val_loss: 0.9321 - val_acc: 0.7560\n",
      "Epoch 756/1000\n",
      "7500/7500 [==============================] - 0s 21us/step - loss: 0.8402 - acc: 0.7953 - val_loss: 0.9358 - val_acc: 0.7680\n",
      "Epoch 757/1000\n",
      "7500/7500 [==============================] - 0s 22us/step - loss: 0.8367 - acc: 0.7981 - val_loss: 0.9385 - val_acc: 0.7570\n",
      "Epoch 758/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.8373 - acc: 0.7957 - val_loss: 0.9538 - val_acc: 0.7620\n",
      "Epoch 759/1000\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 0.8407 - acc: 0.7961 - val_loss: 0.9254 - val_acc: 0.7630\n",
      "Epoch 760/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.8399 - acc: 0.7971 - val_loss: 0.9361 - val_acc: 0.7550\n",
      "Epoch 761/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.8392 - acc: 0.7969 - val_loss: 0.9344 - val_acc: 0.7590\n",
      "Epoch 762/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.8402 - acc: 0.7965 - val_loss: 0.9367 - val_acc: 0.7540\n",
      "Epoch 763/1000\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 0.8352 - acc: 0.7956 - val_loss: 0.9515 - val_acc: 0.7510\n",
      "Epoch 764/1000\n",
      "7500/7500 [==============================] - 0s 21us/step - loss: 0.8382 - acc: 0.7952 - val_loss: 0.9294 - val_acc: 0.7580\n",
      "Epoch 765/1000\n",
      "7500/7500 [==============================] - 0s 21us/step - loss: 0.8363 - acc: 0.7971 - val_loss: 0.9288 - val_acc: 0.7660\n",
      "Epoch 766/1000\n",
      "7500/7500 [==============================] - 0s 22us/step - loss: 0.8352 - acc: 0.7979 - val_loss: 0.9303 - val_acc: 0.7590\n",
      "Epoch 767/1000\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 0.8354 - acc: 0.7965 - val_loss: 0.9366 - val_acc: 0.7570\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 768/1000\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 0.8425 - acc: 0.7944 - val_loss: 0.9269 - val_acc: 0.7660\n",
      "Epoch 769/1000\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 0.8384 - acc: 0.7947 - val_loss: 0.9434 - val_acc: 0.7530\n",
      "Epoch 770/1000\n",
      "7500/7500 [==============================] - 0s 22us/step - loss: 0.8374 - acc: 0.7975 - val_loss: 0.9327 - val_acc: 0.7620\n",
      "Epoch 771/1000\n",
      "7500/7500 [==============================] - 0s 27us/step - loss: 0.8354 - acc: 0.7976 - val_loss: 0.9393 - val_acc: 0.7550\n",
      "Epoch 772/1000\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 0.8402 - acc: 0.7941 - val_loss: 0.9413 - val_acc: 0.7550\n",
      "Epoch 773/1000\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 0.8371 - acc: 0.7941 - val_loss: 0.9304 - val_acc: 0.7650\n",
      "Epoch 774/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.8364 - acc: 0.7969 - val_loss: 0.9381 - val_acc: 0.7620\n",
      "Epoch 775/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.8351 - acc: 0.7984 - val_loss: 0.9436 - val_acc: 0.7580\n",
      "Epoch 776/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.8382 - acc: 0.7955 - val_loss: 0.9529 - val_acc: 0.7510\n",
      "Epoch 777/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.8371 - acc: 0.7973 - val_loss: 0.9570 - val_acc: 0.7470\n",
      "Epoch 778/1000\n",
      "7500/7500 [==============================] - 0s 21us/step - loss: 0.8365 - acc: 0.7987 - val_loss: 0.9369 - val_acc: 0.7610\n",
      "Epoch 779/1000\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 0.8353 - acc: 0.7991 - val_loss: 0.9520 - val_acc: 0.7510\n",
      "Epoch 780/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.8377 - acc: 0.7989 - val_loss: 0.9367 - val_acc: 0.7600\n",
      "Epoch 781/1000\n",
      "7500/7500 [==============================] - 0s 21us/step - loss: 0.8363 - acc: 0.7981 - val_loss: 1.0077 - val_acc: 0.7360\n",
      "Epoch 782/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.8386 - acc: 0.7952 - val_loss: 0.9439 - val_acc: 0.7590\n",
      "Epoch 783/1000\n",
      "7500/7500 [==============================] - 0s 21us/step - loss: 0.8376 - acc: 0.7961 - val_loss: 0.9371 - val_acc: 0.7580\n",
      "Epoch 784/1000\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 0.8371 - acc: 0.7953 - val_loss: 0.9586 - val_acc: 0.7530\n",
      "Epoch 785/1000\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 0.8380 - acc: 0.7952 - val_loss: 0.9824 - val_acc: 0.7420\n",
      "Epoch 786/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.8377 - acc: 0.7977 - val_loss: 0.9349 - val_acc: 0.7650\n",
      "Epoch 787/1000\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 0.8368 - acc: 0.7973 - val_loss: 0.9634 - val_acc: 0.7480\n",
      "Epoch 788/1000\n",
      "7500/7500 [==============================] - 0s 22us/step - loss: 0.8361 - acc: 0.7957 - val_loss: 0.9421 - val_acc: 0.7680\n",
      "Epoch 789/1000\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 0.8382 - acc: 0.7952 - val_loss: 0.9871 - val_acc: 0.7440\n",
      "Epoch 790/1000\n",
      "7500/7500 [==============================] - 0s 31us/step - loss: 0.8425 - acc: 0.7937 - val_loss: 0.9336 - val_acc: 0.7630\n",
      "Epoch 791/1000\n",
      "7500/7500 [==============================] - 0s 28us/step - loss: 0.8375 - acc: 0.7965 - val_loss: 0.9369 - val_acc: 0.7620\n",
      "Epoch 792/1000\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.8351 - acc: 0.7957 - val_loss: 0.9377 - val_acc: 0.7590\n",
      "Epoch 793/1000\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 0.8377 - acc: 0.7936 - val_loss: 0.9308 - val_acc: 0.7620\n",
      "Epoch 794/1000\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 0.8344 - acc: 0.7973 - val_loss: 0.9430 - val_acc: 0.7580\n",
      "Epoch 795/1000\n",
      "7500/7500 [==============================] - 0s 24us/step - loss: 0.8347 - acc: 0.7983 - val_loss: 0.9390 - val_acc: 0.7590\n",
      "Epoch 796/1000\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 0.8365 - acc: 0.7979 - val_loss: 0.9329 - val_acc: 0.7590\n",
      "Epoch 797/1000\n",
      "7500/7500 [==============================] - 0s 21us/step - loss: 0.8331 - acc: 0.7964 - val_loss: 0.9270 - val_acc: 0.7620\n",
      "Epoch 798/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.8326 - acc: 0.7992 - val_loss: 0.9618 - val_acc: 0.7590\n",
      "Epoch 799/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.8455 - acc: 0.7931 - val_loss: 0.9285 - val_acc: 0.7610\n",
      "Epoch 800/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.8367 - acc: 0.7948 - val_loss: 0.9366 - val_acc: 0.7570\n",
      "Epoch 801/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.8358 - acc: 0.7961 - val_loss: 1.0473 - val_acc: 0.7260\n",
      "Epoch 802/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.8436 - acc: 0.7955 - val_loss: 0.9463 - val_acc: 0.7500\n",
      "Epoch 803/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.8351 - acc: 0.7964 - val_loss: 0.9382 - val_acc: 0.7580\n",
      "Epoch 804/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.8319 - acc: 0.7980 - val_loss: 0.9392 - val_acc: 0.7630\n",
      "Epoch 805/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.8329 - acc: 0.7976 - val_loss: 0.9782 - val_acc: 0.7430\n",
      "Epoch 806/1000\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 0.8346 - acc: 0.7977 - val_loss: 0.9438 - val_acc: 0.7670\n",
      "Epoch 807/1000\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 0.8375 - acc: 0.7959 - val_loss: 0.9418 - val_acc: 0.7670\n",
      "Epoch 808/1000\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 0.8333 - acc: 0.7965 - val_loss: 0.9374 - val_acc: 0.7610\n",
      "Epoch 809/1000\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 0.8382 - acc: 0.7953 - val_loss: 0.9668 - val_acc: 0.7430\n",
      "Epoch 810/1000\n",
      "7500/7500 [==============================] - 0s 22us/step - loss: 0.8393 - acc: 0.7955 - val_loss: 0.9495 - val_acc: 0.7600\n",
      "Epoch 811/1000\n",
      "7500/7500 [==============================] - 0s 21us/step - loss: 0.8334 - acc: 0.7972 - val_loss: 0.9296 - val_acc: 0.7640\n",
      "Epoch 812/1000\n",
      "7500/7500 [==============================] - 0s 27us/step - loss: 0.8329 - acc: 0.7964 - val_loss: 0.9293 - val_acc: 0.7600\n",
      "Epoch 813/1000\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 0.8308 - acc: 0.7991 - val_loss: 0.9323 - val_acc: 0.7640\n",
      "Epoch 814/1000\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 0.8324 - acc: 0.7985 - val_loss: 0.9310 - val_acc: 0.7680\n",
      "Epoch 815/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.8345 - acc: 0.7968 - val_loss: 0.9334 - val_acc: 0.7620\n",
      "Epoch 816/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.8386 - acc: 0.7941 - val_loss: 0.9377 - val_acc: 0.7610\n",
      "Epoch 817/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.8360 - acc: 0.7951 - val_loss: 0.9725 - val_acc: 0.7420\n",
      "Epoch 818/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.8317 - acc: 0.8001 - val_loss: 0.9349 - val_acc: 0.7600\n",
      "Epoch 819/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.8355 - acc: 0.7975 - val_loss: 0.9436 - val_acc: 0.7540\n",
      "Epoch 820/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.8337 - acc: 0.7961 - val_loss: 0.9235 - val_acc: 0.7640\n",
      "Epoch 821/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.8366 - acc: 0.7949 - val_loss: 1.0193 - val_acc: 0.7280\n",
      "Epoch 822/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.8365 - acc: 0.7952 - val_loss: 0.9258 - val_acc: 0.7660\n",
      "Epoch 823/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.8341 - acc: 0.7973 - val_loss: 0.9366 - val_acc: 0.7640\n",
      "Epoch 824/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.8338 - acc: 0.7972 - val_loss: 0.9274 - val_acc: 0.7600\n",
      "Epoch 825/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.8292 - acc: 0.7980 - val_loss: 0.9426 - val_acc: 0.7580\n",
      "Epoch 826/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.8290 - acc: 0.7993 - val_loss: 0.9264 - val_acc: 0.7670\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 827/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.8360 - acc: 0.7981 - val_loss: 0.9246 - val_acc: 0.7650\n",
      "Epoch 828/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.8338 - acc: 0.7985 - val_loss: 0.9314 - val_acc: 0.7640\n",
      "Epoch 829/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.8329 - acc: 0.7984 - val_loss: 0.9294 - val_acc: 0.7690\n",
      "Epoch 830/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.8281 - acc: 0.8000 - val_loss: 0.9311 - val_acc: 0.7630\n",
      "Epoch 831/1000\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 0.8281 - acc: 0.7999 - val_loss: 0.9441 - val_acc: 0.7640\n",
      "Epoch 832/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.8284 - acc: 0.8001 - val_loss: 0.9306 - val_acc: 0.7690\n",
      "Epoch 833/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.8357 - acc: 0.7967 - val_loss: 0.9466 - val_acc: 0.7570\n",
      "Epoch 834/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.8432 - acc: 0.7941 - val_loss: 0.9310 - val_acc: 0.7660\n",
      "Epoch 835/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.8317 - acc: 0.7975 - val_loss: 0.9301 - val_acc: 0.7630\n",
      "Epoch 836/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.8300 - acc: 0.7983 - val_loss: 0.9262 - val_acc: 0.7620\n",
      "Epoch 837/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.8268 - acc: 0.7993 - val_loss: 0.9849 - val_acc: 0.7390\n",
      "Epoch 838/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.8332 - acc: 0.7955 - val_loss: 0.9309 - val_acc: 0.7620\n",
      "Epoch 839/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.8308 - acc: 0.7996 - val_loss: 0.9282 - val_acc: 0.7660\n",
      "Epoch 840/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.8318 - acc: 0.7976 - val_loss: 0.9356 - val_acc: 0.7570\n",
      "Epoch 841/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.8319 - acc: 0.7973 - val_loss: 0.9297 - val_acc: 0.7620\n",
      "Epoch 842/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.8279 - acc: 0.7999 - val_loss: 0.9302 - val_acc: 0.7630\n",
      "Epoch 843/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.8289 - acc: 0.7989 - val_loss: 0.9470 - val_acc: 0.7530\n",
      "Epoch 844/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.8275 - acc: 0.8001 - val_loss: 0.9476 - val_acc: 0.7530\n",
      "Epoch 845/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.8312 - acc: 0.7975 - val_loss: 1.0498 - val_acc: 0.7240\n",
      "Epoch 846/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.8423 - acc: 0.7967 - val_loss: 0.9255 - val_acc: 0.7670\n",
      "Epoch 847/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.8369 - acc: 0.7985 - val_loss: 0.9781 - val_acc: 0.7420\n",
      "Epoch 848/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.8379 - acc: 0.7961 - val_loss: 0.9498 - val_acc: 0.7460\n",
      "Epoch 849/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.8392 - acc: 0.7973 - val_loss: 0.9713 - val_acc: 0.7460\n",
      "Epoch 850/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.8348 - acc: 0.7972 - val_loss: 1.0045 - val_acc: 0.7320\n",
      "Epoch 851/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.8348 - acc: 0.7953 - val_loss: 0.9292 - val_acc: 0.7600\n",
      "Epoch 852/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.8290 - acc: 0.7989 - val_loss: 0.9305 - val_acc: 0.7620\n",
      "Epoch 853/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.8283 - acc: 0.8008 - val_loss: 1.0314 - val_acc: 0.7300\n",
      "Epoch 854/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.8377 - acc: 0.7977 - val_loss: 0.9609 - val_acc: 0.7470\n",
      "Epoch 855/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.8315 - acc: 0.7980 - val_loss: 1.0379 - val_acc: 0.7170\n",
      "Epoch 856/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.8348 - acc: 0.7953 - val_loss: 0.9217 - val_acc: 0.7640\n",
      "Epoch 857/1000\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 0.8277 - acc: 0.7987 - val_loss: 0.9527 - val_acc: 0.7470\n",
      "Epoch 858/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.8274 - acc: 0.8020 - val_loss: 0.9255 - val_acc: 0.7650\n",
      "Epoch 859/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.8313 - acc: 0.7981 - val_loss: 0.9425 - val_acc: 0.7600\n",
      "Epoch 860/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.8323 - acc: 0.7964 - val_loss: 0.9995 - val_acc: 0.7370\n",
      "Epoch 861/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.8353 - acc: 0.7965 - val_loss: 0.9292 - val_acc: 0.7640\n",
      "Epoch 862/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.8275 - acc: 0.7973 - val_loss: 0.9452 - val_acc: 0.7660\n",
      "Epoch 863/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.8289 - acc: 0.8005 - val_loss: 0.9414 - val_acc: 0.7580\n",
      "Epoch 864/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.8428 - acc: 0.7928 - val_loss: 0.9472 - val_acc: 0.7580\n",
      "Epoch 865/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.8304 - acc: 0.7981 - val_loss: 0.9316 - val_acc: 0.7640\n",
      "Epoch 866/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.8310 - acc: 0.7996 - val_loss: 0.9250 - val_acc: 0.7660\n",
      "Epoch 867/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.8412 - acc: 0.7935 - val_loss: 0.9472 - val_acc: 0.7540\n",
      "Epoch 868/1000\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 0.8335 - acc: 0.7985 - val_loss: 0.9233 - val_acc: 0.7660\n",
      "Epoch 869/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.8356 - acc: 0.7952 - val_loss: 0.9235 - val_acc: 0.7690\n",
      "Epoch 870/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.8286 - acc: 0.7992 - val_loss: 0.9410 - val_acc: 0.7590\n",
      "Epoch 871/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.8300 - acc: 0.7971 - val_loss: 0.9368 - val_acc: 0.7600\n",
      "Epoch 872/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.8365 - acc: 0.7949 - val_loss: 0.9437 - val_acc: 0.7660\n",
      "Epoch 873/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.8292 - acc: 0.7975 - val_loss: 0.9400 - val_acc: 0.7590\n",
      "Epoch 874/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.8299 - acc: 0.7997 - val_loss: 0.9364 - val_acc: 0.7620\n",
      "Epoch 875/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.8286 - acc: 0.8005 - val_loss: 0.9421 - val_acc: 0.7570\n",
      "Epoch 876/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.8257 - acc: 0.8004 - val_loss: 0.9492 - val_acc: 0.7560\n",
      "Epoch 877/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.8272 - acc: 0.8003 - val_loss: 0.9434 - val_acc: 0.7590\n",
      "Epoch 878/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.8293 - acc: 0.7979 - val_loss: 0.9233 - val_acc: 0.7670\n",
      "Epoch 879/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.8273 - acc: 0.7993 - val_loss: 0.9203 - val_acc: 0.7650\n",
      "Epoch 880/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.8402 - acc: 0.7943 - val_loss: 0.9234 - val_acc: 0.7700\n",
      "Epoch 881/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.8290 - acc: 0.7983 - val_loss: 0.9192 - val_acc: 0.7630\n",
      "Epoch 882/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.8338 - acc: 0.7968 - val_loss: 0.9759 - val_acc: 0.7480\n",
      "Epoch 883/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.8272 - acc: 0.7995 - val_loss: 0.9516 - val_acc: 0.7610\n",
      "Epoch 884/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.8271 - acc: 0.7980 - val_loss: 0.9764 - val_acc: 0.7520\n",
      "Epoch 885/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.8323 - acc: 0.7981 - val_loss: 0.9270 - val_acc: 0.7670\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 886/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.8274 - acc: 0.7973 - val_loss: 0.9190 - val_acc: 0.7690\n",
      "Epoch 887/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.8253 - acc: 0.7989 - val_loss: 0.9498 - val_acc: 0.7580\n",
      "Epoch 888/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.8284 - acc: 0.7992 - val_loss: 0.9406 - val_acc: 0.7570\n",
      "Epoch 889/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.8259 - acc: 0.7991 - val_loss: 0.9337 - val_acc: 0.7600\n",
      "Epoch 890/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.8224 - acc: 0.8016 - val_loss: 0.9329 - val_acc: 0.7740\n",
      "Epoch 891/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.8296 - acc: 0.7989 - val_loss: 0.9188 - val_acc: 0.7660\n",
      "Epoch 892/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.8257 - acc: 0.7976 - val_loss: 0.9214 - val_acc: 0.7590\n",
      "Epoch 893/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.8246 - acc: 0.8003 - val_loss: 0.9211 - val_acc: 0.7630\n",
      "Epoch 894/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.8221 - acc: 0.8016 - val_loss: 0.9348 - val_acc: 0.7620\n",
      "Epoch 895/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.8298 - acc: 0.7991 - val_loss: 0.9743 - val_acc: 0.7440\n",
      "Epoch 896/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.8270 - acc: 0.8019 - val_loss: 0.9442 - val_acc: 0.7600\n",
      "Epoch 897/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.8360 - acc: 0.7955 - val_loss: 0.9265 - val_acc: 0.7670\n",
      "Epoch 898/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.8337 - acc: 0.7971 - val_loss: 0.9258 - val_acc: 0.7630\n",
      "Epoch 899/1000\n",
      "7500/7500 [==============================] - 0s 21us/step - loss: 0.8247 - acc: 0.7992 - val_loss: 0.9997 - val_acc: 0.7360\n",
      "Epoch 900/1000\n",
      "7500/7500 [==============================] - 0s 21us/step - loss: 0.8339 - acc: 0.7945 - val_loss: 0.9341 - val_acc: 0.7600\n",
      "Epoch 901/1000\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 0.8282 - acc: 0.7981 - val_loss: 0.9611 - val_acc: 0.7530\n",
      "Epoch 902/1000\n",
      "7500/7500 [==============================] - 0s 21us/step - loss: 0.8309 - acc: 0.7980 - val_loss: 0.9508 - val_acc: 0.7530\n",
      "Epoch 903/1000\n",
      "7500/7500 [==============================] - 0s 24us/step - loss: 0.8391 - acc: 0.7945 - val_loss: 0.9264 - val_acc: 0.7700\n",
      "Epoch 904/1000\n",
      "7500/7500 [==============================] - 0s 22us/step - loss: 0.8237 - acc: 0.7997 - val_loss: 0.9449 - val_acc: 0.7600\n",
      "Epoch 905/1000\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 0.8267 - acc: 0.7984 - val_loss: 0.9285 - val_acc: 0.7580\n",
      "Epoch 906/1000\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 0.8268 - acc: 0.7983 - val_loss: 0.9397 - val_acc: 0.7540\n",
      "Epoch 907/1000\n",
      "7500/7500 [==============================] - 0s 21us/step - loss: 0.8228 - acc: 0.8007 - val_loss: 0.9244 - val_acc: 0.7630\n",
      "Epoch 908/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.8220 - acc: 0.8015 - val_loss: 0.9288 - val_acc: 0.7660\n",
      "Epoch 909/1000\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 0.8230 - acc: 0.8011 - val_loss: 0.9186 - val_acc: 0.7670\n",
      "Epoch 910/1000\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 0.8223 - acc: 0.7999 - val_loss: 0.9229 - val_acc: 0.7640\n",
      "Epoch 911/1000\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 0.8233 - acc: 0.8008 - val_loss: 0.9588 - val_acc: 0.7530\n",
      "Epoch 912/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.8227 - acc: 0.8008 - val_loss: 0.9592 - val_acc: 0.7510\n",
      "Epoch 913/1000\n",
      "7500/7500 [==============================] - 0s 21us/step - loss: 0.8312 - acc: 0.7993 - val_loss: 0.9278 - val_acc: 0.7670\n",
      "Epoch 914/1000\n",
      "7500/7500 [==============================] - 0s 21us/step - loss: 0.8277 - acc: 0.8008 - val_loss: 0.9187 - val_acc: 0.7680\n",
      "Epoch 915/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.8214 - acc: 0.8032 - val_loss: 0.9236 - val_acc: 0.7640\n",
      "Epoch 916/1000\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 0.8301 - acc: 0.7971 - val_loss: 0.9520 - val_acc: 0.7570\n",
      "Epoch 917/1000\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 0.8286 - acc: 0.7961 - val_loss: 0.9301 - val_acc: 0.7600\n",
      "Epoch 918/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.8246 - acc: 0.7997 - val_loss: 0.9261 - val_acc: 0.7630\n",
      "Epoch 919/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.8236 - acc: 0.8007 - val_loss: 0.9330 - val_acc: 0.7590\n",
      "Epoch 920/1000\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 0.8201 - acc: 0.8012 - val_loss: 0.9213 - val_acc: 0.7640\n",
      "Epoch 921/1000\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 0.8211 - acc: 0.8013 - val_loss: 0.9302 - val_acc: 0.7650\n",
      "Epoch 922/1000\n",
      "7500/7500 [==============================] - 0s 22us/step - loss: 0.8215 - acc: 0.8023 - val_loss: 0.9374 - val_acc: 0.7700\n",
      "Epoch 923/1000\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 0.8345 - acc: 0.7981 - val_loss: 0.9190 - val_acc: 0.7720\n",
      "Epoch 924/1000\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 0.8244 - acc: 0.8000 - val_loss: 0.9519 - val_acc: 0.7570\n",
      "Epoch 925/1000\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 0.8193 - acc: 0.8021 - val_loss: 0.9517 - val_acc: 0.7580\n",
      "Epoch 926/1000\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 0.8231 - acc: 0.8000 - val_loss: 0.9388 - val_acc: 0.7590\n",
      "Epoch 927/1000\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 0.8185 - acc: 0.8056 - val_loss: 0.9360 - val_acc: 0.7600\n",
      "Epoch 928/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.8205 - acc: 0.8019 - val_loss: 0.9184 - val_acc: 0.7660\n",
      "Epoch 929/1000\n",
      "7500/7500 [==============================] - 0s 22us/step - loss: 0.8213 - acc: 0.8013 - val_loss: 0.9919 - val_acc: 0.7440\n",
      "Epoch 930/1000\n",
      "7500/7500 [==============================] - 0s 28us/step - loss: 0.8377 - acc: 0.7944 - val_loss: 0.9403 - val_acc: 0.7550\n",
      "Epoch 931/1000\n",
      "7500/7500 [==============================] - 0s 21us/step - loss: 0.8185 - acc: 0.8025 - val_loss: 0.9334 - val_acc: 0.7660\n",
      "Epoch 932/1000\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 0.8231 - acc: 0.8011 - val_loss: 0.9294 - val_acc: 0.7600\n",
      "Epoch 933/1000\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 0.8283 - acc: 0.7975 - val_loss: 0.9446 - val_acc: 0.7550\n",
      "Epoch 934/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.8237 - acc: 0.7995 - val_loss: 0.9282 - val_acc: 0.7670\n",
      "Epoch 935/1000\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 0.8197 - acc: 0.8017 - val_loss: 0.9221 - val_acc: 0.7610\n",
      "Epoch 936/1000\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 0.8186 - acc: 0.8040 - val_loss: 0.9960 - val_acc: 0.7430\n",
      "Epoch 937/1000\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 0.8247 - acc: 0.8011 - val_loss: 0.9154 - val_acc: 0.7680\n",
      "Epoch 938/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.8207 - acc: 0.8001 - val_loss: 0.9202 - val_acc: 0.7600\n",
      "Epoch 939/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.8191 - acc: 0.8027 - val_loss: 0.9176 - val_acc: 0.7630\n",
      "Epoch 940/1000\n",
      "7500/7500 [==============================] - 0s 21us/step - loss: 0.8225 - acc: 0.8016 - val_loss: 0.9185 - val_acc: 0.7580\n",
      "Epoch 941/1000\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 0.8243 - acc: 0.8008 - val_loss: 0.9831 - val_acc: 0.7470\n",
      "Epoch 942/1000\n",
      "7500/7500 [==============================] - 0s 24us/step - loss: 0.8344 - acc: 0.7941 - val_loss: 0.9231 - val_acc: 0.7680\n",
      "Epoch 943/1000\n",
      "7500/7500 [==============================] - 0s 21us/step - loss: 0.8180 - acc: 0.8027 - val_loss: 0.9263 - val_acc: 0.7710\n",
      "Epoch 944/1000\n",
      "7500/7500 [==============================] - 0s 21us/step - loss: 0.8218 - acc: 0.8013 - val_loss: 0.9287 - val_acc: 0.7700\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 945/1000\n",
      "7500/7500 [==============================] - 0s 21us/step - loss: 0.8208 - acc: 0.8013 - val_loss: 0.9225 - val_acc: 0.7710\n",
      "Epoch 946/1000\n",
      "7500/7500 [==============================] - 0s 21us/step - loss: 0.8211 - acc: 0.8011 - val_loss: 0.9298 - val_acc: 0.7650\n",
      "Epoch 947/1000\n",
      "7500/7500 [==============================] - 0s 22us/step - loss: 0.8331 - acc: 0.7975 - val_loss: 0.9399 - val_acc: 0.7600\n",
      "Epoch 948/1000\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 0.8252 - acc: 0.8001 - val_loss: 0.9310 - val_acc: 0.7690\n",
      "Epoch 949/1000\n",
      "7500/7500 [==============================] - 0s 24us/step - loss: 0.8178 - acc: 0.8021 - val_loss: 0.9396 - val_acc: 0.7580\n",
      "Epoch 950/1000\n",
      "7500/7500 [==============================] - 0s 21us/step - loss: 0.8243 - acc: 0.7991 - val_loss: 0.9222 - val_acc: 0.7690\n",
      "Epoch 951/1000\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 0.8174 - acc: 0.8035 - val_loss: 0.9601 - val_acc: 0.7500\n",
      "Epoch 952/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.8236 - acc: 0.7991 - val_loss: 0.9424 - val_acc: 0.7610\n",
      "Epoch 953/1000\n",
      "7500/7500 [==============================] - 0s 21us/step - loss: 0.8215 - acc: 0.8003 - val_loss: 0.9191 - val_acc: 0.7680\n",
      "Epoch 954/1000\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 0.8176 - acc: 0.8031 - val_loss: 0.9322 - val_acc: 0.7590\n",
      "Epoch 955/1000\n",
      "7500/7500 [==============================] - 0s 22us/step - loss: 0.8234 - acc: 0.7997 - val_loss: 0.9345 - val_acc: 0.7640\n",
      "Epoch 956/1000\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 0.8189 - acc: 0.8012 - val_loss: 0.9428 - val_acc: 0.7620\n",
      "Epoch 957/1000\n",
      "7500/7500 [==============================] - 0s 22us/step - loss: 0.8171 - acc: 0.8029 - val_loss: 0.9381 - val_acc: 0.7590\n",
      "Epoch 958/1000\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 0.8214 - acc: 0.8017 - val_loss: 0.9233 - val_acc: 0.7590\n",
      "Epoch 959/1000\n",
      "7500/7500 [==============================] - 0s 21us/step - loss: 0.8194 - acc: 0.8016 - val_loss: 0.9207 - val_acc: 0.7650\n",
      "Epoch 960/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.8170 - acc: 0.8024 - val_loss: 0.9263 - val_acc: 0.7620\n",
      "Epoch 961/1000\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 0.8171 - acc: 0.8024 - val_loss: 0.9206 - val_acc: 0.7600\n",
      "Epoch 962/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.8222 - acc: 0.8004 - val_loss: 0.9312 - val_acc: 0.7640\n",
      "Epoch 963/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.8214 - acc: 0.8005 - val_loss: 0.9267 - val_acc: 0.7680\n",
      "Epoch 964/1000\n",
      "7500/7500 [==============================] - 0s 21us/step - loss: 0.8169 - acc: 0.8023 - val_loss: 0.9401 - val_acc: 0.7600\n",
      "Epoch 965/1000\n",
      "7500/7500 [==============================] - 0s 21us/step - loss: 0.8216 - acc: 0.8025 - val_loss: 0.9920 - val_acc: 0.7430\n",
      "Epoch 966/1000\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 0.8244 - acc: 0.8004 - val_loss: 1.0152 - val_acc: 0.7390\n",
      "Epoch 967/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.8210 - acc: 0.8033 - val_loss: 0.9241 - val_acc: 0.7640\n",
      "Epoch 968/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.8181 - acc: 0.8028 - val_loss: 0.9796 - val_acc: 0.7380\n",
      "Epoch 969/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.8265 - acc: 0.8024 - val_loss: 0.9248 - val_acc: 0.7630\n",
      "Epoch 970/1000\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 0.8250 - acc: 0.7992 - val_loss: 0.9272 - val_acc: 0.7670\n",
      "Epoch 971/1000\n",
      "7500/7500 [==============================] - 0s 21us/step - loss: 0.8188 - acc: 0.8024 - val_loss: 0.9319 - val_acc: 0.7620\n",
      "Epoch 972/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.8164 - acc: 0.8041 - val_loss: 0.9220 - val_acc: 0.7670\n",
      "Epoch 973/1000\n",
      "7500/7500 [==============================] - 0s 28us/step - loss: 0.8191 - acc: 0.8011 - val_loss: 0.9471 - val_acc: 0.7510\n",
      "Epoch 974/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.8225 - acc: 0.8004 - val_loss: 0.9181 - val_acc: 0.7640\n",
      "Epoch 975/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.8146 - acc: 0.8052 - val_loss: 0.9363 - val_acc: 0.7700\n",
      "Epoch 976/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.8156 - acc: 0.8033 - val_loss: 0.9166 - val_acc: 0.7690\n",
      "Epoch 977/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.8203 - acc: 0.8008 - val_loss: 0.9698 - val_acc: 0.7530\n",
      "Epoch 978/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.8252 - acc: 0.8007 - val_loss: 0.9215 - val_acc: 0.7690\n",
      "Epoch 979/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.8199 - acc: 0.8024 - val_loss: 0.9337 - val_acc: 0.7620\n",
      "Epoch 980/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.8164 - acc: 0.8055 - val_loss: 0.9306 - val_acc: 0.7610\n",
      "Epoch 981/1000\n",
      "7500/7500 [==============================] - 0s 21us/step - loss: 0.8222 - acc: 0.7995 - val_loss: 0.9383 - val_acc: 0.7600\n",
      "Epoch 982/1000\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 0.8176 - acc: 0.8016 - val_loss: 0.9305 - val_acc: 0.7630\n",
      "Epoch 983/1000\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 0.8221 - acc: 0.7991 - val_loss: 0.9142 - val_acc: 0.7630\n",
      "Epoch 984/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.8137 - acc: 0.8051 - val_loss: 0.9314 - val_acc: 0.7670\n",
      "Epoch 985/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.8194 - acc: 0.8035 - val_loss: 0.9196 - val_acc: 0.7600\n",
      "Epoch 986/1000\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 0.8148 - acc: 0.8043 - val_loss: 0.9726 - val_acc: 0.7520\n",
      "Epoch 987/1000\n",
      "7500/7500 [==============================] - 0s 21us/step - loss: 0.8181 - acc: 0.7992 - val_loss: 1.0236 - val_acc: 0.7270\n",
      "Epoch 988/1000\n",
      "7500/7500 [==============================] - 0s 22us/step - loss: 0.8219 - acc: 0.8028 - val_loss: 0.9297 - val_acc: 0.7620\n",
      "Epoch 989/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.8190 - acc: 0.8015 - val_loss: 0.9386 - val_acc: 0.7600\n",
      "Epoch 990/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.8272 - acc: 0.8024 - val_loss: 0.9323 - val_acc: 0.7590\n",
      "Epoch 991/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.8254 - acc: 0.8007 - val_loss: 0.9192 - val_acc: 0.7690\n",
      "Epoch 992/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.8143 - acc: 0.8013 - val_loss: 0.9157 - val_acc: 0.7690\n",
      "Epoch 993/1000\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 0.8164 - acc: 0.8011 - val_loss: 1.0902 - val_acc: 0.7090\n",
      "Epoch 994/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.8219 - acc: 0.8028 - val_loss: 0.9143 - val_acc: 0.7670\n",
      "Epoch 995/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.8147 - acc: 0.8048 - val_loss: 0.9236 - val_acc: 0.7650\n",
      "Epoch 996/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.8145 - acc: 0.8048 - val_loss: 0.9106 - val_acc: 0.7680\n",
      "Epoch 997/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.8131 - acc: 0.8024 - val_loss: 0.9335 - val_acc: 0.7710\n",
      "Epoch 998/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.8134 - acc: 0.8036 - val_loss: 0.9257 - val_acc: 0.7570\n",
      "Epoch 999/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.8182 - acc: 0.8020 - val_loss: 0.9457 - val_acc: 0.7570\n",
      "Epoch 1000/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.8186 - acc: 0.8004 - val_loss: 0.9768 - val_acc: 0.7480\n"
     ]
    }
   ],
   "source": [
    "random.seed(123)\n",
    "model = models.Sequential()\n",
    "model.add(layers.Dense(50, activation='relu',kernel_regularizer=regularizers.l1(0.005), input_shape=(2000,))) #2 hidden layers\n",
    "model.add(layers.Dense(25, kernel_regularizer=regularizers.l1(0.005), activation='relu'))\n",
    "model.add(layers.Dense(7, activation='softmax'))\n",
    "\n",
    "model.compile(optimizer='SGD',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "L1_model = model.fit(train_final,\n",
    "                    label_train_final,\n",
    "                    epochs=1000,\n",
    "                    batch_size=256,\n",
    "                    validation_data=(val, label_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "L1_model_dict = L1_model.history\n",
    "plt.clf()\n",
    "\n",
    "acc_values = L1_model_dict['acc'] \n",
    "val_acc_values = L1_model_dict['val_acc']\n",
    "\n",
    "epochs = range(1, len(acc_values) + 1)\n",
    "plt.plot(epochs, acc_values, 'g', label='Training acc L1')\n",
    "plt.plot(epochs, val_acc_values, 'g,', label='Validation acc L1')\n",
    "plt.title('Training & validation accuracy L2 vs regular')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7500/7500 [==============================] - 0s 30us/step\n",
      "1500/1500 [==============================] - 0s 31us/step\n"
     ]
    }
   ],
   "source": [
    "results_train = model.evaluate(train_final, label_train_final)\n",
    "\n",
    "results_test = model.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.85600242506663, 0.7858666666348775]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1.0026838165918985, 0.7233333328564961]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is about the best we've seen so far, but we were training for quite a while! Let's see if dropout regularization can do even better and/or be more efficient!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dropout Regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 7500 samples, validate on 1000 samples\n",
      "Epoch 1/200\n",
      "7500/7500 [==============================] - 0s 62us/step - loss: 1.9660 - acc: 0.1797 - val_loss: 1.9078 - val_acc: 0.1920\n",
      "Epoch 2/200\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 1.9407 - acc: 0.1865 - val_loss: 1.8972 - val_acc: 0.2100\n",
      "Epoch 3/200\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 1.9212 - acc: 0.1900 - val_loss: 1.8861 - val_acc: 0.2270\n",
      "Epoch 4/200\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 1.9104 - acc: 0.1977 - val_loss: 1.8767 - val_acc: 0.2410\n",
      "Epoch 5/200\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 1.8983 - acc: 0.2056 - val_loss: 1.8643 - val_acc: 0.2420\n",
      "Epoch 6/200\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 1.8896 - acc: 0.2079 - val_loss: 1.8509 - val_acc: 0.2510\n",
      "Epoch 7/200\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 1.8811 - acc: 0.2099 - val_loss: 1.8376 - val_acc: 0.2590\n",
      "Epoch 8/200\n",
      "7500/7500 [==============================] - 0s 27us/step - loss: 1.8658 - acc: 0.2303 - val_loss: 1.8210 - val_acc: 0.2850\n",
      "Epoch 9/200\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 1.8546 - acc: 0.2375 - val_loss: 1.8025 - val_acc: 0.3070\n",
      "Epoch 10/200\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 1.8424 - acc: 0.2441 - val_loss: 1.7817 - val_acc: 0.3330\n",
      "Epoch 11/200\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 1.8284 - acc: 0.2607 - val_loss: 1.7609 - val_acc: 0.3590\n",
      "Epoch 12/200\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 1.8124 - acc: 0.2657 - val_loss: 1.7379 - val_acc: 0.3910\n",
      "Epoch 13/200\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 1.7920 - acc: 0.2887 - val_loss: 1.7116 - val_acc: 0.4210\n",
      "Epoch 14/200\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 1.7650 - acc: 0.2987 - val_loss: 1.6808 - val_acc: 0.4220\n",
      "Epoch 15/200\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 1.7466 - acc: 0.3073 - val_loss: 1.6515 - val_acc: 0.4440\n",
      "Epoch 16/200\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 1.7322 - acc: 0.3204 - val_loss: 1.6211 - val_acc: 0.4640\n",
      "Epoch 17/200\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 1.7087 - acc: 0.3329 - val_loss: 1.5901 - val_acc: 0.4800\n",
      "Epoch 18/200\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 1.6863 - acc: 0.3444 - val_loss: 1.5592 - val_acc: 0.4960\n",
      "Epoch 19/200\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 1.6624 - acc: 0.3632 - val_loss: 1.5318 - val_acc: 0.5090\n",
      "Epoch 20/200\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 1.6546 - acc: 0.3608 - val_loss: 1.5048 - val_acc: 0.5230\n",
      "Epoch 21/200\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 1.6301 - acc: 0.3727 - val_loss: 1.4782 - val_acc: 0.5250\n",
      "Epoch 22/200\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 1.6027 - acc: 0.3804 - val_loss: 1.4487 - val_acc: 0.5370\n",
      "Epoch 23/200\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 1.5838 - acc: 0.3925 - val_loss: 1.4193 - val_acc: 0.5530\n",
      "Epoch 24/200\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 1.5654 - acc: 0.3981 - val_loss: 1.3951 - val_acc: 0.5650\n",
      "Epoch 25/200\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 1.5514 - acc: 0.4044 - val_loss: 1.3741 - val_acc: 0.5750\n",
      "Epoch 26/200\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 1.5283 - acc: 0.4191 - val_loss: 1.3462 - val_acc: 0.5760\n",
      "Epoch 27/200\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 1.5047 - acc: 0.4221 - val_loss: 1.3253 - val_acc: 0.5870\n",
      "Epoch 28/200\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 1.4879 - acc: 0.4300 - val_loss: 1.3008 - val_acc: 0.6000\n",
      "Epoch 29/200\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 1.4721 - acc: 0.4391 - val_loss: 1.2792 - val_acc: 0.5950\n",
      "Epoch 30/200\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 1.4570 - acc: 0.4472 - val_loss: 1.2593 - val_acc: 0.6010\n",
      "Epoch 31/200\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 1.4419 - acc: 0.4479 - val_loss: 1.2437 - val_acc: 0.6070\n",
      "Epoch 32/200\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 1.4329 - acc: 0.4511 - val_loss: 1.2233 - val_acc: 0.6070\n",
      "Epoch 33/200\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 1.4148 - acc: 0.4584 - val_loss: 1.2061 - val_acc: 0.6140\n",
      "Epoch 34/200\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 1.3939 - acc: 0.4676 - val_loss: 1.1888 - val_acc: 0.6200\n",
      "Epoch 35/200\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 1.3798 - acc: 0.4705 - val_loss: 1.1720 - val_acc: 0.6290\n",
      "Epoch 36/200\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 1.3768 - acc: 0.4685 - val_loss: 1.1593 - val_acc: 0.6340\n",
      "Epoch 37/200\n",
      "7500/7500 [==============================] - 0s 27us/step - loss: 1.3557 - acc: 0.4836 - val_loss: 1.1423 - val_acc: 0.6380\n",
      "Epoch 38/200\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 1.3419 - acc: 0.4979 - val_loss: 1.1324 - val_acc: 0.6500\n",
      "Epoch 39/200\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 1.3228 - acc: 0.4937 - val_loss: 1.1126 - val_acc: 0.6430\n",
      "Epoch 40/200\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 1.3160 - acc: 0.5020 - val_loss: 1.1011 - val_acc: 0.6480\n",
      "Epoch 41/200\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 1.3152 - acc: 0.4952 - val_loss: 1.0890 - val_acc: 0.6530\n",
      "Epoch 42/200\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 1.2909 - acc: 0.5057 - val_loss: 1.0750 - val_acc: 0.6600\n",
      "Epoch 43/200\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 1.2880 - acc: 0.5053 - val_loss: 1.0653 - val_acc: 0.6620\n",
      "Epoch 44/200\n",
      "7500/7500 [==============================] - 0s 27us/step - loss: 1.2824 - acc: 0.5104 - val_loss: 1.0551 - val_acc: 0.6710\n",
      "Epoch 45/200\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 1.2637 - acc: 0.5269 - val_loss: 1.0406 - val_acc: 0.6710\n",
      "Epoch 46/200\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 1.2507 - acc: 0.5221 - val_loss: 1.0295 - val_acc: 0.6740\n",
      "Epoch 47/200\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 1.2472 - acc: 0.5217 - val_loss: 1.0187 - val_acc: 0.6790\n",
      "Epoch 48/200\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 1.2346 - acc: 0.5309 - val_loss: 1.0082 - val_acc: 0.6840\n",
      "Epoch 49/200\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 1.2347 - acc: 0.5341 - val_loss: 1.0012 - val_acc: 0.6840\n",
      "Epoch 50/200\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 1.2137 - acc: 0.5385 - val_loss: 0.9908 - val_acc: 0.6850\n",
      "Epoch 51/200\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 1.2094 - acc: 0.5420 - val_loss: 0.9798 - val_acc: 0.6880\n",
      "Epoch 52/200\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 1.2032 - acc: 0.5453 - val_loss: 0.9712 - val_acc: 0.6920\n",
      "Epoch 53/200\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 1.1943 - acc: 0.5467 - val_loss: 0.9644 - val_acc: 0.6900\n",
      "Epoch 54/200\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 1.1856 - acc: 0.5525 - val_loss: 0.9574 - val_acc: 0.7030\n",
      "Epoch 55/200\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 1.1739 - acc: 0.5549 - val_loss: 0.9440 - val_acc: 0.7020\n",
      "Epoch 56/200\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 1.1732 - acc: 0.5549 - val_loss: 0.9362 - val_acc: 0.7010\n",
      "Epoch 57/200\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 1.1549 - acc: 0.5645 - val_loss: 0.9280 - val_acc: 0.7050\n",
      "Epoch 58/200\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 1.1587 - acc: 0.5545 - val_loss: 0.9256 - val_acc: 0.7130\n",
      "Epoch 59/200\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 1.1402 - acc: 0.5745 - val_loss: 0.9134 - val_acc: 0.7060\n",
      "Epoch 60/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7500/7500 [==============================] - 0s 26us/step - loss: 1.1375 - acc: 0.5685 - val_loss: 0.9071 - val_acc: 0.7100\n",
      "Epoch 61/200\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 1.1284 - acc: 0.5721 - val_loss: 0.9012 - val_acc: 0.7080\n",
      "Epoch 62/200\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 1.1225 - acc: 0.5724 - val_loss: 0.8942 - val_acc: 0.7110\n",
      "Epoch 63/200\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 1.1370 - acc: 0.5645 - val_loss: 0.8894 - val_acc: 0.7160\n",
      "Epoch 64/200\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 1.1178 - acc: 0.5805 - val_loss: 0.8833 - val_acc: 0.7160\n",
      "Epoch 65/200\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 1.1206 - acc: 0.5800 - val_loss: 0.8778 - val_acc: 0.7230\n",
      "Epoch 66/200\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 1.1025 - acc: 0.5792 - val_loss: 0.8714 - val_acc: 0.7250\n",
      "Epoch 67/200\n",
      "7500/7500 [==============================] - 0s 29us/step - loss: 1.0892 - acc: 0.5935 - val_loss: 0.8629 - val_acc: 0.7280\n",
      "Epoch 68/200\n",
      "7500/7500 [==============================] - 0s 31us/step - loss: 1.0835 - acc: 0.5916 - val_loss: 0.8570 - val_acc: 0.7270\n",
      "Epoch 69/200\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 1.0716 - acc: 0.6001 - val_loss: 0.8512 - val_acc: 0.7290\n",
      "Epoch 70/200\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 1.0767 - acc: 0.5943 - val_loss: 0.8454 - val_acc: 0.7340\n",
      "Epoch 71/200\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 1.0725 - acc: 0.5941 - val_loss: 0.8408 - val_acc: 0.7300\n",
      "Epoch 72/200\n",
      "7500/7500 [==============================] - 0s 32us/step - loss: 1.0609 - acc: 0.6005 - val_loss: 0.8343 - val_acc: 0.7290\n",
      "Epoch 73/200\n",
      "7500/7500 [==============================] - 0s 28us/step - loss: 1.0594 - acc: 0.6056 - val_loss: 0.8291 - val_acc: 0.7350\n",
      "Epoch 74/200\n",
      "7500/7500 [==============================] - 0s 27us/step - loss: 1.0624 - acc: 0.6068 - val_loss: 0.8236 - val_acc: 0.7320\n",
      "Epoch 75/200\n",
      "7500/7500 [==============================] - 0s 29us/step - loss: 1.0330 - acc: 0.6156 - val_loss: 0.8152 - val_acc: 0.7340\n",
      "Epoch 76/200\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 1.0482 - acc: 0.6056 - val_loss: 0.8139 - val_acc: 0.7350\n",
      "Epoch 77/200\n",
      "7500/7500 [==============================] - 0s 32us/step - loss: 1.0483 - acc: 0.6080 - val_loss: 0.8096 - val_acc: 0.7350\n",
      "Epoch 78/200\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 1.0321 - acc: 0.6140 - val_loss: 0.8042 - val_acc: 0.7370\n",
      "Epoch 79/200\n",
      "7500/7500 [==============================] - 0s 30us/step - loss: 1.0269 - acc: 0.6113 - val_loss: 0.8006 - val_acc: 0.7380\n",
      "Epoch 80/200\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 1.0290 - acc: 0.6261 - val_loss: 0.7965 - val_acc: 0.7370\n",
      "Epoch 81/200\n",
      "7500/7500 [==============================] - 0s 30us/step - loss: 1.0140 - acc: 0.6227 - val_loss: 0.7916 - val_acc: 0.7380\n",
      "Epoch 82/200\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 1.0276 - acc: 0.6185 - val_loss: 0.7923 - val_acc: 0.7390\n",
      "Epoch 83/200\n",
      "7500/7500 [==============================] - 0s 31us/step - loss: 1.0087 - acc: 0.6315 - val_loss: 0.7859 - val_acc: 0.7440\n",
      "Epoch 84/200\n",
      "7500/7500 [==============================] - 0s 29us/step - loss: 1.0177 - acc: 0.6248 - val_loss: 0.7811 - val_acc: 0.7470\n",
      "Epoch 85/200\n",
      "7500/7500 [==============================] - 0s 24us/step - loss: 1.0075 - acc: 0.6235 - val_loss: 0.7788 - val_acc: 0.7460\n",
      "Epoch 86/200\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 0.9974 - acc: 0.6244 - val_loss: 0.7752 - val_acc: 0.7460\n",
      "Epoch 87/200\n",
      "7500/7500 [==============================] - 0s 24us/step - loss: 0.9936 - acc: 0.6315 - val_loss: 0.7706 - val_acc: 0.7400\n",
      "Epoch 88/200\n",
      "7500/7500 [==============================] - 0s 24us/step - loss: 0.9912 - acc: 0.6355 - val_loss: 0.7667 - val_acc: 0.7440\n",
      "Epoch 89/200\n",
      "7500/7500 [==============================] - 0s 24us/step - loss: 0.9856 - acc: 0.6349 - val_loss: 0.7609 - val_acc: 0.7460\n",
      "Epoch 90/200\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 0.9776 - acc: 0.6361 - val_loss: 0.7578 - val_acc: 0.7460\n",
      "Epoch 91/200\n",
      "7500/7500 [==============================] - 0s 24us/step - loss: 0.9937 - acc: 0.6285 - val_loss: 0.7570 - val_acc: 0.7490\n",
      "Epoch 92/200\n",
      "7500/7500 [==============================] - 0s 24us/step - loss: 0.9840 - acc: 0.6333 - val_loss: 0.7521 - val_acc: 0.7460\n",
      "Epoch 93/200\n",
      "7500/7500 [==============================] - 0s 24us/step - loss: 0.9786 - acc: 0.6296 - val_loss: 0.7479 - val_acc: 0.7480\n",
      "Epoch 94/200\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 0.9620 - acc: 0.6423 - val_loss: 0.7456 - val_acc: 0.7510\n",
      "Epoch 95/200\n",
      "7500/7500 [==============================] - 0s 28us/step - loss: 0.9704 - acc: 0.6440 - val_loss: 0.7423 - val_acc: 0.7500\n",
      "Epoch 96/200\n",
      "7500/7500 [==============================] - 0s 28us/step - loss: 0.9652 - acc: 0.6393 - val_loss: 0.7393 - val_acc: 0.7490\n",
      "Epoch 97/200\n",
      "7500/7500 [==============================] - 0s 28us/step - loss: 0.9670 - acc: 0.6371 - val_loss: 0.7391 - val_acc: 0.7510\n",
      "Epoch 98/200\n",
      "7500/7500 [==============================] - 0s 28us/step - loss: 0.9586 - acc: 0.6448 - val_loss: 0.7375 - val_acc: 0.7520\n",
      "Epoch 99/200\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 0.9498 - acc: 0.6504 - val_loss: 0.7327 - val_acc: 0.7520\n",
      "Epoch 100/200\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 0.9284 - acc: 0.6563 - val_loss: 0.7296 - val_acc: 0.7500\n",
      "Epoch 101/200\n",
      "7500/7500 [==============================] - 0s 27us/step - loss: 0.9445 - acc: 0.6461 - val_loss: 0.7266 - val_acc: 0.7540\n",
      "Epoch 102/200\n",
      "7500/7500 [==============================] - 0s 29us/step - loss: 0.9499 - acc: 0.6453 - val_loss: 0.7239 - val_acc: 0.7510\n",
      "Epoch 103/200\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 0.9335 - acc: 0.6504 - val_loss: 0.7203 - val_acc: 0.7520\n",
      "Epoch 104/200\n",
      "7500/7500 [==============================] - 0s 24us/step - loss: 0.9387 - acc: 0.6473 - val_loss: 0.7156 - val_acc: 0.7550\n",
      "Epoch 105/200\n",
      "7500/7500 [==============================] - 0s 24us/step - loss: 0.9378 - acc: 0.6557 - val_loss: 0.7128 - val_acc: 0.7560\n",
      "Epoch 106/200\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 0.9488 - acc: 0.6499 - val_loss: 0.7129 - val_acc: 0.7550\n",
      "Epoch 107/200\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 0.9325 - acc: 0.6583 - val_loss: 0.7122 - val_acc: 0.7580\n",
      "Epoch 108/200\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 0.9212 - acc: 0.6615 - val_loss: 0.7084 - val_acc: 0.7580\n",
      "Epoch 109/200\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 0.9044 - acc: 0.6667 - val_loss: 0.7041 - val_acc: 0.7590\n",
      "Epoch 110/200\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 0.9221 - acc: 0.6567 - val_loss: 0.7029 - val_acc: 0.7580\n",
      "Epoch 111/200\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 0.9017 - acc: 0.6645 - val_loss: 0.7011 - val_acc: 0.7600\n",
      "Epoch 112/200\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 0.9059 - acc: 0.6611 - val_loss: 0.6994 - val_acc: 0.7560\n",
      "Epoch 113/200\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 0.8972 - acc: 0.6675 - val_loss: 0.6951 - val_acc: 0.7580\n",
      "Epoch 114/200\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 0.9121 - acc: 0.6628 - val_loss: 0.6934 - val_acc: 0.7560\n",
      "Epoch 115/200\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 0.8987 - acc: 0.6624 - val_loss: 0.6916 - val_acc: 0.7590\n",
      "Epoch 116/200\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 0.8936 - acc: 0.6685 - val_loss: 0.6901 - val_acc: 0.7580\n",
      "Epoch 117/200\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 0.8851 - acc: 0.6767 - val_loss: 0.6881 - val_acc: 0.7600\n",
      "Epoch 118/200\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 0.8950 - acc: 0.6681 - val_loss: 0.6879 - val_acc: 0.7610\n",
      "Epoch 119/200\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 0.8920 - acc: 0.6676 - val_loss: 0.6840 - val_acc: 0.7600\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 120/200\n",
      "7500/7500 [==============================] - 0s 24us/step - loss: 0.8872 - acc: 0.6703 - val_loss: 0.6816 - val_acc: 0.7600\n",
      "Epoch 121/200\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 0.8848 - acc: 0.6741 - val_loss: 0.6777 - val_acc: 0.7620\n",
      "Epoch 122/200\n",
      "7500/7500 [==============================] - 0s 24us/step - loss: 0.8785 - acc: 0.6713 - val_loss: 0.6778 - val_acc: 0.7590\n",
      "Epoch 123/200\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 0.8847 - acc: 0.6684 - val_loss: 0.6768 - val_acc: 0.7630\n",
      "Epoch 124/200\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 0.8901 - acc: 0.6692 - val_loss: 0.6750 - val_acc: 0.7600\n",
      "Epoch 125/200\n",
      "7500/7500 [==============================] - 0s 24us/step - loss: 0.8754 - acc: 0.6731 - val_loss: 0.6739 - val_acc: 0.7600\n",
      "Epoch 126/200\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 0.8668 - acc: 0.6797 - val_loss: 0.6724 - val_acc: 0.7620\n",
      "Epoch 127/200\n",
      "7500/7500 [==============================] - 0s 24us/step - loss: 0.8576 - acc: 0.6829 - val_loss: 0.6706 - val_acc: 0.7640\n",
      "Epoch 128/200\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 0.8516 - acc: 0.6828 - val_loss: 0.6654 - val_acc: 0.7620\n",
      "Epoch 129/200\n",
      "7500/7500 [==============================] - 0s 24us/step - loss: 0.8537 - acc: 0.6837 - val_loss: 0.6642 - val_acc: 0.7650\n",
      "Epoch 130/200\n",
      "7500/7500 [==============================] - 0s 24us/step - loss: 0.8651 - acc: 0.6783 - val_loss: 0.6636 - val_acc: 0.7620\n",
      "Epoch 131/200\n",
      "7500/7500 [==============================] - 0s 24us/step - loss: 0.8611 - acc: 0.6815 - val_loss: 0.6613 - val_acc: 0.7630\n",
      "Epoch 132/200\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 0.8560 - acc: 0.6847 - val_loss: 0.6614 - val_acc: 0.7630\n",
      "Epoch 133/200\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 0.8421 - acc: 0.6827 - val_loss: 0.6567 - val_acc: 0.7640\n",
      "Epoch 134/200\n",
      "7500/7500 [==============================] - 0s 24us/step - loss: 0.8526 - acc: 0.6840 - val_loss: 0.6567 - val_acc: 0.7640\n",
      "Epoch 135/200\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 0.8553 - acc: 0.6772 - val_loss: 0.6587 - val_acc: 0.7630\n",
      "Epoch 136/200\n",
      "7500/7500 [==============================] - 0s 29us/step - loss: 0.8381 - acc: 0.6860 - val_loss: 0.6582 - val_acc: 0.7650\n",
      "Epoch 137/200\n",
      "7500/7500 [==============================] - 0s 27us/step - loss: 0.8400 - acc: 0.6811 - val_loss: 0.6546 - val_acc: 0.7680\n",
      "Epoch 138/200\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 0.8634 - acc: 0.6804 - val_loss: 0.6530 - val_acc: 0.7670\n",
      "Epoch 139/200\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 0.8512 - acc: 0.6813 - val_loss: 0.6536 - val_acc: 0.7670\n",
      "Epoch 140/200\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 0.8529 - acc: 0.6877 - val_loss: 0.6525 - val_acc: 0.7670\n",
      "Epoch 141/200\n",
      "7500/7500 [==============================] - 0s 29us/step - loss: 0.8278 - acc: 0.6928 - val_loss: 0.6470 - val_acc: 0.7690\n",
      "Epoch 142/200\n",
      "7500/7500 [==============================] - 0s 27us/step - loss: 0.8388 - acc: 0.6891 - val_loss: 0.6452 - val_acc: 0.7670\n",
      "Epoch 143/200\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 0.8290 - acc: 0.6917 - val_loss: 0.6457 - val_acc: 0.7690\n",
      "Epoch 144/200\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 0.8306 - acc: 0.6924 - val_loss: 0.6430 - val_acc: 0.7690\n",
      "Epoch 145/200\n",
      "7500/7500 [==============================] - 0s 29us/step - loss: 0.8262 - acc: 0.6959 - val_loss: 0.6392 - val_acc: 0.7690\n",
      "Epoch 146/200\n",
      "7500/7500 [==============================] - 0s 27us/step - loss: 0.8244 - acc: 0.6984 - val_loss: 0.6392 - val_acc: 0.7690\n",
      "Epoch 147/200\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 0.8274 - acc: 0.6907 - val_loss: 0.6396 - val_acc: 0.7710\n",
      "Epoch 148/200\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 0.8198 - acc: 0.6964 - val_loss: 0.6401 - val_acc: 0.7710\n",
      "Epoch 149/200\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 0.8134 - acc: 0.6953 - val_loss: 0.6337 - val_acc: 0.7720\n",
      "Epoch 150/200\n",
      "7500/7500 [==============================] - 0s 28us/step - loss: 0.8139 - acc: 0.6975 - val_loss: 0.6347 - val_acc: 0.7730\n",
      "Epoch 151/200\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 0.8049 - acc: 0.7041 - val_loss: 0.6327 - val_acc: 0.7720\n",
      "Epoch 152/200\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 0.8103 - acc: 0.6973 - val_loss: 0.6340 - val_acc: 0.7720\n",
      "Epoch 153/200\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 0.8114 - acc: 0.6952 - val_loss: 0.6328 - val_acc: 0.7750\n",
      "Epoch 154/200\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 0.7991 - acc: 0.6996 - val_loss: 0.6311 - val_acc: 0.7760\n",
      "Epoch 155/200\n",
      "7500/7500 [==============================] - 0s 29us/step - loss: 0.8076 - acc: 0.7021 - val_loss: 0.6288 - val_acc: 0.7750\n",
      "Epoch 156/200\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 0.7982 - acc: 0.7060 - val_loss: 0.6260 - val_acc: 0.7760\n",
      "Epoch 157/200\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 0.7958 - acc: 0.7003 - val_loss: 0.6246 - val_acc: 0.7750\n",
      "Epoch 158/200\n",
      "7500/7500 [==============================] - 0s 27us/step - loss: 0.8050 - acc: 0.6963 - val_loss: 0.6283 - val_acc: 0.7720\n",
      "Epoch 159/200\n",
      "7500/7500 [==============================] - 0s 24us/step - loss: 0.7853 - acc: 0.7073 - val_loss: 0.6252 - val_acc: 0.7720\n",
      "Epoch 160/200\n",
      "7500/7500 [==============================] - 0s 24us/step - loss: 0.7881 - acc: 0.7039 - val_loss: 0.6259 - val_acc: 0.7700\n",
      "Epoch 161/200\n",
      "7500/7500 [==============================] - 0s 27us/step - loss: 0.8015 - acc: 0.6989 - val_loss: 0.6246 - val_acc: 0.7740\n",
      "Epoch 162/200\n",
      "7500/7500 [==============================] - 0s 30us/step - loss: 0.7889 - acc: 0.7045 - val_loss: 0.6224 - val_acc: 0.7740\n",
      "Epoch 163/200\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 0.7963 - acc: 0.7065 - val_loss: 0.6216 - val_acc: 0.7700\n",
      "Epoch 164/200\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 0.7739 - acc: 0.7136 - val_loss: 0.6220 - val_acc: 0.7720\n",
      "Epoch 165/200\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 0.7939 - acc: 0.7093 - val_loss: 0.6195 - val_acc: 0.7750\n",
      "Epoch 166/200\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 0.7709 - acc: 0.7153 - val_loss: 0.6166 - val_acc: 0.7760\n",
      "Epoch 167/200\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 0.7876 - acc: 0.7021 - val_loss: 0.6168 - val_acc: 0.7730\n",
      "Epoch 168/200\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 0.7729 - acc: 0.7113 - val_loss: 0.6149 - val_acc: 0.7700\n",
      "Epoch 169/200\n",
      "7500/7500 [==============================] - 0s 24us/step - loss: 0.7991 - acc: 0.7017 - val_loss: 0.6151 - val_acc: 0.7700\n",
      "Epoch 170/200\n",
      "7500/7500 [==============================] - 0s 24us/step - loss: 0.7780 - acc: 0.7081 - val_loss: 0.6148 - val_acc: 0.7710\n",
      "Epoch 171/200\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 0.7971 - acc: 0.7015 - val_loss: 0.6163 - val_acc: 0.7740\n",
      "Epoch 172/200\n",
      "7500/7500 [==============================] - 0s 29us/step - loss: 0.7755 - acc: 0.7095 - val_loss: 0.6144 - val_acc: 0.7760\n",
      "Epoch 173/200\n",
      "7500/7500 [==============================] - 0s 29us/step - loss: 0.7665 - acc: 0.7139 - val_loss: 0.6132 - val_acc: 0.7770\n",
      "Epoch 174/200\n",
      "7500/7500 [==============================] - 0s 28us/step - loss: 0.7792 - acc: 0.7105 - val_loss: 0.6127 - val_acc: 0.7730\n",
      "Epoch 175/200\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 0.7749 - acc: 0.7125 - val_loss: 0.6151 - val_acc: 0.7710\n",
      "Epoch 176/200\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 0.7626 - acc: 0.7149 - val_loss: 0.6107 - val_acc: 0.7720\n",
      "Epoch 177/200\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.7653 - acc: 0.7169 - val_loss: 0.6101 - val_acc: 0.7730\n",
      "Epoch 178/200\n",
      "7500/7500 [==============================] - 0s 29us/step - loss: 0.7624 - acc: 0.7136 - val_loss: 0.6093 - val_acc: 0.7750\n",
      "Epoch 179/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7500/7500 [==============================] - 0s 26us/step - loss: 0.7697 - acc: 0.7131 - val_loss: 0.6072 - val_acc: 0.7740\n",
      "Epoch 180/200\n",
      "7500/7500 [==============================] - 0s 29us/step - loss: 0.7509 - acc: 0.7159 - val_loss: 0.6043 - val_acc: 0.7770\n",
      "Epoch 181/200\n",
      "7500/7500 [==============================] - 0s 27us/step - loss: 0.7572 - acc: 0.7156 - val_loss: 0.6016 - val_acc: 0.7760\n",
      "Epoch 182/200\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 0.7663 - acc: 0.7209 - val_loss: 0.6027 - val_acc: 0.7780\n",
      "Epoch 183/200\n",
      "7500/7500 [==============================] - 0s 46us/step - loss: 0.7744 - acc: 0.7116 - val_loss: 0.6040 - val_acc: 0.7750\n",
      "Epoch 184/200\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 0.7607 - acc: 0.7124 - val_loss: 0.6034 - val_acc: 0.7730\n",
      "Epoch 185/200\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 0.7615 - acc: 0.7185 - val_loss: 0.6026 - val_acc: 0.7780\n",
      "Epoch 186/200\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.7429 - acc: 0.7217 - val_loss: 0.5999 - val_acc: 0.7770\n",
      "Epoch 187/200\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.7516 - acc: 0.7243 - val_loss: 0.6023 - val_acc: 0.7740\n",
      "Epoch 188/200\n",
      "7500/7500 [==============================] - 0s 31us/step - loss: 0.7443 - acc: 0.7220 - val_loss: 0.6003 - val_acc: 0.7780\n",
      "Epoch 189/200\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 0.7604 - acc: 0.7160 - val_loss: 0.6010 - val_acc: 0.7770\n",
      "Epoch 190/200\n",
      "7500/7500 [==============================] - 0s 30us/step - loss: 0.7346 - acc: 0.7251 - val_loss: 0.5990 - val_acc: 0.7760\n",
      "Epoch 191/200\n",
      "7500/7500 [==============================] - 0s 29us/step - loss: 0.7429 - acc: 0.7205 - val_loss: 0.5982 - val_acc: 0.7770\n",
      "Epoch 192/200\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 0.7432 - acc: 0.7203 - val_loss: 0.5970 - val_acc: 0.7790\n",
      "Epoch 193/200\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 0.7328 - acc: 0.7263 - val_loss: 0.5962 - val_acc: 0.7780\n",
      "Epoch 194/200\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 0.7269 - acc: 0.7261 - val_loss: 0.5937 - val_acc: 0.7790\n",
      "Epoch 195/200\n",
      "7500/7500 [==============================] - 0s 27us/step - loss: 0.7474 - acc: 0.7204 - val_loss: 0.5934 - val_acc: 0.7780\n",
      "Epoch 196/200\n",
      "7500/7500 [==============================] - 0s 31us/step - loss: 0.7366 - acc: 0.7249 - val_loss: 0.5933 - val_acc: 0.7770\n",
      "Epoch 197/200\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 0.7387 - acc: 0.7255 - val_loss: 0.5951 - val_acc: 0.7810\n",
      "Epoch 198/200\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 0.7444 - acc: 0.7183 - val_loss: 0.5963 - val_acc: 0.7760\n",
      "Epoch 199/200\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 0.7353 - acc: 0.7297 - val_loss: 0.5936 - val_acc: 0.7760\n",
      "Epoch 200/200\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 0.7314 - acc: 0.7272 - val_loss: 0.5930 - val_acc: 0.7780\n"
     ]
    }
   ],
   "source": [
    "random.seed(123)\n",
    "model = models.Sequential()\n",
    "model.add(layers.Dropout(0.3, input_shape=(2000,)))\n",
    "model.add(layers.Dense(50, activation='relu')) #2 hidden layers\n",
    "model.add(layers.Dropout(0.3))\n",
    "model.add(layers.Dense(25, activation='relu'))\n",
    "model.add(layers.Dropout(0.3))\n",
    "model.add(layers.Dense(7, activation='softmax'))\n",
    "\n",
    "model.compile(optimizer='SGD',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "dropout_model = model.fit(train_final,\n",
    "                    label_train_final,\n",
    "                    epochs=200,\n",
    "                    batch_size=256,\n",
    "                    validation_data=(val, label_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7500/7500 [==============================] - 0s 32us/step\n",
      "1500/1500 [==============================] - 0s 31us/step\n"
     ]
    }
   ],
   "source": [
    "results_train = model.evaluate(train_final, label_train_final)\n",
    "results_test = model.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.4575060317357381, 0.8344000000317892]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.6355382310549418, 0.7579999995231629]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see here that the validation performance has improved again! the variance did become higher again compared to L1-regularization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bigger Data?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the lecture, one of the solutions to high variance was just getting more data. We actually *have* more data, but took a subset of 10,000 units before. Let's now quadruple our data set, and see what happens. Note that we are really just lucky here, and getting more data isn't always possible, but this is a useful exercise in order to understand the power of big data sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('Bank_complaints.csv')\n",
    "random.seed(123)\n",
    "df = df.sample(40000)\n",
    "df.index = range(40000)\n",
    "product = df[\"Product\"]\n",
    "complaints = df[\"Consumer complaint narrative\"]\n",
    "\n",
    "#one-hot encoding of the complaints\n",
    "tokenizer = Tokenizer(num_words=2000)\n",
    "tokenizer.fit_on_texts(complaints)\n",
    "sequences = tokenizer.texts_to_sequences(complaints)\n",
    "one_hot_results= tokenizer.texts_to_matrix(complaints, mode='binary')\n",
    "word_index = tokenizer.word_index\n",
    "np.shape(one_hot_results)\n",
    "\n",
    "#one-hot encoding of products\n",
    "le = preprocessing.LabelEncoder()\n",
    "le.fit(product)\n",
    "list(le.classes_)\n",
    "product_cat = le.transform(product) \n",
    "product_onehot = to_categorical(product_cat)\n",
    "\n",
    "# train test split\n",
    "test_index = random.sample(range(1,40000), 4000)\n",
    "test = one_hot_results[test_index]\n",
    "train = np.delete(one_hot_results, test_index, 0)\n",
    "label_test = product_onehot[test_index]\n",
    "label_train = np.delete(product_onehot, test_index, 0)\n",
    "\n",
    "#Validation set\n",
    "random.seed(123)\n",
    "val = train[:3000]\n",
    "train_final = train[3000:]\n",
    "label_val = label_train[:3000]\n",
    "label_train_final = label_train[3000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 33000 samples, validate on 3000 samples\n",
      "Epoch 1/120\n",
      "33000/33000 [==============================] - 1s 25us/step - loss: 1.9431 - acc: 0.1766 - val_loss: 1.9089 - val_acc: 0.2323\n",
      "Epoch 2/120\n",
      "33000/33000 [==============================] - 1s 17us/step - loss: 1.8800 - acc: 0.2588 - val_loss: 1.8463 - val_acc: 0.3000\n",
      "Epoch 3/120\n",
      "33000/33000 [==============================] - 1s 17us/step - loss: 1.7941 - acc: 0.3395 - val_loss: 1.7298 - val_acc: 0.3890\n",
      "Epoch 4/120\n",
      "33000/33000 [==============================] - 1s 17us/step - loss: 1.6487 - acc: 0.4402 - val_loss: 1.5549 - val_acc: 0.4803\n",
      "Epoch 5/120\n",
      "33000/33000 [==============================] - 1s 17us/step - loss: 1.4566 - acc: 0.5368 - val_loss: 1.3509 - val_acc: 0.5713\n",
      "Epoch 6/120\n",
      "33000/33000 [==============================] - 1s 17us/step - loss: 1.2541 - acc: 0.6184 - val_loss: 1.1572 - val_acc: 0.6427\n",
      "Epoch 7/120\n",
      "33000/33000 [==============================] - 1s 17us/step - loss: 1.0779 - acc: 0.6705 - val_loss: 1.0053 - val_acc: 0.6877\n",
      "Epoch 8/120\n",
      "33000/33000 [==============================] - 1s 17us/step - loss: 0.9461 - acc: 0.7004 - val_loss: 0.8960 - val_acc: 0.7100\n",
      "Epoch 9/120\n",
      "33000/33000 [==============================] - 1s 17us/step - loss: 0.8544 - acc: 0.7185 - val_loss: 0.8219 - val_acc: 0.7167\n",
      "Epoch 10/120\n",
      "33000/33000 [==============================] - 1s 18us/step - loss: 0.7900 - acc: 0.7329 - val_loss: 0.7726 - val_acc: 0.7310\n",
      "Epoch 11/120\n",
      "33000/33000 [==============================] - 1s 18us/step - loss: 0.7438 - acc: 0.7421 - val_loss: 0.7337 - val_acc: 0.7417\n",
      "Epoch 12/120\n",
      "33000/33000 [==============================] - 1s 17us/step - loss: 0.7085 - acc: 0.7526 - val_loss: 0.7053 - val_acc: 0.7523\n",
      "Epoch 13/120\n",
      "33000/33000 [==============================] - 1s 17us/step - loss: 0.6809 - acc: 0.7584 - val_loss: 0.6834 - val_acc: 0.7570\n",
      "Epoch 14/120\n",
      "33000/33000 [==============================] - 1s 16us/step - loss: 0.6585 - acc: 0.7652 - val_loss: 0.6661 - val_acc: 0.7607\n",
      "Epoch 15/120\n",
      "33000/33000 [==============================] - 1s 16us/step - loss: 0.6396 - acc: 0.7705 - val_loss: 0.6509 - val_acc: 0.7657\n",
      "Epoch 16/120\n",
      "33000/33000 [==============================] - 1s 17us/step - loss: 0.6235 - acc: 0.7756 - val_loss: 0.6376 - val_acc: 0.7670\n",
      "Epoch 17/120\n",
      "33000/33000 [==============================] - 1s 17us/step - loss: 0.6091 - acc: 0.7808 - val_loss: 0.6274 - val_acc: 0.7697\n",
      "Epoch 18/120\n",
      "33000/33000 [==============================] - 1s 17us/step - loss: 0.5967 - acc: 0.7859 - val_loss: 0.6178 - val_acc: 0.7773\n",
      "Epoch 19/120\n",
      "33000/33000 [==============================] - 1s 16us/step - loss: 0.5857 - acc: 0.7899 - val_loss: 0.6086 - val_acc: 0.7760\n",
      "Epoch 20/120\n",
      "33000/33000 [==============================] - 1s 16us/step - loss: 0.5755 - acc: 0.7927 - val_loss: 0.6028 - val_acc: 0.7787\n",
      "Epoch 21/120\n",
      "33000/33000 [==============================] - 1s 18us/step - loss: 0.5662 - acc: 0.7963 - val_loss: 0.5959 - val_acc: 0.7817\n",
      "Epoch 22/120\n",
      "33000/33000 [==============================] - 1s 16us/step - loss: 0.5575 - acc: 0.7991 - val_loss: 0.5914 - val_acc: 0.7873\n",
      "Epoch 23/120\n",
      "33000/33000 [==============================] - 1s 17us/step - loss: 0.5495 - acc: 0.8012 - val_loss: 0.5863 - val_acc: 0.7860\n",
      "Epoch 24/120\n",
      "33000/33000 [==============================] - 1s 20us/step - loss: 0.5419 - acc: 0.8048 - val_loss: 0.5809 - val_acc: 0.7863\n",
      "Epoch 25/120\n",
      "33000/33000 [==============================] - 1s 20us/step - loss: 0.5344 - acc: 0.8082 - val_loss: 0.5769 - val_acc: 0.7920\n",
      "Epoch 26/120\n",
      "33000/33000 [==============================] - 1s 18us/step - loss: 0.5280 - acc: 0.8098 - val_loss: 0.5728 - val_acc: 0.7930\n",
      "Epoch 27/120\n",
      "33000/33000 [==============================] - 1s 24us/step - loss: 0.5217 - acc: 0.8122 - val_loss: 0.5698 - val_acc: 0.7950\n",
      "Epoch 28/120\n",
      "33000/33000 [==============================] - 1s 21us/step - loss: 0.5156 - acc: 0.8147 - val_loss: 0.5691 - val_acc: 0.7980\n",
      "Epoch 29/120\n",
      "33000/33000 [==============================] - 1s 22us/step - loss: 0.5097 - acc: 0.8172 - val_loss: 0.5642 - val_acc: 0.7973\n",
      "Epoch 30/120\n",
      "33000/33000 [==============================] - 1s 24us/step - loss: 0.5041 - acc: 0.8195 - val_loss: 0.5621 - val_acc: 0.7983\n",
      "Epoch 31/120\n",
      "33000/33000 [==============================] - 1s 23us/step - loss: 0.4988 - acc: 0.8211 - val_loss: 0.5599 - val_acc: 0.8027\n",
      "Epoch 32/120\n",
      "33000/33000 [==============================] - 1s 21us/step - loss: 0.4938 - acc: 0.8235 - val_loss: 0.5563 - val_acc: 0.8000\n",
      "Epoch 33/120\n",
      "33000/33000 [==============================] - 1s 23us/step - loss: 0.4891 - acc: 0.8261 - val_loss: 0.5547 - val_acc: 0.8023\n",
      "Epoch 34/120\n",
      "33000/33000 [==============================] - 1s 24us/step - loss: 0.4844 - acc: 0.8268 - val_loss: 0.5548 - val_acc: 0.8047\n",
      "Epoch 35/120\n",
      "33000/33000 [==============================] - 1s 24us/step - loss: 0.4799 - acc: 0.8282 - val_loss: 0.5531 - val_acc: 0.8020\n",
      "Epoch 36/120\n",
      "33000/33000 [==============================] - 1s 24us/step - loss: 0.4758 - acc: 0.8305 - val_loss: 0.5509 - val_acc: 0.8023\n",
      "Epoch 37/120\n",
      "33000/33000 [==============================] - 1s 18us/step - loss: 0.4715 - acc: 0.8324 - val_loss: 0.5499 - val_acc: 0.8020\n",
      "Epoch 38/120\n",
      "33000/33000 [==============================] - 1s 18us/step - loss: 0.4673 - acc: 0.8335 - val_loss: 0.5497 - val_acc: 0.8010\n",
      "Epoch 39/120\n",
      "33000/33000 [==============================] - 1s 18us/step - loss: 0.4636 - acc: 0.8347 - val_loss: 0.5458 - val_acc: 0.8043\n",
      "Epoch 40/120\n",
      "33000/33000 [==============================] - 1s 17us/step - loss: 0.4596 - acc: 0.8359 - val_loss: 0.5451 - val_acc: 0.8037\n",
      "Epoch 41/120\n",
      "33000/33000 [==============================] - 1s 17us/step - loss: 0.4562 - acc: 0.8380 - val_loss: 0.5466 - val_acc: 0.8027\n",
      "Epoch 42/120\n",
      "33000/33000 [==============================] - 1s 19us/step - loss: 0.4526 - acc: 0.8394 - val_loss: 0.5450 - val_acc: 0.8060\n",
      "Epoch 43/120\n",
      "33000/33000 [==============================] - 1s 18us/step - loss: 0.4492 - acc: 0.8405 - val_loss: 0.5417 - val_acc: 0.8057\n",
      "Epoch 44/120\n",
      "33000/33000 [==============================] - 1s 19us/step - loss: 0.4456 - acc: 0.8417 - val_loss: 0.5409 - val_acc: 0.8067\n",
      "Epoch 45/120\n",
      "33000/33000 [==============================] - 1s 23us/step - loss: 0.4422 - acc: 0.8427 - val_loss: 0.5421 - val_acc: 0.8057\n",
      "Epoch 46/120\n",
      "33000/33000 [==============================] - 1s 18us/step - loss: 0.4393 - acc: 0.8447 - val_loss: 0.5420 - val_acc: 0.8057\n",
      "Epoch 47/120\n",
      "33000/33000 [==============================] - 1s 17us/step - loss: 0.4363 - acc: 0.8440 - val_loss: 0.5415 - val_acc: 0.8080\n",
      "Epoch 48/120\n",
      "33000/33000 [==============================] - 1s 17us/step - loss: 0.4331 - acc: 0.8464 - val_loss: 0.5436 - val_acc: 0.8080\n",
      "Epoch 49/120\n",
      "33000/33000 [==============================] - 1s 19us/step - loss: 0.4301 - acc: 0.8476 - val_loss: 0.5397 - val_acc: 0.8040\n",
      "Epoch 50/120\n",
      "33000/33000 [==============================] - 1s 18us/step - loss: 0.4275 - acc: 0.8481 - val_loss: 0.5398 - val_acc: 0.8060\n",
      "Epoch 51/120\n",
      "33000/33000 [==============================] - 1s 18us/step - loss: 0.4250 - acc: 0.8488 - val_loss: 0.5405 - val_acc: 0.8063\n",
      "Epoch 52/120\n",
      "33000/33000 [==============================] - 1s 19us/step - loss: 0.4219 - acc: 0.8504 - val_loss: 0.5385 - val_acc: 0.8050\n",
      "Epoch 53/120\n",
      "33000/33000 [==============================] - 1s 17us/step - loss: 0.4196 - acc: 0.8504 - val_loss: 0.5379 - val_acc: 0.8070\n",
      "Epoch 54/120\n",
      "33000/33000 [==============================] - 1s 18us/step - loss: 0.4168 - acc: 0.8518 - val_loss: 0.5388 - val_acc: 0.8083\n",
      "Epoch 55/120\n",
      "33000/33000 [==============================] - 1s 18us/step - loss: 0.4141 - acc: 0.8529 - val_loss: 0.5389 - val_acc: 0.8057\n",
      "Epoch 56/120\n",
      "33000/33000 [==============================] - 1s 17us/step - loss: 0.4117 - acc: 0.8549 - val_loss: 0.5390 - val_acc: 0.8080\n",
      "Epoch 57/120\n",
      "33000/33000 [==============================] - 1s 18us/step - loss: 0.4093 - acc: 0.8551 - val_loss: 0.5392 - val_acc: 0.8073\n",
      "Epoch 58/120\n",
      "33000/33000 [==============================] - 1s 17us/step - loss: 0.4067 - acc: 0.8565 - val_loss: 0.5395 - val_acc: 0.8103\n",
      "Epoch 59/120\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "33000/33000 [==============================] - 1s 18us/step - loss: 0.4043 - acc: 0.8567 - val_loss: 0.5383 - val_acc: 0.8087\n",
      "Epoch 60/120\n",
      "33000/33000 [==============================] - 1s 18us/step - loss: 0.4020 - acc: 0.8577 - val_loss: 0.5385 - val_acc: 0.8057\n",
      "Epoch 61/120\n",
      "33000/33000 [==============================] - 1s 18us/step - loss: 0.3996 - acc: 0.8580 - val_loss: 0.5403 - val_acc: 0.8073\n",
      "Epoch 62/120\n",
      "33000/33000 [==============================] - 1s 20us/step - loss: 0.3979 - acc: 0.8582 - val_loss: 0.5395 - val_acc: 0.8087\n",
      "Epoch 63/120\n",
      "33000/33000 [==============================] - 1s 19us/step - loss: 0.3955 - acc: 0.8595 - val_loss: 0.5413 - val_acc: 0.8080\n",
      "Epoch 64/120\n",
      "33000/33000 [==============================] - 1s 17us/step - loss: 0.3933 - acc: 0.8614 - val_loss: 0.5405 - val_acc: 0.8077\n",
      "Epoch 65/120\n",
      "33000/33000 [==============================] - 1s 17us/step - loss: 0.3911 - acc: 0.8617 - val_loss: 0.5431 - val_acc: 0.8080\n",
      "Epoch 66/120\n",
      "33000/33000 [==============================] - 1s 17us/step - loss: 0.3893 - acc: 0.8618 - val_loss: 0.5427 - val_acc: 0.8090\n",
      "Epoch 67/120\n",
      "33000/33000 [==============================] - 1s 16us/step - loss: 0.3872 - acc: 0.8636 - val_loss: 0.5423 - val_acc: 0.8057\n",
      "Epoch 68/120\n",
      "33000/33000 [==============================] - 1s 17us/step - loss: 0.3851 - acc: 0.8631 - val_loss: 0.5440 - val_acc: 0.8077\n",
      "Epoch 69/120\n",
      "33000/33000 [==============================] - 1s 17us/step - loss: 0.3830 - acc: 0.8645 - val_loss: 0.5479 - val_acc: 0.8067\n",
      "Epoch 70/120\n",
      "33000/33000 [==============================] - 1s 17us/step - loss: 0.3813 - acc: 0.8651 - val_loss: 0.5446 - val_acc: 0.8090\n",
      "Epoch 71/120\n",
      "33000/33000 [==============================] - 1s 17us/step - loss: 0.3795 - acc: 0.8656 - val_loss: 0.5454 - val_acc: 0.8063\n",
      "Epoch 72/120\n",
      "33000/33000 [==============================] - 1s 17us/step - loss: 0.3772 - acc: 0.8673 - val_loss: 0.5484 - val_acc: 0.8067\n",
      "Epoch 73/120\n",
      "33000/33000 [==============================] - 1s 18us/step - loss: 0.3757 - acc: 0.8675 - val_loss: 0.5469 - val_acc: 0.8097\n",
      "Epoch 74/120\n",
      "33000/33000 [==============================] - 1s 17us/step - loss: 0.3737 - acc: 0.8682 - val_loss: 0.5463 - val_acc: 0.8087\n",
      "Epoch 75/120\n",
      "33000/33000 [==============================] - 1s 18us/step - loss: 0.3716 - acc: 0.8688 - val_loss: 0.5512 - val_acc: 0.8057\n",
      "Epoch 76/120\n",
      "33000/33000 [==============================] - 1s 18us/step - loss: 0.3699 - acc: 0.8701 - val_loss: 0.5470 - val_acc: 0.8110\n",
      "Epoch 77/120\n",
      "33000/33000 [==============================] - 1s 19us/step - loss: 0.3686 - acc: 0.8701 - val_loss: 0.5466 - val_acc: 0.8100\n",
      "Epoch 78/120\n",
      "33000/33000 [==============================] - 1s 16us/step - loss: 0.3663 - acc: 0.8706 - val_loss: 0.5492 - val_acc: 0.8077\n",
      "Epoch 79/120\n",
      "33000/33000 [==============================] - 1s 16us/step - loss: 0.3648 - acc: 0.8713 - val_loss: 0.5485 - val_acc: 0.8073\n",
      "Epoch 80/120\n",
      "33000/33000 [==============================] - 1s 16us/step - loss: 0.3633 - acc: 0.8724 - val_loss: 0.5533 - val_acc: 0.8090\n",
      "Epoch 81/120\n",
      "33000/33000 [==============================] - 1s 16us/step - loss: 0.3616 - acc: 0.8736 - val_loss: 0.5503 - val_acc: 0.8083\n",
      "Epoch 82/120\n",
      "33000/33000 [==============================] - 1s 17us/step - loss: 0.3600 - acc: 0.8741 - val_loss: 0.5536 - val_acc: 0.8087\n",
      "Epoch 83/120\n",
      "33000/33000 [==============================] - 1s 18us/step - loss: 0.3585 - acc: 0.8737 - val_loss: 0.5558 - val_acc: 0.8063\n",
      "Epoch 84/120\n",
      "33000/33000 [==============================] - 1s 17us/step - loss: 0.3567 - acc: 0.8746 - val_loss: 0.5536 - val_acc: 0.8090\n",
      "Epoch 85/120\n",
      "33000/33000 [==============================] - 1s 17us/step - loss: 0.3552 - acc: 0.8753 - val_loss: 0.5549 - val_acc: 0.8070\n",
      "Epoch 86/120\n",
      "33000/33000 [==============================] - 1s 17us/step - loss: 0.3536 - acc: 0.8768 - val_loss: 0.5555 - val_acc: 0.8090\n",
      "Epoch 87/120\n",
      "33000/33000 [==============================] - 1s 16us/step - loss: 0.3522 - acc: 0.8762 - val_loss: 0.5558 - val_acc: 0.8087\n",
      "Epoch 88/120\n",
      "33000/33000 [==============================] - 1s 17us/step - loss: 0.3506 - acc: 0.8772 - val_loss: 0.5584 - val_acc: 0.8073\n",
      "Epoch 89/120\n",
      "33000/33000 [==============================] - 1s 20us/step - loss: 0.3491 - acc: 0.8773 - val_loss: 0.5573 - val_acc: 0.8100\n",
      "Epoch 90/120\n",
      "33000/33000 [==============================] - 1s 17us/step - loss: 0.3476 - acc: 0.8785 - val_loss: 0.5614 - val_acc: 0.8050\n",
      "Epoch 91/120\n",
      "33000/33000 [==============================] - 1s 18us/step - loss: 0.3461 - acc: 0.8788 - val_loss: 0.5591 - val_acc: 0.8087\n",
      "Epoch 92/120\n",
      "33000/33000 [==============================] - 1s 18us/step - loss: 0.3446 - acc: 0.8802 - val_loss: 0.5602 - val_acc: 0.8097\n",
      "Epoch 93/120\n",
      "33000/33000 [==============================] - 1s 17us/step - loss: 0.3431 - acc: 0.8805 - val_loss: 0.5630 - val_acc: 0.8083\n",
      "Epoch 94/120\n",
      "33000/33000 [==============================] - 1s 18us/step - loss: 0.3417 - acc: 0.8811 - val_loss: 0.5636 - val_acc: 0.8077\n",
      "Epoch 95/120\n",
      "33000/33000 [==============================] - 1s 17us/step - loss: 0.3405 - acc: 0.8801 - val_loss: 0.5645 - val_acc: 0.8087\n",
      "Epoch 96/120\n",
      "33000/33000 [==============================] - 1s 16us/step - loss: 0.3392 - acc: 0.8815 - val_loss: 0.5647 - val_acc: 0.8073\n",
      "Epoch 97/120\n",
      "33000/33000 [==============================] - 1s 17us/step - loss: 0.3374 - acc: 0.8819 - val_loss: 0.5666 - val_acc: 0.8050\n",
      "Epoch 98/120\n",
      "33000/33000 [==============================] - 1s 22us/step - loss: 0.3366 - acc: 0.8823 - val_loss: 0.5674 - val_acc: 0.8080\n",
      "Epoch 99/120\n",
      "33000/33000 [==============================] - 1s 18us/step - loss: 0.3348 - acc: 0.8828 - val_loss: 0.5690 - val_acc: 0.8080\n",
      "Epoch 100/120\n",
      "33000/33000 [==============================] - 1s 19us/step - loss: 0.3334 - acc: 0.8834 - val_loss: 0.5689 - val_acc: 0.8050\n",
      "Epoch 101/120\n",
      "33000/33000 [==============================] - 1s 20us/step - loss: 0.3321 - acc: 0.8839 - val_loss: 0.5707 - val_acc: 0.8070\n",
      "Epoch 102/120\n",
      "33000/33000 [==============================] - 1s 21us/step - loss: 0.3310 - acc: 0.8849 - val_loss: 0.5735 - val_acc: 0.8047\n",
      "Epoch 103/120\n",
      "33000/33000 [==============================] - 1s 23us/step - loss: 0.3297 - acc: 0.8852 - val_loss: 0.5725 - val_acc: 0.8070\n",
      "Epoch 104/120\n",
      "33000/33000 [==============================] - 1s 21us/step - loss: 0.3282 - acc: 0.8855 - val_loss: 0.5734 - val_acc: 0.8067\n",
      "Epoch 105/120\n",
      "33000/33000 [==============================] - 1s 19us/step - loss: 0.3269 - acc: 0.8873 - val_loss: 0.5771 - val_acc: 0.8080\n",
      "Epoch 106/120\n",
      "33000/33000 [==============================] - 1s 18us/step - loss: 0.3260 - acc: 0.8859 - val_loss: 0.5771 - val_acc: 0.8057\n",
      "Epoch 107/120\n",
      "33000/33000 [==============================] - 1s 17us/step - loss: 0.3247 - acc: 0.8862 - val_loss: 0.5789 - val_acc: 0.8063\n",
      "Epoch 108/120\n",
      "33000/33000 [==============================] - 1s 17us/step - loss: 0.3233 - acc: 0.8882 - val_loss: 0.5780 - val_acc: 0.8070\n",
      "Epoch 109/120\n",
      "33000/33000 [==============================] - 1s 17us/step - loss: 0.3220 - acc: 0.8877 - val_loss: 0.5828 - val_acc: 0.8057\n",
      "Epoch 110/120\n",
      "33000/33000 [==============================] - 1s 17us/step - loss: 0.3206 - acc: 0.8883 - val_loss: 0.5817 - val_acc: 0.8070\n",
      "Epoch 111/120\n",
      "33000/33000 [==============================] - 1s 17us/step - loss: 0.3198 - acc: 0.8895 - val_loss: 0.5821 - val_acc: 0.8067\n",
      "Epoch 112/120\n",
      "33000/33000 [==============================] - 1s 18us/step - loss: 0.3184 - acc: 0.8890 - val_loss: 0.5837 - val_acc: 0.8077\n",
      "Epoch 113/120\n",
      "33000/33000 [==============================] - 1s 17us/step - loss: 0.3173 - acc: 0.8898 - val_loss: 0.5839 - val_acc: 0.8053\n",
      "Epoch 114/120\n",
      "33000/33000 [==============================] - 1s 17us/step - loss: 0.3162 - acc: 0.8900 - val_loss: 0.5881 - val_acc: 0.8037\n",
      "Epoch 115/120\n",
      "33000/33000 [==============================] - 1s 17us/step - loss: 0.3149 - acc: 0.8910 - val_loss: 0.5878 - val_acc: 0.8047\n",
      "Epoch 116/120\n",
      "33000/33000 [==============================] - 1s 17us/step - loss: 0.3140 - acc: 0.8904 - val_loss: 0.5868 - val_acc: 0.8050\n",
      "Epoch 117/120\n",
      "33000/33000 [==============================] - 1s 17us/step - loss: 0.3129 - acc: 0.8913 - val_loss: 0.5908 - val_acc: 0.8050\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 118/120\n",
      "33000/33000 [==============================] - 1s 17us/step - loss: 0.3114 - acc: 0.8923 - val_loss: 0.5906 - val_acc: 0.8050\n",
      "Epoch 119/120\n",
      "33000/33000 [==============================] - 1s 17us/step - loss: 0.3105 - acc: 0.8922 - val_loss: 0.5912 - val_acc: 0.8057\n",
      "Epoch 120/120\n",
      "33000/33000 [==============================] - 1s 17us/step - loss: 0.3094 - acc: 0.8929 - val_loss: 0.5943 - val_acc: 0.8050\n"
     ]
    }
   ],
   "source": [
    "random.seed(123)\n",
    "model = models.Sequential()\n",
    "model.add(layers.Dense(50, activation='relu', input_shape=(2000,))) #2 hidden layers\n",
    "model.add(layers.Dense(25, activation='relu'))\n",
    "model.add(layers.Dense(7, activation='softmax'))\n",
    "\n",
    "model.compile(optimizer='SGD',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "moredata_model = model.fit(train_final,\n",
    "                    label_train_final,\n",
    "                    epochs=120,\n",
    "                    batch_size=256,\n",
    "                    validation_data=(val, label_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "33000/33000 [==============================] - 1s 30us/step\n",
      "4000/4000 [==============================] - 0s 32us/step\n"
     ]
    }
   ],
   "source": [
    "results_train = model.evaluate(train_final, label_train_final)\n",
    "results_test = model.evaluate(test, label_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.3044920818155462, 0.8954545454545455]"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.5706395657062531, 0.801]"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the same amount of epochs, we were able to get a fairly similar validation accuracy of 89.67 (compared to 88.55 in obtained in the first model in this lab). Our test set accuracy went up from 75.8 to a staggering 80.225% though, without any other regularization technique. You can still consider early stopping, L1, L2 and dropout here. It's clear that having more data has a strong impact on model performance!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Additional Resources\n",
    "\n",
    "* https://github.com/susanli2016/Machine-Learning-with-Python/blob/master/Consumer_complaints.ipynb\n",
    "* https://machinelearningmastery.com/dropout-regularization-deep-learning-models-keras/\n",
    "* https://catalog.data.gov/dataset/consumer-complaint-database"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary  \n",
    "\n",
    "In this lesson, we not only built an initial deep-learning model, we then used a validation set to tune our model using various types of regularization. From here, we'll continue to describe more practice and theory regarding tuning and optimizing deep-learning networks."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "learn-env",
   "language": "python",
   "name": "learn-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
